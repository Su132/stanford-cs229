{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nb\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs229.stanford.edu/ps/ps2/ps2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple example to understand Multinomial Naive Bayes.\n",
    "\n",
    "| Text | Tag   |\n",
    "|:------|:------:|\n",
    "| investors! free for qualified investors &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Spam |\n",
    "| discount prices for game investors | Spam |\n",
    "| lets grab a coffee | Not spam |\n",
    "| investors get a free discount | Spam |\n",
    "| our coffee game is a game for | Not spam |\n",
    "\n",
    "Then our vocabulary, denoted by $V$, with an extra column for the number of times used, is\n",
    "\n",
    "| Index | Word | Times used   |\n",
    "|:------:|:------:|:------:|\n",
    "| &nbsp; 0 &nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 3 |\n",
    "| 1 | coffee | 2 |\n",
    "| 2 | discount | 2 |\n",
    "| 3 | for | 3 |\n",
    "| 4 | free | 2 |\n",
    "| 5 | game | 3 |\n",
    "| 6 | get | 1 |\n",
    "| 7 | grab | 1 |\n",
    "| 8 | investors | 4 |\n",
    "| 9 | is | 1 |\n",
    "| 10 | lets | 1 |\n",
    "| 11 | our | 1 |\n",
    "| 12 | prices | 1 |\n",
    "| 13 | qualified | 1 |\n",
    "\n",
    "But it's easier to code this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voc1={0: 'a', 1: 'coffee', 2: 'discount', 3: 'for', 4: 'free', 5: 'game', 6: 'get', 7: 'grab', 8: 'investors', 9: 'is', 10: 'lets', 11: 'our', 12: 'prices', 13: 'qualified'}\n",
      "\n",
      "Xtr_1=\n",
      "[[0 0 0 1 1 0 0 0 2 0 0 0 0 1]\n",
      " [0 0 1 1 0 1 0 0 1 0 0 0 1 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 1 0 0 0]\n",
      " [1 0 1 0 1 0 1 0 1 0 0 0 0 0]\n",
      " [1 1 0 1 0 2 0 0 0 1 0 1 0 0]]\n",
      "\n",
      "Occurrence count=\n",
      "Ind  Word         Total Spam  Not\n",
      "---------------------------------\n",
      "0    a               3    1    2\n",
      "1    coffee          2    0    2\n",
      "2    discount        2    2    0\n",
      "3    for             3    2    1\n",
      "4    free            2    2    0\n",
      "5    game            3    1    2\n",
      "6    get             1    1    0\n",
      "7    grab            1    0    1\n",
      "8    investors       4    4    0\n",
      "9    is              1    0    1\n",
      "10   lets            1    0    1\n",
      "11   our             1    0    1\n",
      "12   prices          1    1    0\n",
      "13   qualified       1    1    0\n",
      "\n",
      "cnt_spam_words=15  cnt_ns_words=11\n"
     ]
    }
   ],
   "source": [
    "train_phrases = ['investors free for qualified investors',\n",
    "                'discount prices for game investors',\n",
    "                'lets grab a coffee',\n",
    "                'investors get a free discount',\n",
    "                'our coffee game is a game for']\n",
    "\n",
    "def make_vocab(train_phrases):\n",
    "    all_words = []\n",
    "    for phrase in train_phrases:\n",
    "        word_list = phrase.split()\n",
    "        for w in word_list: all_words.append(w)\n",
    "    auw = list(set(all_words)) # removes dups\n",
    "    auw.sort()\n",
    "    return {i:w for i,w in zip(range(len(auw)), auw)}\n",
    "\n",
    "Voc1 = make_vocab(train_phrases)\n",
    "print(\"Voc1={0}\\n\".format(Voc1))\n",
    "\n",
    "def get_V_key(value, V):\n",
    "    for key in V:\n",
    "        if V[key] == value: return key\n",
    "    return None\n",
    "\n",
    "def parse_phrases(phrases, V):\n",
    "    X = []\n",
    "    for phrase in phrases:\n",
    "        js = [get_V_key(word, V) for word in phrase.split()]\n",
    "        d = {j:js.count(j) for j in js}\n",
    "        x = np.zeros(len(V))\n",
    "        for k, v in d.items():\n",
    "            x[k] = v\n",
    "        X.append(x)\n",
    "    return np.array(X, dtype=int)\n",
    "\n",
    "Xtr_1 = parse_phrases(train_phrases, Voc1)\n",
    "\n",
    "ytr_1 = np.array([1,1,0,1,0]) # 1 denotes Spam, 0 denotes non-Spam\n",
    "\n",
    "Xtr_1s, Xtr_1n = Xtr_1[ytr_1==1], Xtr_1[ytr_1==0]\n",
    "\n",
    "print(\"Xtr_1=\\n{}\".format(Xtr_1))\n",
    "\n",
    "print(\"\\nOccurrence count=\")\n",
    "\n",
    "print(\"%-4s %-12s %-4s %-4s %4s\" % (\"Ind\", \"Word\", \"Total\", \"Spam\", \"Not\"))\n",
    "print(\"---------------------------------\")\n",
    "for key, value in Voc1.items():\n",
    "    tc, sc, nc = np.sum(Xtr_1[:,key]), np.sum(Xtr_1s[:,key]), np.sum(Xtr_1n[:,key])\n",
    "    print(\"%-4.0f %-12s %4.0f %4.0f %4.0f\" % (key, value, tc, sc, nc))\n",
    "\n",
    "cnt_spam_words, cnt_ns_words = np.sum(Xtr_1s), np.sum(Xtr_1n)\n",
    "print(\"\\ncnt_spam_words={}  cnt_ns_words={}\".format(cnt_spam_words, cnt_ns_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Multinomial\n",
    "\n",
    "Note let's suppose we get a new phrase \"discount for coffee investors\" and we wish to classify it as spam or not. Using Bayes, we compute\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})=\\frac{P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})}{P(\\text{discount for coffee investors})}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})=\\frac{P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})}{P(\\text{discount for coffee investors})}\n",
    "\\end{align*}$$\n",
    "\n",
    "Then we pick the larger of the two. But we can discard the denominator because it is the same in both cases. Discarding this, we still pick the larger of the two:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})\\propto P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})\\tag{SE.1.1}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})\\propto P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})\\tag{SE.1.2}\n",
    "\\end{align*}$$\n",
    "\n",
    "Estimating the prior class probabilities is easy. Define\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_c\\equiv\\frac{\\text{number of occurences of class }c\\text{ in training data}}{\\text{number of observations in training data}}\\approx P(c)\\tag{SE.1.3}\n",
    "\\end{align*}$$\n",
    "\n",
    "so that $\\phi_c$ denotes the estimated prior probability of class $c$. Then\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{Spam}=\\frac{3}{5}\\quad\\quad\\quad \\phi_{Not-Spam}=\\frac{2}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "But we have a problem. The phrase \"discount for coffee investors\" doesn't appear in our training data so the probabilities in SE.1.1 and SE.1.2 will both be zero. To circumvent this, we make the Naive Bayes Assumption: the words in each phrase are conditionally independent given the classfication. That is, we assume\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee investors}\\mid c)=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)P(\\text{coffee}\\mid c)P(\\text{investors}\\mid c)\\tag{SE.1.4}\n",
    "\\end{align*}$$\n",
    "\n",
    "Note that $P(\\text{for}\\mid c)$ denotes the probability of word \"for\" appearing a single time. If \"for\" were to appear twice in a phrase, then we would have\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee for investors}\\mid c) &=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)P(\\text{coffee}\\mid c)P(\\text{for}\\mid c)P(\\text{investors}\\mid c)\\\\\n",
    "     &=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)^{2}P(\\text{coffee}\\mid c)P(\\text{investors}\\mid c)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Similarly, if \"for\" were to appear three times in a phrase, then we would have\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee for investors for}\\mid c) &=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)P(\\text{coffee}\\mid c)P(\\text{for}\\mid c)P(\\text{investors}\\mid c)P(\\text{for}\\mid c)\\\\\n",
    "     &=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)^{3}P(\\text{coffee}\\mid c)P(\\text{investors}\\mid c)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Let $P(w_j\\mid c)$ denote the probability that the word with index $j$ appears once in a phrase classified as $c$. And let $N(w,c)$ denote the number of occurences of word $w$ for observations classified as $c$. We can estimate each of the probabilities on the right side of SE.1.4. Define\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=c}\\equiv\\frac{N(w_j,c)}{\\sum_{w\\in V}N(w,c)}\\approx P(w_j\\mid c)\\tag{SE.1.5}\n",
    "\\end{align*}$$\n",
    "\n",
    "so that $\\phi_{j\\mid y=c}$ denotes the estimated conditional probability that the word $w_j$ appears once in an observation classified as $c$. For example:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Not-Spam})\\approx\\phi_{8\\mid y=0}=\\frac{N(w_8,0)}{\\sum_{w\\in V}N(w,0)}=\\frac{0}{11}=0\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Spam})\\approx\\phi_{8\\mid y=1}=\\frac{N(w_8,1)}{\\sum_{w\\in V}N(w,1)}=\\frac{4}{15}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Not-Spam})\\approx\\phi_{1\\mid y=0}=\\frac{N(w_1,0)}{\\sum_{w\\in V}N(w,0)}=\\frac{2}{11}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Spam})\\approx\\phi_{1\\mid y=1}=\\frac{N(w_1,1)}{\\sum_{w\\in V}N(w,1)}=\\frac{0}{15}=0\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Not-Spam})\\approx\\phi_{5\\mid y=0}=\\frac{N(w_5,0)}{\\sum_{w\\in V}N(w,0)}=\\frac{2}{11}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Spam})\\approx\\phi_{5\\mid y=1}=\\frac{N(w_5,1)}{\\sum_{w\\in V}N(w,1)}=\\frac{1}{15}\n",
    "\\end{align*}$$\n",
    "\n",
    "Now we have another problem. The word \"coffee\" doesn't appear in any training observations that are classified as spam. Hence $\\phi_{1\\mid y=1}=0\\approx P(\\text{coffee}\\mid\\text{Spam})$. This is a problem because in equation SE.1.4, this would give $P(\\text{discount for coffee investors}\\mid \\text{Spam})=0$. But we would obviously like to classify the phrase \"discount for coffee investors\" as spam.\n",
    "\n",
    "To get around this, we use Laplace Smoothing. We add $1$ to every count $N(w,c)$. Then SE.1.5 becomes\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(w_j\\mid c)\\approx\\phi_{j\\mid y=c}=\\frac{N(w_j,c)+1}{\\sum_{w\\in V}\\big(N(w,c)+1\\big)}=\\frac{N(w_j,c)+1}{\\sum_{w\\in V}N(w,c)+\\lvert V\\rvert}\\tag{SE.1.6}\n",
    "\\end{align*}$$\n",
    "\n",
    "Besides helping us avoid zero probability estimates, Laplace Smoothing is intuitive and realistic. That is, it is not impossible that the word \"coffee\" will appear in test (or real-world) spam emails, simply because it didn't appear in our spam-labeled training data.\n",
    "\n",
    "Then our examples become\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Not-Spam})\\approx\\phi_{8\\mid y=0}=\\frac{0+1}{11+14}=\\frac{1}{25}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Spam})\\approx\\phi_{8\\mid y=1}=\\frac{4+1}{15+14}=\\frac{5}{29}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Not-Spam})\\approx\\phi_{1\\mid y=0}=\\frac{2+1}{11+14}=\\frac{3}{25}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Spam})\\approx\\phi_{1\\mid y=1}=\\frac{0+1}{15+14}=\\frac{1}{29}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Not-Spam})\\approx\\phi_{5\\mid y=0}=\\frac{2+1}{11+14}=\\frac{3}{25}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Spam})\\approx\\phi_{5\\mid y=1}=\\frac{1+1}{15+14}=\\frac{2}{29}\n",
    "\\end{align*}$$\n",
    "\n",
    "Then our computations for SE.1.4 are\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee investors}\\mid\\text{Spam}) &= P(\\text{discount}\\mid\\text{Spam})P(\\text{for}\\mid \\text{Spam})P(\\text{coffee}\\mid\\text{Spam})P(\\text{investors}\\mid\\text{Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=1}\\phi_{3\\mid y=1}\\phi_{1\\mid y=1}\\phi_{8\\mid y=1}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee investors}\\mid\\text{Not-Spam}) &\\approx \\phi_{2\\mid y=0}\\phi_{3\\mid y=0}\\phi_{1\\mid y=0}\\phi_{8\\mid y=0}\n",
    "\\end{align*}$$\n",
    "\n",
    "Then we can compare\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})&\\propto P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=1}\\phi_{3\\mid y=1}\\phi_{1\\mid y=1}\\phi_{8\\mid y=1}\\frac{3}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})&\\propto P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=0}\\phi_{3\\mid y=0}\\phi_{1\\mid y=0}\\phi_{8\\mid y=0}\\frac{2}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "But of course this is easier with code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam=0.00003817  Not=0.00000614 \"discount for coffee investors\"\n",
      "Spam=0.00000395  Not=0.00000049 \"discount for coffee for investors\"\n",
      "Spam=0.00003817  Not=0.00000614  ratio=6.21  \"discount for coffee investors\"  class=SPAM\n",
      "Spam=0.00000395  Not=0.00000049  ratio=8.03  \"discount for coffee for investors\"  class=SPAM\n",
      "Spam=0.00000170  Not=0.00005530  ratio=32.59  \"a coffee is coffee\"  class=NOT\n",
      "Spam=0.00000509  Not=0.00001843  ratio=3.62  \"a coffee is free\"  class=NOT\n",
      "Spam=0.00001272  Not=0.00000614  ratio=2.07  \"investors coffee is free\"  class=SPAM\n",
      "Spam=0.00001272  Not=0.00000614  ratio=2.07  \"investors coffee is discount\"  class=SPAM\n",
      "Spam=0.00000848  Not=0.00001843  ratio=2.17  \"investors coffee is game\"  class=NOT\n",
      "Spam=0.00001697  Not=0.00000922  ratio=1.84  \"investors coffee get game\"  class=SPAM\n",
      "Spam=0.00000006  Not=0.00000000  ratio=477.67  \"investors get prices free for qualified investors\"  class=SPAM\n",
      "Spam=0.00000000  Not=0.00000024  ratio=233.89  \"our coffee grab is lets coffee\"  class=NOT\n",
      "TIME=0.009073972702026367 seconds\n",
      "s1=[ 0.00003817  0.00000395  0.0000017   0.00000509  0.00001272  0.00001272\n",
      "  0.00000848  0.00001697  0.00000006  0.        ]\n",
      "s0=[ 0.00000614  0.00000049  0.0000553   0.00001843  0.00000614  0.00000614\n",
      "  0.00001843  0.00000922  0.          0.00000024]\n",
      "rs=[   6.21327485    8.03440714   32.59150848    3.62127872    2.07109162\n",
      "    2.07109162    2.17276723    1.84097033  477.66986535  233.89404699]\n",
      "TIME=0.0016341209411621094 seconds\n"
     ]
    }
   ],
   "source": [
    "Njc = lambda j, Xc: np.sum(Xc[:,j])\n",
    "\n",
    "phijc = lambda j, Xc, V: (Njc(j, Xc) + 1)/(np.sum([Njc(i, Xc) for i in range(len(V))]) + len(V))\n",
    "\n",
    "prob_sp_dfci = phijc(2,Xtr_1s,Voc1) * phijc(3,Xtr_1s,Voc1) * phijc(1,Xtr_1s,Voc1) * phijc(8,Xtr_1s,Voc1) * 3/5\n",
    "prob_ns_dfci = phijc(2,Xtr_1n,Voc1) * phijc(3,Xtr_1n,Voc1) * phijc(1,Xtr_1n,Voc1) * phijc(8,Xtr_1n,Voc1) * 2/5\n",
    "print(\"Spam={:.8f}  Not={:.8f} \\\"discount for coffee investors\\\"\".format(prob_sp_dfci, prob_ns_dfci))\n",
    "\n",
    "prob_sp_dfci = phijc(2,Xtr_1s,Voc1) * np.power(phijc(3,Xtr_1s,Voc1), 2) * phijc(1,Xtr_1s,Voc1) * phijc(8,Xtr_1s,Voc1) * 3/5\n",
    "prob_ns_dfci = phijc(2,Xtr_1n,Voc1) * np.power(phijc(3,Xtr_1n,Voc1), 2) * phijc(1,Xtr_1n,Voc1) * phijc(8,Xtr_1n,Voc1) * 2/5\n",
    "print(\"Spam={:.8f}  Not={:.8f} \\\"discount for coffee for investors\\\"\".format(prob_sp_dfci, prob_ns_dfci))\n",
    "\n",
    "# to check more of these, it will be easier to use this approach:\n",
    "\n",
    "def compute_probs(phrase, X1, X0, y, V):\n",
    "    js = [get_V_key(word, V) for word in phrase.split()]\n",
    "    prob_sp = np.sum(y==1) / y.shape[0] * np.prod([phijc(j, X1, V) for j in js])\n",
    "    prob_ns = np.sum(y==0) / y.shape[0] * np.prod([phijc(j, X0, V) for j in js])\n",
    "    return np.array([prob_sp, prob_ns])\n",
    "\n",
    "def classify(phrase, X, y, V):\n",
    "    X1, X0 = X[y==1], X[y==0]\n",
    "    cp = compute_probs(phrase, X1, X0, y, V)\n",
    "    ratio = cp[1] / cp[0] if cp[1] >= cp[0] else cp[0] / cp[1]\n",
    "    label = \"SPAM\" if cp[0] >= cp[1] else \"NOT\"\n",
    "    print(\"Spam={:.8f}  Not={:.8f}  ratio={:.2f}  \\\"{}\\\"  class={}\".format(cp[0], cp[1], ratio, phrase, label))\n",
    "\n",
    "test_phrases = ['discount for coffee investors',\n",
    "    'discount for coffee for investors',\n",
    "    'a coffee is coffee',\n",
    "    'a coffee is free',\n",
    "    'investors coffee is free',\n",
    "    'investors coffee is discount',\n",
    "    'investors coffee is game',\n",
    "    'investors coffee get game',\n",
    "    'investors get prices free for qualified investors',\n",
    "    'our coffee grab is lets coffee']\n",
    "\n",
    "start = time.time()\n",
    "for phr in test_phrases: classify(phr, Xtr_1, ytr_1, Voc1)\n",
    "end = time.time()\n",
    "print(\"TIME={} seconds\".format(end-start))\n",
    "    \n",
    "# but we can vectorize this\n",
    "\n",
    "def train_1(X, y, V):\n",
    "    X1, X0 = X[y==1], X[y==0]\n",
    "    phi_js_yeq1 = (np.sum(X1, axis=0) + 1) / (np.sum(X1) + len(V))\n",
    "    phi_js_yeq0 = (np.sum(X0, axis=0) + 1) / (np.sum(X0) + len(V))\n",
    "    phi_yeq1 = np.sum(y==1) / y.shape[0]\n",
    "    phi_yeq0 = np.sum(y==0) / y.shape[0]\n",
    "    return phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0\n",
    "\n",
    "def predict_1(phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0, V, test_phrases):\n",
    "    Xtest = parse_phrases(test_phrases, V)\n",
    "    pX1 = np.power(phi_js_yeq1, Xtest)\n",
    "    pX0 = np.power(phi_js_yeq0, Xtest)\n",
    "    s1 = phi_yeq1 * np.prod(pX1, axis=1)\n",
    "    s0 = phi_yeq0 * np.prod(pX0, axis=1)\n",
    "    rs = s1/s0\n",
    "    rs[rs<1] = 1./rs[rs<1]\n",
    "    return s1, s0, rs\n",
    "\n",
    "def train_and_predict_1(X, y, V, test_phrases):\n",
    "    phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0 = train_1(X, y, V)\n",
    "    return predict_1(phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0, V, test_phrases)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "start = time.time()\n",
    "s1, s0, rs = train_and_predict_1(Xtr_1, ytr_1, Voc1, test_phrases)\n",
    "print(\"s1={}\\ns0={}\\nrs={}\".format(s1.round(8), s0.round(8), rs))\n",
    "end = time.time()\n",
    "print(\"TIME={} seconds\".format(end-start))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formalize Multinomial Estimations\n",
    "\n",
    "Now let's formalize and clearly write out a few items from above. Recall SE.1.6:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=c}\\equiv\\frac{N(w_j,c)+1}{\\sum_{w\\in V}\\big(N(w,c)+1\\big)}=\\frac{N(w_j,c)+1}{\\sum_{w\\in V}N(w,c)+\\lvert V\\rvert}\\approx P(w_j\\mid c)\\tag{SE.1.6}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $N(w,c)$ denotes the number of occurences of word $w$ for observations classified as $c$. If we let $\\phi_j\\approx P(w_j)$ denote the estimated probability that word $j$ appears in the training data regardless of class, then\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j}\\equiv\\frac{\\sum_{i=1}^{m}x_j^{(i)}}{\\sum_{i=1}^{m}n_i}\\approx P(w_j)\\tag{SE.1.7}\n",
    "\\end{align*}$$\n",
    "\n",
    "where\n",
    "\n",
    "- $m$ is the number of training observations\n",
    "- $x^{(i)}\\in\\mathbb{R}^n$ denotes the $i^{th}$ training observation\n",
    "- $x_j^{(i)}$ denotes the $j^{th}$ element of the $i^{th}$ training observation\n",
    "- $n_i$ denotes the number of words, aka __tokens__, in observation $i$.\n",
    "\n",
    "In SE.1.7, the numerator is the count of all the times that word $j$ appears in all of the training observations. And the denominator is the count (including multiplicities) of all the words that appear in all of the training observations. So this estimate is intuitive.\n",
    "\n",
    "With this in mind, we can rewrite SE.1.6 withOUT Laplace smoothing as\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=c}=\\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =c \\}x_j^{(i)}}{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =c \\}n_i}\\approx P(w_j\\mid y=c)\n",
    "\\end{align*}$$\n",
    "\n",
    "and with Laplace smoothing as\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=c}=\\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =c \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =c \\}n_i + \\lvert V \\rvert}\\approx P(w_j\\mid y=c)\n",
    "\\end{align*}$$\n",
    "\n",
    "Similarly recall SE.1.3:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_c\\equiv\\frac{\\text{number of occurences of class }c\\text{ in training data}}{\\text{number of observations in training data}}\\approx P(c)\\tag{SE.1.3}\n",
    "\\end{align*}$$\n",
    "\n",
    "so that $\\phi_c$ denotes the estimated prior probability of class $c$. The numerator equals $\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = c \\}$. And the denominator is $m$. Hence\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_c=\\frac{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = c \\}}{m}\\approx P(c)\n",
    "\\end{align*}$$\n",
    "\n",
    "But throughout this document, $y$ will take values in $\\{0,1\\}$. So we will define\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_y\\equiv\\frac{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\}}{m}\\approx P(y=1)\n",
    "\\end{align*}$$\n",
    "\n",
    "And when we want to denote the estimated prior $P(y=0)$, we will write $1-\\phi_y$.\n",
    "\n",
    "Hence our maximum likelihood estimates for Multinomial Naive Bayes can be written as\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\phi_{j\\mid y=1}\\equiv\\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =1 \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\} n_i + \\lvert V \\rvert}=\\hat{P}(w_j\\mid y=1)\\tag{SE.1.8} \\\\\\\\\n",
    "\\phi_{j\\mid y=0}\\equiv\\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =0 \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 0 \\} n_i + \\lvert V \\rvert}=\\hat{P}(w_j\\mid y=0)\\tag{SE.1.9} \\\\\\\\\n",
    "\\phi_{y}\\equiv\\frac{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\}}{m}=\\hat{P}(y=1)\\tag{SE.1.10}\n",
    "\\end{gather}$$\n",
    "\n",
    "where the hat $\\hat{P}$ denotes the maximum likelihood estimate.\n",
    " \n",
    "Since $\\hat{P}(w_j\\mid y=1)=\\phi_{j\\mid y=1}$ denotes the estimated probability of word $j$ appearing once, then the estimated probability that word $j$ appears $x_j$ number of times is\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\hat{p}(x_j\\mid y=1)=\\prod_{k=1}^{x_j}\\hat{P}(w_j\\mid y=1)=\\phi_{j\\mid y=1}^{x_j}\\tag{SE.1.11} \\\\\\\\\n",
    "\\end{gather}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Training\n",
    "\n",
    "We wish to produce three things from training:\n",
    "\n",
    "1. An array of the $\\{\\phi_{j\\mid y=1}\\}_{j=1}^{n}$\n",
    "1. An array of the $\\{\\phi_{j\\mid y=0}\\}_{j=1}^{n}$\n",
    "1. The number $\\phi_y$\n",
    "\n",
    "Let $X$ denote the training data matrix. Let $X_1$ denote the matrix of rows from $X$ such that the corresponding label is $y=1$, and similarly for $X_0$. From SE.1.8, we have\n",
    "\n",
    "$$\n",
    "\\phi_{j\\mid y=1}\\equiv\\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =1 \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\} n_i + \\lvert V \\rvert}=\\frac{\\sum_{i=1}^{m_1}x_j^{(1,i)} + 1}{\\sum_{i=1}^{m_1} n_{1,i} + \\lvert V \\rvert}\n",
    "$$\n",
    "\n",
    "where $m_1$ is the number of rows in matrix $X_1$, $x^{(1,i)}$ is the $i^{th}$ row in matrix $X_1$, and $n_{1,i}$ is the total number of words in the $i^{th}$ row of matrix $X_1$. But then $\\sum_{i=1}^{m_1} n_{1,i}$ is total number of words in matrix $X_1$, denoted by $t_1$:\n",
    "\n",
    "$$\n",
    "\\phi_{j\\mid y=1}=\\frac{\\sum_{i=1}^{m_1}x_j^{(1,i)} + 1}{t_1 + \\lvert V \\rvert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb_train_ex(X, y):\n",
    "    X1, X0 = X[y==1], X[y==0]\n",
    "    vocab_size = X.shape[1]\n",
    "    phi_js_yeq1 = (np.sum(X1, axis=0) + 1) / (np.sum(X1) + vocab_size)\n",
    "    phi_js_yeq0 = (np.sum(X0, axis=0) + 1) / (np.sum(X0) + vocab_size)\n",
    "    phi_y = np.sum(y==1) / y.shape[0]\n",
    "    return phi_js_yeq1, phi_js_yeq0, phi_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli NB Likelihood\n",
    "\n",
    "Let's derive the log-likelihood in the case of Bernoulli Naive Bayes.\n",
    "\n",
    "Let $y$ be a discrete random variable with values in $\\{0,1\\}$. Define $\\phi_y\\equiv \\hat{P}(y=1)=\\hat{p}_{y}(1)$.\n",
    "\n",
    "Let $x=[x_1,...,x_n]^{T}$ be a discrete random vector with values in $\\{0,1\\}^{n}$, where $n$ is the number of features. That is, an observation from $x$ is a column vector in $\\mathbb{R}^n$ whose elements are $0$ or $1$. Define\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\phi_{j\\mid y=1}\\equiv\\hat{P}(x_j=1\\mid y=1)=\\hat{p}_{x_j\\mid y}(1\\mid 1)\\\\\\\\\n",
    "\\phi_{j\\mid y=0}\\equiv\\hat{P}(x_j=1\\mid y=0)=\\hat{p}_{x_j\\mid y}(1\\mid 0)\n",
    "\\end{gather}$$\n",
    "\n",
    "Given a training set $\\{x^{(i)},y^{(i)}\\}_{i=1}^{m}$ where each $(x^{(i)},y^{(i)})$ is an independent observation from the joint random variable $(x,y)$, then the joint likelihood of the training data is\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\mathscr{L}\\big(\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y;\\{x^{(i)},y^{(i)}\\}_{i=1}^{m}\\big) &= P\\big((x,y=x^{(1)},y^{(1)}),...,(x,y=x^{(m)},y^{(m)});\\phi_{j|y=1},\\phi_{j|y=0},\\phi_y\\big)\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}P\\big(x,y=x^{(i)},y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big)\\tag{SCN.1}\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}p_{x,y}\\big(x^{(i)},y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big)\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}p_{x\\mid y}\\big(x^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0}\\big)p_{y}\\big(y^{(i)};\\phi_y\\big)\\tag{SCN.2}\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "SCN.1 holds from the assumption that observations are independent. SCN.2 holds from the definition of conditional probability. And the log-likelihood is\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\ell\\big(\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big) &= \\sum_{i=1}^m\\mathrm{ln}\\big[p_{x\\mid y}(x^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big]\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\mathrm{ln}\\Big[\\prod_{j=1}^{n}p_{x_{j}\\mid y}(x_j^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\Big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big]\\tag{SCN.3}\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\sum_{j=1}^{n}\\mathrm{ln}\\big[p_{x_{j}\\mid y}(x_j^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big] \\tag{SCN.4}\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\sum_{j=1}^{n}\\big\\{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}\\mathrm{ln}(\\phi_{j\\mid y=1})+\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=1\\}\\mathrm{ln}(\\phi_{j\\mid y=0}) \\\\\n",
    "     &+ \\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}\\mathrm{ln}(1-\\phi_{j\\mid y=1})+\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=0\\}\\mathrm{ln}(1-\\phi_{j|y=0})\\big\\} \\tag{SCN.5} \\\\\n",
    "     &+ \\sum_{i=1}^m\\big\\{\\boldsymbol{1}\\{y^{(i)}=1\\}\\mathrm{ln}(\\phi_y)+\\boldsymbol{1}\\{y^{(i)}=0\\}\\mathrm{ln}(1-\\phi_y)\\big\\}\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "SCN.3 follows from the Naive Bayes Assumption: $x_1^{(i)},...,x_n^{(i)}$ are conditionally independent given $y^{(i)}$. Intuitively, if we know whether an email is spam or not, then the appearance of different words is assumed to be independent. For example, if we know that an email is spam, then the appearances of \"price\" and \"discount\" are assumed to be independent events. This example emphasizes the general inaccuracy of this assumption and explains \"Naive\" in the name. If we know the email is spam and we know that \"price\" appeared, then it seems more likely that \"discount\" also appeared. Nevertheless, NB can give good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli NB MLE\n",
    "\n",
    "To maximize the log-likelihood in $\\phi_y$, let's compute the partial derivative of SCN.5 with respect to $\\phi_y$ and set it equal to zero:\n",
    "\n",
    "$$\\begin{align*} \n",
    "0=\\frac{\\partial\\ell}{\\partial\\phi_y} &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\frac1{\\phi_y} - \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}}\\frac1{1-\\phi_y}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}}\\frac1{1-\\phi_y} = \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\frac1{\\phi_y}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\frac1{1-\\phi_y}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}} = \\frac1{\\phi_y}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Multiplying both sides by $\\phi_y(1-\\phi_y)$, we get\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}} &= (1-\\phi_y)\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\\\\n",
    "    &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}} - \\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}} &= \\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}} + \\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\\\\n",
    "    &= \\phi_y\\sum_{i=1}^m\\big[\\boldsymbol{1}\\{y^{(i)}=0\\} + \\boldsymbol{1}\\{y^{(i)}=1\\}\\big]\\\\\n",
    "    &= \\phi_y\\sum_{i=1}^m1\\\\\n",
    "    &= \\phi_y m\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_y = \\frac1m\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can maximize in $\\phi_{j|y=1}$:\n",
    "\n",
    "$$\\begin{align*} \n",
    "0=\\frac{\\partial\\ell}{\\partial\\phi_{j|y=1}} &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\frac1{\\phi_{j|y=1}} - \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}}\\frac1{1-\\phi_{j|y=1}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}}\\frac1{1-\\phi_{j|y=1}} = \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\frac1{\\phi_{j|y=1}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\frac1{1-\\phi_{j|y=1}}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}} = \\frac1{\\phi_{j|y=1}}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Multiplying both sides by $\\phi_{j|y=1}(1-\\phi_{j|y=1})$, we get\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}} &= (1-\\phi_{j|y=1})\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\\\\n",
    "    &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}} - \\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}} &= \\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}} + \\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\\\\n",
    "    &= \\phi_{j|y=1}\\sum_{i=1}^m\\big[\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\} + \\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}\\big]\\\\\n",
    "    &= \\phi_{j|y=1}\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j|y=1} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}x_j^{(i)}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}}\\tag{SN.6}\n",
    "\\end{align*}$$\n",
    "\n",
    "Similarly we can compute\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j|y=0} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=1\\}}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=0\\}} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}}x_j^{(i)}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=0\\}}\\tag{SN.7}\n",
    "\\end{align*}$$\n",
    "\n",
    "SCN.6 and SCN.7 are identical to the corresponding multinomial equations in SE.1.8 and SE.1.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction & Underflow\n",
    "\n",
    "Earlier we compared the following values and predicted spam or not:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})&\\propto P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})\\\\\\\\\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})&\\propto P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "More succintly, we picked the larger of these two quantities\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y=1\\mid x)&\\propto p(x\\mid y=1)p(y=1)\\\\\\\\\n",
    "p(y=0\\mid x)&\\propto p(x\\mid y=0)p(y=0)\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "But when our vocabulary is sufficiently large, we will easily encounter an __underflow__ problem. Recall that we computed $p(x\\mid y=c)$ with the Naive Bayes assumption of conditional independence:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(x\\mid y=1)=\\prod_{j=1}^{n}p(x_j\\mid y=1)\\approx\\prod_{j=1}^{n}\\phi_{j\\mid y=1}^{x_j}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $n$ is the size of our vocabulary. But the product of a large number of values between $0$ and $1$ will get very close to $0$. And the standard computer representation of real numbers cannot handle numbers that are too small, and instead rounds them off to zero... underflow.\n",
    "\n",
    "Let's take a very conservative example and show that $p(x\\mid y=1)\\approx0$ can easily produce underflow. Suppose we have a $10{,}000$ words in our vocabulary. And suppose $20\\%$ of the words each have a probability near $0.99$ of appearing in a given email/document given that it is spam. Also suppose that the remaining $80\\%$ of the words each have a probability near $0.5$ of appearing in a given email/document given that it is spam.\n",
    "\n",
    "This is an unrealistic example but it's conservative in that we will show how easy it is to produce underflow. In reality, the proposed probabilities would be much lower and hence more likely to induce underflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeroprod=0.0  nzprod=5e-324\n",
      "zeroprod=0.0\n"
     ]
    }
   ],
   "source": [
    "uf_a = .000001 * np.random.randn(8000)+.5\n",
    "uf_b = .000001 * np.random.randn(2000)+.99\n",
    "zeroprod = np.prod(np.concatenate((uf_a, uf_b)))\n",
    "uf_a = .000001 * np.random.randn(8000)+.501\n",
    "nzprod = np.prod(np.concatenate((uf_a, uf_b)))\n",
    "print(\"zeroprod={}  nzprod={}\".format(zeroprod, nzprod))\n",
    "\n",
    "# a bit more realistic\n",
    "uf_a = .000001 * np.random.randn(8000)+.01\n",
    "uf_b = .000001 * np.random.randn(1000)+.5\n",
    "uf_c = .000001 * np.random.randn(500)+.80\n",
    "uf_d = .000001 * np.random.randn(500)+.99\n",
    "zeroprod = np.prod(np.concatenate((uf_a, uf_b, uf_c, uf_d)))\n",
    "print(\"zeroprod={}\".format(zeroprod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid underflow, we compute\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y=1\\mid x) &= \\frac{p(x\\mid y=1)p(y=1)}{p(x)} \\\\\n",
    "              &= \\frac{p(x\\mid y=1)p(y=1)}{p(x\\mid y=1)p(y=1) + p(x\\mid y=0) p(y=0)} \\\\\n",
    "              &= \\frac{1}{1 + \\frac{p(x\\mid y=0) p(y=0)}{p(x\\mid y=1) p(y=1)}} \\\\\n",
    "              &= \\frac{1}{1 + \\mathrm{exp}\\big(\\mathrm{log}\\big[\\frac{p(x\\mid y=0) p(y=0)}{p(x\\mid y=1) p(y=1)}\\big]\\big)} \\\\\n",
    "              &= \\frac{1}{1 + \\mathrm{exp}\\big(\\mathrm{log}[p(x\\mid y=0)] + \\mathrm{log}[p(y=0)] - \\mathrm{log}[p(x\\mid y=1)] - \\mathrm{log}[p(y=1)]\\big)} \\\\\n",
    "              &\\approx \\frac{1}{1 + \\mathrm{exp}\\big(\\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=0}]x_j + \\mathrm{log}[1 - \\phi_y] - \\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=1}]x_j - \\mathrm{log}[\\phi_y]\\big)}\\tag{PRUN.1}\n",
    "\\end{align*}$$\n",
    "\n",
    "since\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathrm{log}[p(x\\mid y=1)] &= \\mathrm{log}\\big[\\prod_{j=1}^{n}p(x_j\\mid y=1)\\big] \\\\\n",
    "              &= \\sum_{j=1}^{n}\\mathrm{log}[p(x_j\\mid y=1)] \\\\\n",
    "              &\\approx \\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=1}^{x_j}] \\\\\n",
    "              &= \\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=1}]x_j \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Let's check that this avoids underflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This produces the 'invalid value' warning due to underflow:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def generate_random_phi_js_yeq(std = .0001):\n",
    "    uf_a = np.maximum(1e-5, .00001*np.random.randn(8000)+.01) # words occurring rarely\n",
    "    uf_b = std * np.random.randn(500)+.2 # sometimes\n",
    "    uf_c = std * np.random.randn(500)+.5 # frequently\n",
    "    uf_d = std * np.random.randn(500)+.8 # very frequently\n",
    "    uf_e = np.minimum(100-(1e-7), .006*np.random.randn(500)+.99) # all the time\n",
    "    return np.concatenate((uf_a, uf_b, uf_c, uf_d, uf_e))\n",
    "\n",
    "phi_js_yeq1 = generate_random_phi_js_yeq()\n",
    "phi_js_yeq0 = generate_random_phi_js_yeq()\n",
    "\n",
    "py1 = 27/100\n",
    "py0 = 1 - py1\n",
    "phi_y = py1\n",
    "\n",
    "xa = np.random.choice(5, 8000, p=[.9, .06, .02, .015, .005])\n",
    "xb = np.random.choice(5, 500, p=[.7, .2, .05, .03, .02])\n",
    "xc = np.random.choice(5, 500, p=[.4, .3, .2, .07, .03])\n",
    "xd = np.random.choice(5, 500, p=[.1, .2, .3, .3, .1])\n",
    "xe = np.random.choice(5, 500, p=[.005, .185, .41, .25, .15])\n",
    "x = np.concatenate((xa, xb, xc, xd, xe))\n",
    "\n",
    "pxy1 = np.prod(np.power(phi_js_yeq1, x))\n",
    "pxy0 = np.prod(np.power(phi_js_yeq0, x))\n",
    "\n",
    "py1x_uf = lambda pxy0, py0, pxy1, py1: (pxy1 * py1) / (pxy1 * py1 + pxy0 * py0)\n",
    "\n",
    "py1x_uf(pxy0, py0, pxy1, py1)\n",
    "print(\"This produces the 'invalid value' warning due to underflow:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_clip_ufw = 0.2698\n",
      "log_probs=1.0134027135306503  ly1=-6810.92491742092  ly0=-6810.906137282534\n",
      "pred_avoid_uf = 0.2663\n"
     ]
    }
   ],
   "source": [
    "cl = lambda x, t=1e-320: np.clip(x, t, 1-t)\n",
    "py1x_cuf = lambda pxy0, py0, pxy1, py1: (cl(pxy1) * py1) / (cl(pxy1) * py1 + cl(pxy0) * py0)\n",
    "\n",
    "def py1x_auf(phi_js_yeq1, phi_js_yeq0, phi_y, x, prnt_log_probs=True):\n",
    "    log_phi_js_yeq1 = np.sum(np.log(phi_js_yeq1) * x)\n",
    "    log_phi_js_yeq0 = np.sum(np.log(phi_js_yeq0) * x)\n",
    "    log_probs = log_phi_js_yeq0 + np.log(1-phi_y) - log_phi_js_yeq1 - np.log(phi_y)\n",
    "    if prnt_log_probs:\n",
    "        print(\"log_probs={}  ly1={}  ly0={}\".format(log_probs, log_phi_js_yeq1, log_phi_js_yeq0))\n",
    "    return 1./(1 + np.exp(log_probs))\n",
    "\n",
    "pred_clip_ufw = py1x_cuf(pxy0, py0, pxy1, py1)\n",
    "print(\"pred_clip_ufw = %.4f\" % pred_clip_ufw)\n",
    "\n",
    "pred_avoid_uf = py1x_auf(phi_js_yeq1, phi_js_yeq0, phi_y, x)\n",
    "print(\"pred_avoid_uf = %.4f\" % pred_avoid_uf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Prediction\n",
    "\n",
    "Suppose our test matrix $X$ has $q$ observations. Then\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y\\mid X) &\\approx\\begin{bmatrix}\\frac{1}{1 + \\mathrm{exp}\\big(\\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=0}]x_j^{(1)} + \\mathrm{log}[1 - \\phi_y] - \\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=1}]x_j^{(1)} - \\mathrm{log}[\\phi_y]\\big)}\\\\\\vdots\\\\\\frac{1}{1 + \\mathrm{exp}\\big(\\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=0}]x_j^{(q)} + \\mathrm{log}[1 - \\phi_y] - \\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=1}]x_j^{(q)} - \\mathrm{log}[\\phi_y]\\big)}\\end{bmatrix}\\\\\\\\\n",
    "    &= 1 /^B (1 +^B \\exp(r))\n",
    "\\end{align*}$$\n",
    "\n",
    "where $/^B$ denotes divide broadcast, $+^B$ denotes plus broadcast, and $r$ is the vector of log propability ratios:\n",
    "\n",
    "$$\\begin{align*}\n",
    "r &\\equiv \\begin{bmatrix}\\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=0})x_j^{(1)} + \\mathrm{log}(1 - \\phi_y) - \\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=1})x_j^{(1)} - \\mathrm{log}(\\phi_y)\\\\\\vdots\\\\\\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=0})x_j^{(q)} + \\mathrm{log}(1 - \\phi_y) - \\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=1})x_j^{(q)} - \\mathrm{log}(\\phi_y)\\end{bmatrix} \\\\\\\\\n",
    " &= \\begin{bmatrix}\\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=0})x_j^{(1)}\\\\\\vdots\\\\\\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=0})x_j^{(q)}\\end{bmatrix}\n",
    " + \\begin{bmatrix}\\mathrm{log}(1 - \\phi_y)\\\\\\vdots\\\\\\mathrm{log}(1 - \\phi_y)\\end{bmatrix}\n",
    " + \\begin{bmatrix}-\\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=1})x_j^{(1)}\\\\\\vdots\\\\- \\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=1})x_j^{(q)}\\end{bmatrix}\n",
    " + \\begin{bmatrix}- \\mathrm{log}(\\phi_y)\\\\\\vdots\\\\- \\mathrm{log}(\\phi_y)\\end{bmatrix} \\\\\\\\\n",
    " &= \\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(1)}+\\dots+\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(1)}\\\\\\vdots\\\\\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(q)}+\\dots+\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(q)}\\end{bmatrix}\n",
    " +^B \\mathrm{log}(1 - \\phi_y) \n",
    " -\\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=1})x_1^{(1)}+\\dots+\\mathrm{log}(\\phi_{n\\mid y=1})x_n^{(1)}\\\\\\vdots\\\\\\mathrm{log}(\\phi_{1\\mid y=1})x_1^{(q)}+\\dots+\\mathrm{log}(\\phi_{n\\mid y=1})x_n^{(q)}\\end{bmatrix}\n",
    " -^B \\mathrm{log}(\\phi_y) \\\\\\\\\n",
    " &= {\\Large S}_{ax=1}\\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(1)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(1)}\\\\\\vdots&\\ddots&\\vdots\\\\\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(q)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(q)}\\end{bmatrix}\n",
    " +^B \\mathrm{log}(1 - \\phi_y) \n",
    " -{\\Large S}_{ax=1}\\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=1})x_1^{(1)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=1})x_n^{(1)}\\\\\\vdots&\\ddots&\\vdots\\\\\\mathrm{log}(\\phi_{1\\mid y=1})x_1^{(q)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=1})x_n^{(q)}\\end{bmatrix}\n",
    " -^B \\mathrm{log}(\\phi_y) \\\\\\\\\n",
    " &= {\\Large S}_{ax=1}L_0\n",
    " +^B \\mathrm{log}(1 - \\phi_y) \n",
    " -{\\Large S}_{ax=1}L_1\n",
    " -^B \\mathrm{log}(\\phi_y) \\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where ${\\large S}_{ax=1}L$ denotes the sum of the elements of the matrix $L$ across axis $1$ (i.e. sum the rows) and\n",
    "\n",
    "$$\\begin{align*}\n",
    "L_0 &\\equiv \\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(1)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(1)}\\\\\\vdots&\\ddots&\\vdots\\\\\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(q)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(q)}\\end{bmatrix} \\\\\\\\\n",
    "    &= \\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=0})&\\dots&\\mathrm{log}(\\phi_{n\\mid y=0})\\end{bmatrix}*_{R} \\begin{bmatrix}x_1^{(1)}&\\dots&x_n^{(1)}\\\\\\vdots&\\ddots&\\vdots\\\\x_1^{(q)}&\\dots&x_n^{(q)}\\end{bmatrix} \\\\\\\\\n",
    "    &= \\mathrm{log}\\big(\\begin{bmatrix}\\phi_{1\\mid y=0}&\\dots&\\phi_{n\\mid y=0}\\end{bmatrix}\\big)*_{R}X \\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where $v^T*_RX$ denotes multiplying elementwise the row vector $v^T$ by each row of matrix $X$. And similarly for $L_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phr='discount for coffee investors' class=1 prob=0.8613667133673921\n",
      "phr='discount for coffee for investors' class=1 prob=0.8893120505812462\n",
      "phr='a coffee is coffee' class=0 prob=0.02976942820520807\n",
      "phr='a coffee is free' class=0 prob=0.21639032410492626\n",
      "phr='investors coffee is free' class=1 prob=0.6743828824748668\n",
      "phr='investors coffee is discount' class=1 prob=0.6743828824748668\n",
      "phr='investors coffee is game' class=0 prob=0.31518227681947986\n",
      "phr='investors coffee get game' class=1 prob=0.648007587142401\n",
      "phr='investors get prices free for qualified investors' class=1 prob=0.9979108774702627\n",
      "phr='our coffee grab is lets coffee' class=0 prob=0.004257238584003545\n"
     ]
    }
   ],
   "source": [
    "def nb_test_ex(X, state):\n",
    "    output = np.zeros(X.shape[0], dtype=int)\n",
    "    phi_js_yeq1, phi_js_yeq0, phi_y = state['phi_js_yeq1'], state['phi_js_yeq0'], state['phi_y']\n",
    "\n",
    "    L1 = np.log(phi_js_yeq1) * X # (n,) * (X.shape[0], n) = (X.shape[0], n)\n",
    "    L0 = np.log(phi_js_yeq0) * X\n",
    "    log_phi_yeq1 = np.sum(L1, axis=1) # (X.shape[0],)\n",
    "    log_phi_yeq0 = np.sum(L0, axis=1)\n",
    "    r = log_phi_yeq0 + np.log(1-phi_y) - log_phi_yeq1 - np.log(phi_y)\n",
    "    probs = 1./(1 + np.exp(np.clip(r, -700, 700)))\n",
    "    \n",
    "    output[probs>.5] = 1\n",
    "    return output, probs\n",
    "\n",
    "phi_js_yeq1, phi_js_yeq0, phi_y = nb_train_ex(Xtr_1, ytr_1)\n",
    "state = {'phi_js_yeq1':phi_js_yeq1, 'phi_js_yeq0':phi_js_yeq0, 'phi_y':phi_y}\n",
    "preds, probs = nb_test_ex(parse_phrases(test_phrases, Voc1), state)\n",
    "for phr, cls, prob in zip(test_phrases, preds, probs):\n",
    "    print(\"phr='{}' class={} prob={}\".format(phr, cls, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the probability distribution of words is identical between classes, the NBC (Naive Bayes Classifier)    \n",
      "cannot classify any better than random:\n",
      "accuracy=0.517\n",
      "When the distributions are even slightly different between classes, the NBC can classify much better than random:\n",
      "accuracy=0.783\n",
      "When the distributions are even marginally different between classes, the NBC can classify perfectly:\n",
      "accuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "def generate_class_c_matrix(c=0, num_obs=5000):\n",
    "    Xc = []\n",
    "    for obs in range(num_obs):\n",
    "        xa = np.random.choice(5, 8000, p=[.9, .06, .02, .015, .005])\n",
    "        xb = np.random.choice(5, 500, p=[.7, .2, .05, .03, .02])\n",
    "        xc = np.random.choice(5, 500, p=[.4, .3, .2, .07, .03])\n",
    "        xd = np.random.choice(5, 500, p=[.005, .16, .39, .24, .205])\n",
    "        xe = np.random.choice(5, 500, p=[.005, .185, .41, .25, .15])\n",
    "        if c == 0:\n",
    "            x = np.concatenate((xa, xb, xc, xd, xe))\n",
    "        elif c == 1:\n",
    "            x = np.concatenate((xa, xb, xc, xe, xd))\n",
    "        elif c == 2:\n",
    "            x = np.concatenate((xa, xb, xe, xd, xc))\n",
    "        Xc.append(x)\n",
    "    return np.array(Xc, dtype=int)\n",
    "\n",
    "def gen_tr_te(oc=1):\n",
    "    X1 = generate_class_c_matrix(c=oc)\n",
    "    X0 = generate_class_c_matrix(c=0)\n",
    "    Xtr_2 = np.vstack((X1, X0))\n",
    "    ytr_2 = np.concatenate((np.full(5000, 1), np.full(5000, 0)))\n",
    "\n",
    "    X1 = generate_class_c_matrix(c=oc, num_obs=1000)\n",
    "    X0 = generate_class_c_matrix(c=0, num_obs=1000)\n",
    "    Xte_2 = np.vstack((X1, X0))\n",
    "    yte_2 = np.concatenate((np.full(1000, 1), np.full(1000, 0)))\n",
    "\n",
    "    phi_js_yeq1, phi_js_yeq0, phi_y = nb_train_ex(Xtr_2, ytr_2)\n",
    "    state = {'phi_js_yeq1':phi_js_yeq1, 'phi_js_yeq0':phi_js_yeq0, 'phi_y':phi_y}\n",
    "    preds, probs = nb_test_ex(Xte_2, state)\n",
    "    print(\"accuracy={}\".format(np.sum(preds==yte_2)/2000))\n",
    "\n",
    "print(\"When the probability distribution of words is identical between classes, the NBC (Naive Bayes Classifier)\\\n",
    "    \\ncannot classify any better than random:\")\n",
    "gen_tr_te(oc=0)\n",
    "print(\"When the distributions are even slightly different between classes, the NBC can classify much better than random:\")\n",
    "gen_tr_te(oc=1)\n",
    "print(\"When the distributions are even marginally different between classes, the NBC can classify perfectly:\")\n",
    "gen_tr_te(oc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See implementation in `nb.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types: tM=<class 'numpy.ndarray'>  tl=<class 'list'>  tle=<class 'str'>  tC=<class 'numpy.ndarray'>\n",
      "shapes: tM=(2144, 1448)  tl=1448  tC=(2144,)\n",
      "previews:\n",
      "tl=['abil', 'absolut', 'abus']\n",
      "tC=[1 0 1]\n"
     ]
    }
   ],
   "source": [
    "trainMatrix, tokenlist, trainCategory = nb.readMatrix('unzipped_spam_data/MATRIX.TRAIN')\n",
    "print(\"types: tM={}  tl={}  tle={}  tC={}\".format(type(trainMatrix), type(tokenlist), type(tokenlist[0]), type(trainCategory)))\n",
    "print(\"shapes: tM={}  tl={}  tC={}\".format(trainMatrix.shape, len(tokenlist), trainCategory.shape))\n",
    "print(\"previews:\\ntl={}\\ntC={}\".format(tokenlist[:3], trainCategory[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = nb.nb_train(trainMatrix, trainCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['phi_js_yeq1', 'phi_js_yeq0', 'phi_y'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['httpaddr', 'spam', 'unsubscrib', 'ebai', 'valet', 'diploma', 'dvd',\n",
       "       'websit', 'click', 'lowest', 'numberpx', 'arial', 'helvetica',\n",
       "       'serif', 'nashua', 'amherst', 'mortgag', 'refin', 'charset',\n",
       "       'newslett', 'customerservic', 'numberpt', 'iso', 'web', 'lender',\n",
       "       'numberd', 'loan', 'dailybargainmail', 'coral', 'html', 'unsolicit',\n",
       "       'www', 'fl', 'holidai', 'equiti', 'tal', 'consolid', 'bachelor',\n",
       "       'sweepstak', 'subscript', 'mba', 'bonu', 'ae', 'refinanc', 'hover',\n",
       "       'mime', 'untitl', 'subscrib', 'recur', 'postal'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = np.array(tokenlist)\n",
    "tokens[np.argsort(state['phi_js_yeq1'] / state['phi_js_yeq0'])[::-1]][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top words most indicative of spam emails, kind of makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.0387\n",
      "Error: 0.0262\n",
      "Error: 0.0262\n",
      "Error: 0.0187\n",
      "Error: 0.0175\n",
      "Error: 0.0163\n"
     ]
    }
   ],
   "source": [
    "files = sorted(glob.glob('./unzipped_spam_data/MATRIX.TRAIN.[0-9]*'), key=lambda s: int(s.rsplit('.')[-1]))\n",
    "\n",
    "nb_sizes = []\n",
    "nb_errs = []\n",
    "mat_test, tok_test, cat_test = nb.readMatrix('./unzipped_spam_data/MATRIX.TEST')\n",
    "for f in files:\n",
    "    mat, tok, cat = nb.readMatrix(f)\n",
    "    nb_sizes.append(mat.shape[0])\n",
    "    mod = nb.nb_train(mat, cat)\n",
    "    output = nb.nb_test(mat_test, mod)\n",
    "    nb_errs.append(nb.evaluate(output, cat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x113a8c978>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113a79828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nb_sizes, nb_errs)\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'svm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dc2dd6bff9e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'svm'"
     ]
    }
   ],
   "source": [
    "import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob('./spam_data/MATRIX.TRAIN.[0-9]*'), key=lambda s: int(s.rsplit('.')[-1]))\n",
    "\n",
    "svm_sizes = []\n",
    "svm_errs = []\n",
    "mat_test, tok_test, cat_test = svm.readMatrix('./spam_data/MATRIX.TEST')\n",
    "for f in files:\n",
    "    mat, tok, cat = svm.readMatrix(f)\n",
    "    svm_sizes.append(mat.shape[0])\n",
    "    mod = svm.svm_train(mat, cat)\n",
    "    output = svm.svm_test(mat_test, mod)\n",
    "    svm_errs.append(svm.evaluate(output, cat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(svm_sizes, svm_errs)\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nb_sizes, nb_errs, label='navie bayes')\n",
    "plt.plot(svm_sizes, svm_errs, label='svm')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM outperforms NB at each training set size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
