{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs229.stanford.edu/ps/ps2/ps2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple example to understand Multinomial Naive Bayes.\n",
    "\n",
    "| Text | Tag   |\n",
    "|:------|:------:|\n",
    "| investors! free for qualified investors &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Spam |\n",
    "| discount prices for game investors | Spam |\n",
    "| let's grab a coffee | Not spam |\n",
    "| investors get a free discount | Spam |\n",
    "| our coffee game is a game for | Not spam |\n",
    "\n",
    "Then our vocabulary, denoted by $V$, with an extra column for the number of times used, is\n",
    "\n",
    "| Index | Word | Times used   |\n",
    "|:------:|:------:|:------:|\n",
    "| &nbsp; 0 &nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 3 |\n",
    "| 1 | coffee | 2 |\n",
    "| 2 | discount | 2 |\n",
    "| 3 | for | 3 |\n",
    "| 4 | free | 2 |\n",
    "| 5 | game | 3 |\n",
    "| 6 | get | 1 |\n",
    "| 7 | grab | 1 |\n",
    "| 8 | investors | 4 |\n",
    "| 9 | is | 1 |\n",
    "| 10 | let's | 1 |\n",
    "| 11 | our | 1 |\n",
    "| 12 | prices | 1 |\n",
    "| 13 | qualified | 1 |\n",
    "\n",
    "But it's easier to code this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=\n",
      "[[0 0 0 1 1 0 0 0 2 0 0 0 0 1]\n",
      " [0 0 1 1 0 1 0 0 1 0 0 0 1 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 1 0 0 0]\n",
      " [1 0 1 0 1 0 1 0 1 0 0 0 0 0]\n",
      " [1 1 0 1 0 2 0 0 0 1 0 1 0 0]]\n",
      "\n",
      "V=\n",
      "0  a  3  Spam=1  NS=2\n",
      "1  coffee  2  Spam=0  NS=2\n",
      "2  discount  2  Spam=2  NS=0\n",
      "3  for  3  Spam=2  NS=1\n",
      "4  free  2  Spam=2  NS=0\n",
      "5  game  3  Spam=1  NS=2\n",
      "6  get  1  Spam=1  NS=0\n",
      "7  grab  1  Spam=0  NS=1\n",
      "8  investors  4  Spam=4  NS=0\n",
      "9  is  1  Spam=0  NS=1\n",
      "10  lets  1  Spam=0  NS=1\n",
      "11  our  1  Spam=0  NS=1\n",
      "12  prices  1  Spam=1  NS=0\n",
      "13  qualified  1  Spam=1  NS=0\n",
      "\n",
      "cnt_spam_words=15  cnt_ns_words=11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "V = {0:'a', 1:'coffee', 2:'discount', 3:'for', 4:'free', 5:'game', 6:'get', 7:'grab', \\\n",
    "     8:'investors', 9:'is', 10:'lets', 11:'our', 12:'prices', 13:'qualified'}\n",
    "\n",
    "X = np.array([[0,0,0,1,1,0,0,0,2,0,0,0,0,1],[0,0,1,1,0,1,0,0,1,0,0,0,1,0],[1,1,0,0,0,0,0,1,0,0,1,0,0,0],\\\n",
    "              [1,0,1,0,1,0,1,0,1,0,0,0,0,0],[1,1,0,1,0,2,0,0,0,1,0,1,0,0]])\n",
    "\n",
    "y = np.array([1,1,0,1,0]) # 1 denotes Spam, 0 denotes non-Spam\n",
    "\n",
    "Xs, Xn = X[y==1], X[y==0]\n",
    "\n",
    "print(\"X=\\n{}\".format(X))\n",
    "\n",
    "print(\"\\nV=\")\n",
    "\n",
    "for key, value in V.items():\n",
    "    tc, sc, nc = np.sum(X[:,key]), np.sum(Xs[:,key]), np.sum(Xn[:,key])\n",
    "    print(\"{}  {}  {}  Spam={}  NS={}\".format(key, value, tc, sc, nc))\n",
    "\n",
    "cnt_spam_words, cnt_ns_words = np.sum(Xs), np.sum(Xn)\n",
    "print(\"\\ncnt_spam_words={}  cnt_ns_words={}\".format(cnt_spam_words, cnt_ns_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note let's suppose we get a new phrase \"discount for coffee investors\" and we wish to classify it as spam or not. Using Bayes, we compute\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})=\\frac{P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})}{P(\\text{discount for coffee investors})}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})=\\frac{P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})}{P(\\text{discount for coffee investors})}\n",
    "\\end{align*}$$\n",
    "\n",
    "Then we pick the larger of the two. But we can discard the denominator because it is the same in both cases. Discarding this, we still pick the larger of the two:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})\\propto P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})\\tag{SE.1.1}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})\\propto P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})\\tag{SE.1.2}\n",
    "\\end{align*}$$\n",
    "\n",
    "Estimating the prior class probabilities is easy\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_c=\\frac{\\text{number of occurences of class }c\\text{ in training data}}{\\text{number of observations in training data}}\\approx P(c)\\tag{SE.1.3}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\phi_c$ denotes the estimated prior probability of class $c$. Then\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{Spam}=\\frac{3}{5}\\quad\\quad\\quad \\phi_{Not-Spam}=\\frac{2}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "But we have a problem. The phrase \"discount for coffee investors\" doesn't appear in our training data so the probabilities in SE.1.1 and SE.1.2 will both be zero. To circumvent this, we make the Naive Bayes Assumption: the words in each phrase are conditionally independent given the classfication. That is, we assume\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee investors}\\mid c)=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)P(\\text{coffee}\\mid c)P(\\text{investors}\\mid c)\\tag{SE.1.4}\n",
    "\\end{align*}$$\n",
    "\n",
    "We can estimate each of these probabilities. Let $N(w,c)$ denote the number of occurences of word $w$ for observations classified as $c$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=c}=\\frac{N(w_j,c)}{\\sum_{w\\in V}N(w,c)}\\approx P(w_j\\mid c)\\tag{SE.1.5}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\phi_{j\\mid y=c}$ denotes the estimated conditional probability that the word $w_j$ appears in an observation classified as $c$. For example:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Not-Spam})\\approx\\phi_{8\\mid y=0}=\\frac{N(w_8,0)}{\\sum_{w\\in V}N(w,0)}=\\frac{0}{11}=0\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Spam})\\approx\\phi_{8\\mid y=1}=\\frac{N(w_8,1)}{\\sum_{w\\in V}N(w,1)}=\\frac{4}{15}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Not-Spam})\\approx\\phi_{1\\mid y=0}=\\frac{N(w_1,0)}{\\sum_{w\\in V}N(w,0)}=\\frac{2}{11}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Spam})\\approx\\phi_{1\\mid y=1}=\\frac{N(w_1,1)}{\\sum_{w\\in V}N(w,1)}=\\frac{0}{15}=0\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Not-Spam})\\approx\\phi_{5\\mid y=0}=\\frac{N(w_5,0)}{\\sum_{w\\in V}N(w,0)}=\\frac{2}{11}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Spam})\\approx\\phi_{5\\mid y=1}=\\frac{N(w_5,1)}{\\sum_{w\\in V}N(w,1)}=\\frac{1}{15}\n",
    "\\end{align*}$$\n",
    "\n",
    "Now we have another problem. The word \"coffee\" doesn't appear in any training observations that are classified as spam. Hence $P(\\text{coffee}\\mid\\text{Spam})\\approx0$. This is a problem because in equation SE.1.4, this would give $P(\\text{discount for coffee investors}\\mid \\text{Spam})=0$. But we would obviously like to classify the phrase \"discount for coffee investors\" as spam.\n",
    "\n",
    "To get around this, we use Laplace Smoothing. We add $1$ to every count $N(w,c)$. Then SE.1.5 becomes\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(w_j\\mid c)\\approx\\phi_{j\\mid y=c}=\\frac{N(w_j,c)+1}{\\sum_{w\\in V}\\big(N(w,c)+1\\big)}=\\frac{N(w_j,c)+1}{\\sum_{w\\in V}N(w,c)+\\lvert V\\rvert}\\tag{SE.1.6}\n",
    "\\end{align*}$$\n",
    "\n",
    "And our examples become\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Not-Spam})\\approx\\phi_{8\\mid y=0}=\\frac{0+1}{11+14}=\\frac{1}{25}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Spam})\\approx\\phi_{8\\mid y=1}=\\frac{4+1}{15+14}=\\frac{5}{29}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Not-Spam})\\approx\\phi_{1\\mid y=0}=\\frac{2+1}{11+14}=\\frac{3}{25}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Spam})\\approx\\phi_{1\\mid y=1}=\\frac{0+1}{15+14}=\\frac{1}{29}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Not-Spam})\\approx\\phi_{5\\mid y=0}=\\frac{2+1}{11+14}=\\frac{3}{25}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Spam})\\approx\\phi_{5\\mid y=1}=\\frac{1+1}{15+14}=\\frac{2}{29}\n",
    "\\end{align*}$$\n",
    "\n",
    "Then our computations for SE.1.4 are\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee investors}\\mid\\text{Spam}) &= P(\\text{discount}\\mid\\text{Spam})P(\\text{for}\\mid \\text{Spam})P(\\text{coffee}\\mid\\text{Spam})P(\\text{investors}\\mid\\text{Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=1}\\phi_{3\\mid y=1}\\phi_{1\\mid y=1}\\phi_{8\\mid y=1}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee investors}\\mid\\text{Not-Spam}) &\\approx \\phi_{2\\mid y=0}\\phi_{3\\mid y=0}\\phi_{1\\mid y=0}\\phi_{8\\mid y=0}\n",
    "\\end{align*}$$\n",
    "\n",
    "Then we can compare\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})&\\propto P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=1}\\phi_{3\\mid y=1}\\phi_{1\\mid y=1}\\phi_{8\\mid y=1}\\frac{3}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})&\\propto P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=0}\\phi_{3\\mid y=0}\\phi_{1\\mid y=0}\\phi_{8\\mid y=0}\\frac{2}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "But of course this is easier with code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam=0.00003817  Not=0.00000614\n",
      "Spam=0.00003817  Not=0.00000614  ratio=6.21  \"discount for coffee investors\"  class=SPAM\n",
      "Spam=0.00000170  Not=0.00005530  ratio=32.59  \"a coffee is coffee\"  class=NOT\n",
      "Spam=0.00000509  Not=0.00001843  ratio=3.62  \"a coffee is free\"  class=NOT\n",
      "Spam=0.00001272  Not=0.00000614  ratio=2.07  \"investors coffee is free\"  class=SPAM\n",
      "Spam=0.00001272  Not=0.00000614  ratio=2.07  \"investors coffee is discount\"  class=SPAM\n",
      "Spam=0.00000848  Not=0.00001843  ratio=2.17  \"investors coffee is game\"  class=NOT\n",
      "Spam=0.00001697  Not=0.00000922  ratio=1.84  \"investors coffee get game\"  class=SPAM\n",
      "Spam=0.00000006  Not=0.00000000  ratio=477.67  \"investors get prices free for qualified investors\"  class=SPAM\n",
      "Spam=0.00000000  Not=0.00000024  ratio=233.89  \"our coffee grab is lets coffee\"  class=NOT\n",
      "TIME=0.007946014404296875 seconds\n",
      "s1=[ 0.00003817  0.0000017   0.00000509  0.00001272  0.00001272  0.00000848\n",
      "  0.00001697  0.00000006  0.        ]\n",
      "s0=[ 0.00000614  0.0000553   0.00001843  0.00000614  0.00000614  0.00001843\n",
      "  0.00000922  0.          0.00000024]\n",
      "rs=[   6.21327485   32.59150848    3.62127872    2.07109162    2.07109162\n",
      "    2.17276723    1.84097033  477.66986535  233.89404699]\n",
      "TIME=0.001417398452758789 seconds\n"
     ]
    }
   ],
   "source": [
    "Njc = lambda j,c: np.sum((Xs if c==1 else Xn)[:,j])\n",
    "\n",
    "phijc = lambda j,c: (Njc(j,c) + 1)/(np.sum([Njc(i,c) for i in range(len(V))]) + len(V))\n",
    "\n",
    "prob_sp_dfci = phijc(2,1) * phijc(3,1) * phijc(1,1) * phijc(8,1) * 3/5\n",
    "prob_ns_dfci = phijc(2,0) * phijc(3,0) * phijc(1,0) * phijc(8,0) * 2/5\n",
    "print(\"Spam={:.8f}  Not={:.8f}\".format(prob_sp_dfci, prob_ns_dfci))\n",
    "\n",
    "# to check more of these, it will be easier to use this approach:\n",
    "\n",
    "def get_V_key(value, V):\n",
    "    for key in V:\n",
    "        if V[key] == value: return key\n",
    "    return None\n",
    "\n",
    "def compute_probs(phrase, V, rnd=9):\n",
    "    js = [get_V_key(word, V) for word in phrase.split()]\n",
    "    prob_sp = np.sum(y==1)/5 * np.prod([phijc(j,1) for j in js])\n",
    "    prob_ns = np.sum(y==0)/5 * np.prod([phijc(j,0) for j in js])\n",
    "    return np.array([prob_sp, prob_ns])\n",
    "\n",
    "def classify(phrase, V):\n",
    "    cp = compute_probs(phrase, V)\n",
    "    ratio = cp[1] / cp[0] if cp[1] >= cp[0] else cp[0] / cp[1]\n",
    "    label = \"SPAM\" if cp[0] >= cp[1] else \"NOT\"\n",
    "    print(\"Spam={:.8f}  Not={:.8f}  ratio={:.2f}  \\\"{}\\\"  class={}\".format(cp[0], cp[1], ratio, phrase, label))\n",
    "\n",
    "phrases = ['discount for coffee investors',\n",
    "    'a coffee is coffee',\n",
    "    'a coffee is free',\n",
    "    'investors coffee is free',\n",
    "    'investors coffee is discount',\n",
    "    'investors coffee is game',\n",
    "    'investors coffee get game',\n",
    "    'investors get prices free for qualified investors',\n",
    "    'our coffee grab is lets coffee']\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for phr in phrases: classify(phr, V)\n",
    "end = time.time()\n",
    "print(\"TIME={} seconds\".format(end-start))\n",
    "    \n",
    "# but we can vectorize this\n",
    "\n",
    "def parse_phrases(phrases, V):\n",
    "    X = []\n",
    "    for phrase in phrases:\n",
    "        js = [get_V_key(word, V) for word in phrase.split()]\n",
    "        d = {j:js.count(j) for j in js}\n",
    "        x = np.zeros(len(V))\n",
    "        for k, v in d.items():\n",
    "            x[k] = v\n",
    "        X.append(x)\n",
    "    return np.array(X)\n",
    "\n",
    "def train_1(X, y, Voc):\n",
    "    X1, X0 = X[y==1], X[y==0]\n",
    "    phi_js_yeq1 = (np.sum(X1, axis=0) + 1) / (np.sum(X1) + len(Voc))\n",
    "    phi_js_yeq0 = (np.sum(X0, axis=0) + 1) / (np.sum(X0) + len(Voc))\n",
    "    phi_yeq1 = np.sum(y==1)/y.shape[0]\n",
    "    phi_yeq0 = np.sum(y==0)/y.shape[0]\n",
    "    return phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0\n",
    "\n",
    "def test_1(phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0, test_phrases, Voc):\n",
    "    Xtest = parse_phrases(test_phrases, Voc)\n",
    "    pX1 = np.power(phi_js_yeq1, Xtest)\n",
    "    pX0 = np.power(phi_js_yeq0, Xtest)\n",
    "    s1 = phi_yeq1 * np.prod(pX1, axis=1)\n",
    "    s0 = phi_yeq0 * np.prod(pX0, axis=1)\n",
    "    rs = s1/s0\n",
    "    rs[rs<1] = 1./rs[rs<1]\n",
    "    return s1, s0, rs\n",
    "\n",
    "def train_and_test_1(X, y, Voc, test_phrases):\n",
    "    phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0 = train_1(X, y, Voc)\n",
    "    return test_1(phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0, test_phrases, Voc)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "start = time.time()\n",
    "s1, s0, rs = train_and_test_1(X, y, V, phrases)\n",
    "print(\"s1={}\\ns0={}\\nrs={}\".format(s1.round(8), s0.round(8), rs))\n",
    "end = time.time()\n",
    "print(\"TIME={} seconds\".format(end-start))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Hence our ML estimates are\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=1} &= \\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =1 \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\} n_i + \\left| V \\right|} \\\\\\\\\n",
    "\\phi_{j\\mid y=0} &= \\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =0 \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 0 \\} n_i + \\left| V \\right|} \\\\\\\\\n",
    "\\phi_{y} &= \\frac{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\}}{m}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli NB Likelihood\n",
    "\n",
    "Let's derive the log-likelihood in the case of Bernoulli Naive Bayes.\n",
    "\n",
    "Let $y$ be a discrete random variable with values in $\\{0,1\\}$. Define $\\phi_y\\equiv P(y=1)=p_{y}(1)$.\n",
    "\n",
    "Let $x=[x_1,...,x_n]^{T}$ be a discrete random vector with values in $\\{0,1\\}^{n}$, where $n$ is the number of features. That is, an observation from $x$ is a column vector in $\\mathbb{R}^n$ whose elements are $0$ or $1$. Define\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\phi_{j\\mid y=1}\\equiv P(x_j=1\\mid y=1)=p_{x_j\\mid y}(1\\mid 1)\\\\\\\\\n",
    "\\phi_{j\\mid y=0}\\equiv P(x_j=1\\mid y=0)=p_{x_j\\mid y}(1\\mid 0)\n",
    "\\end{gather}$$\n",
    "\n",
    "Given a training set $\\{x^{(i)},y^{(i)}\\}_{i=1}^{m}$ where each $(x^{(i)},y^{(i)})$ is an independent observation from the joint random variable $(x,y)$, then the joint likelihood of the training data is\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\mathscr{L}\\big(\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y;\\{x^{(i)},y^{(i)}\\}_{i=1}^{m}\\big) &= P\\big((x,y=x^{(1)},y^{(1)}),...,(x,y=x^{(m)},y^{(m)});\\phi_{j|y=1},\\phi_{j|y=0},\\phi_y\\big)\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}P\\big(x,y=x^{(i)},y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big)\\tag{SCN.1}\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}p_{x,y}\\big(x^{(i)},y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big)\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}p_{x\\mid y}\\big(x^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0}\\big)p_{y}\\big(y^{(i)};\\phi_y\\big)\\tag{SCN.2}\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "SCN.1 holds from the assumption that observations are independent. SCN.2 holds from the definition of conditional probability. And the log-likelihood is\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\ell\\big(\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big) &= \\sum_{i=1}^m\\mathrm{ln}\\big[p_{x\\mid y}(x^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big]\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\mathrm{ln}\\Big[\\prod_{j=1}^{n}p_{x_{j}\\mid y}(x_j^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\Big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big]\\tag{SCN.3}\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\sum_{j=1}^{n}\\mathrm{ln}\\big[p_{x_{j}\\mid y}(x_j^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big] \\tag{SCN.4}\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\sum_{j=1}^{n}\\big\\{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}\\mathrm{ln}(\\phi_{j\\mid y=1})+\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=1\\}\\mathrm{ln}(\\phi_{j\\mid y=0}) \\\\\n",
    "     &+ \\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}\\mathrm{ln}(1-\\phi_{j\\mid y=1})+\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=0\\}\\mathrm{ln}(1-\\phi_{j|y=0})\\big\\} \\tag{SCN.5} \\\\\n",
    "     &+ \\sum_{i=1}^m\\big\\{\\boldsymbol{1}\\{y^{(i)}=1\\}\\mathrm{ln}(\\phi_y)+\\boldsymbol{1}\\{y^{(i)}=0\\}\\mathrm{ln}(1-\\phi_y)\\big\\}\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "SCN.3 follows from the Naive Bayes Assumption: $x_1^{(i)},...,x_n^{(i)}$ are conditionally independent given $y^{(i)}$. Intuitively, if we know whether an email is spam or not, then the appearance of different words is independent. For example, if we know that an email is spam, then the appearances of \"free\" and \"discount\" are independent events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli NB MLE\n",
    "\n",
    "To maximize the log-likelihood in $\\phi_y$, let's compute the partial derivative of SCN.5 with respect to $\\phi_y$ and set it equal to zero:\n",
    "\n",
    "$$\\begin{align*} \n",
    "0=\\frac{\\partial\\ell}{\\partial\\phi_y} &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\frac1{\\phi_y} - \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}}\\frac1{1-\\phi_y}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}}\\frac1{1-\\phi_y} = \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\frac1{\\phi_y}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\frac1{1-\\phi_y}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}} = \\frac1{\\phi_y}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Multiplying both sides by $\\phi_y(1-\\phi_y)$, we get\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}} &= (1-\\phi_y)\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\\\\n",
    "    &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}} - \\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}} &= \\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}} + \\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\\\\n",
    "    &= \\phi_y\\sum_{i=1}^m\\big[\\boldsymbol{1}\\{y^{(i)}=0\\} + \\boldsymbol{1}\\{y^{(i)}=1\\}\\big]\\\\\n",
    "    &= \\phi_y\\sum_{i=1}^m1\\\\\n",
    "    &= \\phi_y m\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_y = \\frac1m\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can maximize in $\\phi_{j|y=1}$:\n",
    "\n",
    "$$\\begin{align*} \n",
    "0=\\frac{\\partial\\ell}{\\partial\\phi_{j|y=1}} &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\frac1{\\phi_{j|y=1}} - \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}}\\frac1{1-\\phi_{j|y=1}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}}\\frac1{1-\\phi_{j|y=1}} = \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\frac1{\\phi_{j|y=1}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\frac1{1-\\phi_{j|y=1}}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}} = \\frac1{\\phi_{j|y=1}}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Multiplying both sides by $\\phi_{j|y=1}(1-\\phi_{j|y=1})$, we get\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}} &= (1-\\phi_{j|y=1})\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\\\\n",
    "    &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}} - \\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}} &= \\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}} + \\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\\\\n",
    "    &= \\phi_{j|y=1}\\sum_{i=1}^m\\big[\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\} + \\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}\\big]\\\\\n",
    "    &= \\phi_{j|y=1}\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j|y=1} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}}\n",
    "\\end{align*}$$\n",
    "\n",
    "Similarly we can compute\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j|y=0} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=1\\}}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=0\\}}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's suppose that $x=[x_1,...,x_n]^T$ is discrete and $x_j$ takes values in the nonnegative integers $0,1,2,...$. And let's define\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\phi_{j\\mid y=1}\\equiv P(x_j=1\\mid y=1)=p_{x_j\\mid y}(1\\mid 1)\\\\\\\\\n",
    "\\phi_{j\\mid y=0}\\equiv P(x_j=1\\mid y=0)=p_{x_j\\mid y}(1\\mid 0)\n",
    "\\end{gather}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=1} &= \\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =1 \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\} n_i + \\left| V \\right|} \\\\\\\\\n",
    "\\phi_{y} &= \\frac{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\}}{m}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Navie Bayes represented as a multinomial event model, to maximize its maxmimum likelihood with Laplace smoothing,\n",
    "\n",
    "\\begin{align*}\n",
    "\\phi_{k|y=1} &= \\frac{\\sum_{i=1}^{m} \\sum_{j=1}^{n_i} \\boldsymbol{1} \\{ x_j^{(i)} = k \\wedge y^{(i)} =1 \\} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\} n_i + \\left| V \\right|} \\\\\n",
    "\\phi_{k|y=0} &= \\frac{\\sum_{i=1}^{m} \\sum_{j=1}^{n_i} \\boldsymbol{1} \\{ x_j^{(i)} = k \\wedge y^{(i)} =0 \\} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 0 \\} n_i + \\left| V \\right|} \\\\\n",
    "\\phi_{y} &= \\frac{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\}}{m}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction,\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y=1|x) &= \\frac{p(x|y=1)p(y=1)}{p(x)} \\\\\n",
    "              &= \\frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1) + p(x|y=0) p(y=0)} \\\\\n",
    "              &= \\frac{1}{1 + \\frac{p(x|y=0) p(y=0)}{p(x|y=1) p(y=1)}} \\\\\n",
    "              &= \\frac{1}{1 + \\mathrm{exp}\\big[\\mathrm{log}P(x|y=0) + \\mathrm{log}P(y=0) - \\mathrm{log}P(x|y=1) - \\mathrm{log} P(y=1)\\big]}\n",
    "\\end{align*}$$\n",
    "\n",
    "The last two equalities are for numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See implementation in `nb.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nb\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types: tM=<class 'numpy.ndarray'>  tl=<class 'list'>  tle=<class 'str'>  tC=<class 'numpy.ndarray'>\n",
      "shapes: tM=(2144, 1448)  tl=1448  tC=(2144,)\n",
      "previews:\n",
      "tl=['abil', 'absolut', 'abus']\n",
      "tC=[1 0 1]\n"
     ]
    }
   ],
   "source": [
    "trainMatrix, tokenlist, trainCategory = nb.readMatrix('unzipped_spam_data/MATRIX.TRAIN')\n",
    "print(\"types: tM={}  tl={}  tle={}  tC={}\".format(type(trainMatrix), type(tokenlist), type(tokenlist[0]), type(trainCategory)))\n",
    "print(\"shapes: tM={}  tl={}  tC={}\".format(trainMatrix.shape, len(tokenlist), trainCategory.shape))\n",
    "print(\"previews:\\ntl={}\\ntC={}\".format(tokenlist[:3], trainCategory[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = nb.nb_train(trainMatrix, trainCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'phi_yeq1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f5de19b3476a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phi_yeq1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phi_yeq0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'phi_yeq1'"
     ]
    }
   ],
   "source": [
    "tokens = np.array(tokenlist)\n",
    "tokens[np.argsort(state['phi_yeq1'] / state['phi_yeq0'])[::-1]][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top words most indicative of spam emails, kind of makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob('./spam_data/MATRIX.TRAIN.[0-9]*'), key=lambda s: int(s.rsplit('.')[-1]))\n",
    "\n",
    "nb_sizes = []\n",
    "nb_errs = []\n",
    "mat_test, tok_test, cat_test = nb.readMatrix('./spam_data/MATRIX.TEST')\n",
    "for f in files:\n",
    "    mat, tok, cat = nb.readMatrix(f)\n",
    "    nb_sizes.append(mat.shape[0])\n",
    "    mod = nb.nb_train(mat, cat)\n",
    "    output = nb.nb_test(mat_test, mod)\n",
    "    nb_errs.append(nb.evaluate(output, cat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nb_sizes, nb_errs)\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob('./spam_data/MATRIX.TRAIN.[0-9]*'), key=lambda s: int(s.rsplit('.')[-1]))\n",
    "\n",
    "svm_sizes = []\n",
    "svm_errs = []\n",
    "mat_test, tok_test, cat_test = svm.readMatrix('./spam_data/MATRIX.TEST')\n",
    "for f in files:\n",
    "    mat, tok, cat = svm.readMatrix(f)\n",
    "    svm_sizes.append(mat.shape[0])\n",
    "    mod = svm.svm_train(mat, cat)\n",
    "    output = svm.svm_test(mat_test, mod)\n",
    "    svm_errs.append(svm.evaluate(output, cat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(svm_sizes, svm_errs)\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nb_sizes, nb_errs, label='navie bayes')\n",
    "plt.plot(svm_sizes, svm_errs, label='svm')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM outperforms NB at each training set size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
