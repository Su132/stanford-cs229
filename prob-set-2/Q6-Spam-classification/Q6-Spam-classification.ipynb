{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "\n",
    "import nb\n",
    "import svm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs229.stanford.edu/ps/ps2/ps2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction & Example\n",
    "\n",
    "Let's start with a simple example to understand Multinomial Naive Bayes. Suppose we want to build a learning algorithm to classify email as spam or not. And suppose we are given training data:\n",
    "\n",
    "| Email | Tag   |\n",
    "|:------|:------:|\n",
    "| investors! free for qualified investors &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Spam |\n",
    "| discount prices for game investors | Spam |\n",
    "| lets grab a coffee | Not spam |\n",
    "| investors get a free discount | Spam |\n",
    "| our coffee game is a game for | Not spam |\n",
    "\n",
    "Then our vocabulary, denoted by $V$, with an extra column for the number of times used, is\n",
    "\n",
    "| Index | Word | Times used   |\n",
    "|:------:|:------:|:------:|\n",
    "| &nbsp; 0 &nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; a &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 3 |\n",
    "| 1 | coffee | 2 |\n",
    "| 2 | discount | 2 |\n",
    "| 3 | for | 3 |\n",
    "| 4 | free | 2 |\n",
    "| 5 | game | 3 |\n",
    "| 6 | get | 1 |\n",
    "| 7 | grab | 1 |\n",
    "| 8 | investors | 4 |\n",
    "| 9 | is | 1 |\n",
    "| 10 | lets | 1 |\n",
    "| 11 | our | 1 |\n",
    "| 12 | prices | 1 |\n",
    "| 13 | qualified | 1 |\n",
    "\n",
    "But it's easier to code this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voc1={0: 'a', 1: 'coffee', 2: 'discount', 3: 'for', 4: 'free', 5: 'game', 6: 'get', 7: 'grab', 8: 'investors', 9: 'is', 10: 'lets', 11: 'our', 12: 'prices', 13: 'qualified'}\n",
      "\n",
      "Xtr_1=\n",
      "[[0 0 0 1 1 0 0 0 2 0 0 0 0 1]\n",
      " [0 0 1 1 0 1 0 0 1 0 0 0 1 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 1 0 0 0]\n",
      " [1 0 1 0 1 0 1 0 1 0 0 0 0 0]\n",
      " [1 1 0 1 0 2 0 0 0 1 0 1 0 0]]\n",
      "\n",
      "Occurrence count=\n",
      "Ind  Word         Total Spam  Not\n",
      "---------------------------------\n",
      "0    a               3    1    2\n",
      "1    coffee          2    0    2\n",
      "2    discount        2    2    0\n",
      "3    for             3    2    1\n",
      "4    free            2    2    0\n",
      "5    game            3    1    2\n",
      "6    get             1    1    0\n",
      "7    grab            1    0    1\n",
      "8    investors       4    4    0\n",
      "9    is              1    0    1\n",
      "10   lets            1    0    1\n",
      "11   our             1    0    1\n",
      "12   prices          1    1    0\n",
      "13   qualified       1    1    0\n",
      "\n",
      "cnt_spam_words=15  cnt_ns_words=11\n"
     ]
    }
   ],
   "source": [
    "train_emails = ['investors free for qualified investors',\n",
    "                'discount prices for game investors',\n",
    "                'lets grab a coffee',\n",
    "                'investors get a free discount',\n",
    "                'our coffee game is a game for']\n",
    "\n",
    "def make_vocab(train_emails):\n",
    "    all_words = []\n",
    "    for email in train_emails:\n",
    "        word_list = email.split()\n",
    "        for w in word_list: all_words.append(w)\n",
    "    auw = list(set(all_words)) # removes dups\n",
    "    auw.sort()\n",
    "    return {i:w for i,w in zip(range(len(auw)), auw)}\n",
    "\n",
    "Voc1 = make_vocab(train_emails)\n",
    "print(\"Voc1={0}\\n\".format(Voc1))\n",
    "\n",
    "def get_V_key(value, V):\n",
    "    for key in V:\n",
    "        if V[key] == value: return key\n",
    "    return None\n",
    "\n",
    "def parse_emails(emails, V):\n",
    "    X = []\n",
    "    for email in emails:\n",
    "        js = [get_V_key(word, V) for word in email.split()]\n",
    "        d = {j:js.count(j) for j in js}\n",
    "        x = np.zeros(len(V))\n",
    "        for k, v in d.items():\n",
    "            x[k] = v\n",
    "        X.append(x)\n",
    "    return np.array(X, dtype=int)\n",
    "\n",
    "Xtr_1 = parse_emails(train_emails, Voc1)\n",
    "\n",
    "ytr_1 = np.array([1,1,0,1,0]) # 1 denotes Spam, 0 denotes non-Spam\n",
    "\n",
    "Xtr_1s, Xtr_1n = Xtr_1[ytr_1==1], Xtr_1[ytr_1==0]\n",
    "\n",
    "print(\"Xtr_1=\\n{}\".format(Xtr_1))\n",
    "\n",
    "print(\"\\nOccurrence count=\")\n",
    "\n",
    "print(\"%-4s %-12s %-4s %-4s %4s\" % (\"Ind\", \"Word\", \"Total\", \"Spam\", \"Not\"))\n",
    "print(\"---------------------------------\")\n",
    "for key, value in Voc1.items():\n",
    "    tc, sc, nc = np.sum(Xtr_1[:,key]), np.sum(Xtr_1s[:,key]), np.sum(Xtr_1n[:,key])\n",
    "    print(\"%-4.0f %-12s %4.0f %4.0f %4.0f\" % (key, value, tc, sc, nc))\n",
    "\n",
    "cnt_spam_words, cnt_ns_words = np.sum(Xtr_1s), np.sum(Xtr_1n)\n",
    "print(\"\\ncnt_spam_words={}  cnt_ns_words={}\".format(cnt_spam_words, cnt_ns_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Multinomial\n",
    "\n",
    "Note let's suppose we get a new email \"discount for coffee investors\" and we wish to classify it as spam or not. Using Bayes, we compute\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})=\\frac{P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})}{P(\\text{discount for coffee investors})}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})=\\frac{P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})}{P(\\text{discount for coffee investors})}\n",
    "\\end{align*}$$\n",
    "\n",
    "Then we pick the larger of the two. But we can discard the denominator because it is the same in both cases. Discarding this, we still pick the larger of the two:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})\\propto P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})\\tag{SE.1}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})\\propto P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})\\tag{SE.2}\n",
    "\\end{align*}$$\n",
    "\n",
    "Estimating the prior class probabilities is easy. Define\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_c\\equiv\\frac{\\text{number of occurences of class }c\\text{ in training data}}{\\text{number of observations in training data}}\\approx P(c)\\tag{SE.3}\n",
    "\\end{align*}$$\n",
    "\n",
    "so that $\\phi_c$ denotes the estimated prior probability of class $c$. Then\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{Spam}=\\frac{3}{5}\\quad\\quad\\quad \\phi_{Not-Spam}=\\frac{2}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "But we have a problem. The email \"discount for coffee investors\" doesn't appear in our training data. So if we try to estimate the probability $P(\\text{discount for coffee investors}\\mid\\text{Spam})$ with its relative frequency in the training data, then its estimated probability will be $0$, and the probabilities in SE.1 and SE.2 will both be zero.\n",
    "\n",
    "To circumvent this problem, we make the Naive Bayes Assumption: the words in each email are conditionally independent given the classfication. That is, we assume\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee investors}\\mid c)=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)P(\\text{coffee}\\mid c)P(\\text{investors}\\mid c)\\tag{SE.4}\n",
    "\\end{align*}$$\n",
    "\n",
    "That is, we model an email, given its class, as a multinomial random variable with the multinomial coefficient omitted (this is further discussed below). Hence each word in an email is an IID experiment. Hence $P(\\text{for}\\mid c)$ denotes the probability of the word \"for\" appearing as the second word, or as the third word, or as the $19^{th}$ word, or as any particular word. If \"for\" were to appear twice in an email, then we would have\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee for investors}\\mid c) &=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)P(\\text{coffee}\\mid c)P(\\text{for}\\mid c)P(\\text{investors}\\mid c)\\\\\n",
    "     &=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)^{2}P(\\text{coffee}\\mid c)P(\\text{investors}\\mid c)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Similarly, if \"for\" were to appear three times in an email, then we would have\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee for investors for}\\mid c) &=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)P(\\text{coffee}\\mid c)P(\\text{for}\\mid c)P(\\text{investors}\\mid c)P(\\text{for}\\mid c)\\\\\n",
    "     &=P(\\text{discount}\\mid c)P(\\text{for}\\mid c)^{3}P(\\text{coffee}\\mid c)P(\\text{investors}\\mid c)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Let $P(w_j\\mid c)$ denote the probability that the vocabulary word with index $j$ appears as any particular word in an email classified as $c$. And let $N(w,c)$ denote the number of occurrences of word $w$ in any email in the training data classified as $c$. Intuitively, using the maximum likelihood principal (or the intrepretation of probability as a relative frequncy), we can estimate the probability $P(w_j\\mid c)$ as\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=c}\\equiv\\hat{P}(w_j\\mid c)\\equiv\\frac{N(w_j,c)}{\\sum_{w\\in V}N(w,c)}\\approx P(w_j\\mid c)\\tag{SE.5}\n",
    "\\end{align*}$$\n",
    "\n",
    "so that $\\phi_{j\\mid y=c}$ or $\\hat{P}(w_j\\mid c)$ denote the estimated conditional probability that the vocabulary word $w_j$ appears as any particular word in an observed email classified as $c$. For example:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Not-Spam})\\approx\\phi_{8\\mid y=0}=\\frac{N(w_8,0)}{\\sum_{w\\in V}N(w,0)}=\\frac{0}{11}=0\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Spam})\\approx\\phi_{8\\mid y=1}=\\frac{N(w_8,1)}{\\sum_{w\\in V}N(w,1)}=\\frac{4}{15}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Not-Spam})\\approx\\phi_{1\\mid y=0}=\\frac{N(w_1,0)}{\\sum_{w\\in V}N(w,0)}=\\frac{2}{11}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Spam})\\approx\\phi_{1\\mid y=1}=\\frac{N(w_1,1)}{\\sum_{w\\in V}N(w,1)}=\\frac{0}{15}=0\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Not-Spam})\\approx\\phi_{5\\mid y=0}=\\frac{N(w_5,0)}{\\sum_{w\\in V}N(w,0)}=\\frac{2}{11}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Spam})\\approx\\phi_{5\\mid y=1}=\\frac{N(w_5,1)}{\\sum_{w\\in V}N(w,1)}=\\frac{1}{15}\n",
    "\\end{align*}$$\n",
    "\n",
    "Now we have another problem. The word \"coffee\" doesn't appear in any training observations that are classified as spam. Hence $\\phi_{1\\mid y=1}=0\\approx P(\\text{coffee}\\mid\\text{Spam})$. This is a problem because in equation SE.4, this would give $P(\\text{discount for coffee investors}\\mid \\text{Spam})=0$. But we would obviously like to classify the email \"discount for coffee investors\" as spam.\n",
    "\n",
    "To get around this, we use Laplace Smoothing. We add $1$ to every count $N(w,c)$. Then SE.5 becomes\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(w_j\\mid c)\\approx\\phi_{j\\mid y=c}=\\frac{N(w_j,c)+1}{\\sum_{w\\in V}\\big(N(w,c)+1\\big)}=\\frac{N(w_j,c)+1}{\\sum_{w\\in V}N(w,c)+\\lvert V\\rvert}\\tag{SE.6}\n",
    "\\end{align*}$$\n",
    "\n",
    "Besides helping us avoid zero probability estimates, Laplace Smoothing is intuitive and realistic. That is, it is not impossible that the word \"coffee\" will appear in test (or real-world) spam emails, simply because it didn't appear in our spam-labeled training data.\n",
    "\n",
    "Then our examples become\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Not-Spam})\\approx\\phi_{8\\mid y=0}=\\frac{0+1}{11+14}=\\frac{1}{25}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{investors}\\mid\\text{Spam})\\approx\\phi_{8\\mid y=1}=\\frac{4+1}{15+14}=\\frac{5}{29}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Not-Spam})\\approx\\phi_{1\\mid y=0}=\\frac{2+1}{11+14}=\\frac{3}{25}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{coffee}\\mid\\text{Spam})\\approx\\phi_{1\\mid y=1}=\\frac{0+1}{15+14}=\\frac{1}{29}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Not-Spam})\\approx\\phi_{5\\mid y=0}=\\frac{2+1}{11+14}=\\frac{3}{25}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{game}\\mid\\text{Spam})\\approx\\phi_{5\\mid y=1}=\\frac{1+1}{15+14}=\\frac{2}{29}\n",
    "\\end{align*}$$\n",
    "\n",
    "Then our computations for SE.4 are\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee investors}\\mid\\text{Spam}) &= P(\\text{discount}\\mid\\text{Spam})P(\\text{for}\\mid \\text{Spam})P(\\text{coffee}\\mid\\text{Spam})P(\\text{investors}\\mid\\text{Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=1}\\phi_{3\\mid y=1}\\phi_{1\\mid y=1}\\phi_{8\\mid y=1}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{discount for coffee investors}\\mid\\text{Not-Spam}) &\\approx \\phi_{2\\mid y=0}\\phi_{3\\mid y=0}\\phi_{1\\mid y=0}\\phi_{8\\mid y=0}\n",
    "\\end{align*}$$\n",
    "\n",
    "Then we can compare\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})&\\propto P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=1}\\phi_{3\\mid y=1}\\phi_{1\\mid y=1}\\phi_{8\\mid y=1}\\frac{3}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})&\\propto P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=0}\\phi_{3\\mid y=0}\\phi_{1\\mid y=0}\\phi_{8\\mid y=0}\\frac{2}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "But of course this is easier with code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam=0.00003817  Not=0.00000614 \"discount for coffee investors\"\n",
      "Spam=0.00000395  Not=0.00000049 \"discount for coffee for investors\"\n",
      "Spam=0.00003817  Not=0.00000614  ratio=6.21  \"discount for coffee investors\"  class=SPAM\n",
      "Spam=0.00000395  Not=0.00000049  ratio=8.03  \"discount for coffee for investors\"  class=SPAM\n",
      "Spam=0.00000170  Not=0.00005530  ratio=32.59  \"a coffee is coffee\"  class=NOT\n",
      "Spam=0.00000509  Not=0.00001843  ratio=3.62  \"a coffee is free\"  class=NOT\n",
      "Spam=0.00001272  Not=0.00000614  ratio=2.07  \"investors coffee is free\"  class=SPAM\n",
      "Spam=0.00001272  Not=0.00000614  ratio=2.07  \"investors coffee is discount\"  class=SPAM\n",
      "Spam=0.00000848  Not=0.00001843  ratio=2.17  \"investors coffee is game\"  class=NOT\n",
      "Spam=0.00001697  Not=0.00000922  ratio=1.84  \"investors coffee get game\"  class=SPAM\n",
      "Spam=0.00000006  Not=0.00000000  ratio=477.67  \"investors get prices free for qualified investors\"  class=SPAM\n",
      "Spam=0.00000000  Not=0.00000024  ratio=233.89  \"our coffee grab is lets coffee\"  class=NOT\n",
      "TIME=0.008636951446533203 seconds\n",
      "s1=[ 0.00003817  0.00000395  0.0000017   0.00000509  0.00001272  0.00001272\n",
      "  0.00000848  0.00001697  0.00000006  0.        ]\n",
      "s0=[ 0.00000614  0.00000049  0.0000553   0.00001843  0.00000614  0.00000614\n",
      "  0.00001843  0.00000922  0.          0.00000024]\n",
      "rs=[   6.21327485    8.03440714   32.59150848    3.62127872    2.07109162\n",
      "    2.07109162    2.17276723    1.84097033  477.66986535  233.89404699]\n",
      "TIME=0.0017209053039550781 seconds\n"
     ]
    }
   ],
   "source": [
    "Njc = lambda j, Xc: np.sum(Xc[:,j])\n",
    "\n",
    "phijc = lambda j, Xc, V: (Njc(j, Xc) + 1)/(np.sum([Njc(i, Xc) for i in range(len(V))]) + len(V))\n",
    "\n",
    "prob_sp_dfci = phijc(2,Xtr_1s,Voc1) * phijc(3,Xtr_1s,Voc1) * phijc(1,Xtr_1s,Voc1) * phijc(8,Xtr_1s,Voc1) * 3/5\n",
    "prob_ns_dfci = phijc(2,Xtr_1n,Voc1) * phijc(3,Xtr_1n,Voc1) * phijc(1,Xtr_1n,Voc1) * phijc(8,Xtr_1n,Voc1) * 2/5\n",
    "print(\"Spam={:.8f}  Not={:.8f} \\\"discount for coffee investors\\\"\".format(prob_sp_dfci, prob_ns_dfci))\n",
    "\n",
    "prob_sp_dfci = phijc(2,Xtr_1s,Voc1) * np.power(phijc(3,Xtr_1s,Voc1), 2) * phijc(1,Xtr_1s,Voc1) * phijc(8,Xtr_1s,Voc1) * 3/5\n",
    "prob_ns_dfci = phijc(2,Xtr_1n,Voc1) * np.power(phijc(3,Xtr_1n,Voc1), 2) * phijc(1,Xtr_1n,Voc1) * phijc(8,Xtr_1n,Voc1) * 2/5\n",
    "print(\"Spam={:.8f}  Not={:.8f} \\\"discount for coffee for investors\\\"\".format(prob_sp_dfci, prob_ns_dfci))\n",
    "\n",
    "# to check more of these, it will be easier to use this approach:\n",
    "\n",
    "def compute_probs(email, X1, X0, y, V):\n",
    "    js = [get_V_key(word, V) for word in email.split()]\n",
    "    prob_sp = np.sum(y==1) / y.shape[0] * np.prod([phijc(j, X1, V) for j in js])\n",
    "    prob_ns = np.sum(y==0) / y.shape[0] * np.prod([phijc(j, X0, V) for j in js])\n",
    "    return np.array([prob_sp, prob_ns])\n",
    "\n",
    "def classify(email, X, y, V):\n",
    "    X1, X0 = X[y==1], X[y==0]\n",
    "    cp = compute_probs(email, X1, X0, y, V)\n",
    "    ratio = cp[1] / cp[0] if cp[1] >= cp[0] else cp[0] / cp[1]\n",
    "    label = \"SPAM\" if cp[0] >= cp[1] else \"NOT\"\n",
    "    print(\"Spam={:.8f}  Not={:.8f}  ratio={:.2f}  \\\"{}\\\"  class={}\".format(cp[0], cp[1], ratio, email, label))\n",
    "\n",
    "test_emails = ['discount for coffee investors',\n",
    "    'discount for coffee for investors',\n",
    "    'a coffee is coffee',\n",
    "    'a coffee is free',\n",
    "    'investors coffee is free',\n",
    "    'investors coffee is discount',\n",
    "    'investors coffee is game',\n",
    "    'investors coffee get game',\n",
    "    'investors get prices free for qualified investors',\n",
    "    'our coffee grab is lets coffee']\n",
    "\n",
    "start = time.time()\n",
    "for email in test_emails: classify(email, Xtr_1, ytr_1, Voc1)\n",
    "end = time.time()\n",
    "print(\"TIME={} seconds\".format(end-start))\n",
    "    \n",
    "# but we can vectorize this\n",
    "\n",
    "def train_1(X, y, V):\n",
    "    X1, X0 = X[y==1], X[y==0]\n",
    "    phi_js_yeq1 = (np.sum(X1, axis=0) + 1) / (np.sum(X1) + len(V))\n",
    "    phi_js_yeq0 = (np.sum(X0, axis=0) + 1) / (np.sum(X0) + len(V))\n",
    "    phi_yeq1 = np.sum(y==1) / y.shape[0]\n",
    "    phi_yeq0 = np.sum(y==0) / y.shape[0]\n",
    "    return phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0\n",
    "\n",
    "def predict_1(phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0, V, test_emails):\n",
    "    Xtest = parse_emails(test_emails, V)\n",
    "    pX1 = np.power(phi_js_yeq1, Xtest)\n",
    "    pX0 = np.power(phi_js_yeq0, Xtest)\n",
    "    s1 = phi_yeq1 * np.prod(pX1, axis=1)\n",
    "    s0 = phi_yeq0 * np.prod(pX0, axis=1)\n",
    "    rs = s1/s0\n",
    "    rs[rs<1] = 1./rs[rs<1]\n",
    "    return s1, s0, rs\n",
    "\n",
    "def train_and_predict_1(X, y, V, test_emails):\n",
    "    phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0 = train_1(X, y, V)\n",
    "    return predict_1(phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0, V, test_emails)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "start = time.time()\n",
    "s1, s0, rs = train_and_predict_1(Xtr_1, ytr_1, Voc1, test_emails)\n",
    "print(\"s1={}\\ns0={}\\nrs={}\".format(s1.round(8), s0.round(8), rs))\n",
    "end = time.time()\n",
    "print(\"TIME={} seconds\".format(end-start))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes is Naive and Generative\n",
    "\n",
    "Intuitively, if we know whether an email is spam or not, then the appearance of different words is assumed to be independent. For example, if we know that an email is spam, then the appearances of \"price\" and \"discount\" are assumed to be independent events. This example emphasizes the general inaccuracy of this assumption and explains \"Naive\" in the name. If we know the email is spam and we know that \"price\" appeared, then it seems more likely that \"discount\" also appeared. Nevertheless, NB can give good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formalize Multinomial Estimations\n",
    "\n",
    "Now let's formalize and clearly write out a few items from above. Recall SE.6:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=c}\\equiv\\frac{N(w_j,c)+1}{\\sum_{w\\in V}\\big(N(w,c)+1\\big)}=\\frac{N(w_j,c)+1}{\\sum_{w\\in V}N(w,c)+\\lvert V\\rvert}\\approx P(w_j\\mid c)\\tag{SE.6}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $N(w,c)$ denotes the number of occurences of word $w$ for observations classified as $c$. If we let $\\phi_j\\approx P(w_j)$ denote the estimated probability that word $j$ appears in the training data regardless of class, then\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j}\\equiv\\frac{\\sum_{i=1}^{m}x_j^{(i)}}{\\sum_{i=1}^{m}n_i}\\approx P(w_j)\\tag{SE.7}\n",
    "\\end{align*}$$\n",
    "\n",
    "where\n",
    "\n",
    "- $m$ is the number of training observations\n",
    "- $x^{(i)}\\in\\mathbb{R}^n$ denotes the $i^{th}$ training observation\n",
    "- $x_j^{(i)}$ denotes the $j^{th}$ element of the $i^{th}$ training observation\n",
    "- $n_i$ denotes the number of words, aka __tokens__, in observation $i$.\n",
    "\n",
    "In SE.1.7, the numerator is the count of all the times that word $j$ appears in all of the training observations. And the denominator is the count (including multiplicities) of all the words that appear in all of the training observations. So this estimate is intuitive.\n",
    "\n",
    "With this in mind, we can rewrite SE.6 withOUT Laplace smoothing as\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=c}=\\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =c \\}x_j^{(i)}}{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =c \\}n_i}\\approx P(w_j\\mid y=c)\n",
    "\\end{align*}$$\n",
    "\n",
    "and with Laplace smoothing as\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=c}=\\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =c \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =c \\}n_i + \\lvert V \\rvert}\\approx P(w_j\\mid y=c)\n",
    "\\end{align*}$$\n",
    "\n",
    "Similarly recall SE.3:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_c\\equiv\\frac{\\text{number of occurences of class }c\\text{ in training data}}{\\text{number of observations in training data}}\\approx P(c)\\tag{SE.3}\n",
    "\\end{align*}$$\n",
    "\n",
    "so that $\\phi_c$ denotes the estimated prior probability of class $c$. The numerator equals $\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = c \\}$. And the denominator is $m$. Hence\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_c=\\frac{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = c \\}}{m}\\approx P(c)\n",
    "\\end{align*}$$\n",
    "\n",
    "But throughout this document, $y$ will take values in $\\{0,1\\}$. So we will define\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_y\\equiv\\frac{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\}}{m}\\approx P(y=1)\n",
    "\\end{align*}$$\n",
    "\n",
    "And when we want to denote the estimated prior $P(y=0)$, we will write $1-\\phi_y$.\n",
    "\n",
    "Hence our maximum likelihood estimates for Multinomial Naive Bayes can be written as\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\phi_{j\\mid y=1}\\equiv\\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =1 \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\} n_i + \\lvert V \\rvert}=\\hat{P}(w_j\\mid y=1)\\tag{SE.8} \\\\\\\\\n",
    "\\phi_{j\\mid y=0}\\equiv\\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =0 \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 0 \\} n_i + \\lvert V \\rvert}=\\hat{P}(w_j\\mid y=0)\\tag{SE.9} \\\\\\\\\n",
    "\\phi_{y}\\equiv\\frac{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\}}{m}=\\hat{P}(y=1)\\tag{SE.10}\n",
    "\\end{gather}$$\n",
    "\n",
    "where the hat $\\hat{P}$ denotes the maximum likelihood estimate.\n",
    " \n",
    "Since $\\hat{P}(w_j\\mid y=1)=\\phi_{j\\mid y=1}$ denotes the estimated probability of word $j$ appearing once, then the estimated probability that word $j$ appears $x_j$ number of times is\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\hat{p}(x_j\\mid y=1)=\\prod_{k=1}^{x_j}\\hat{P}(w_j\\mid y=1)=\\phi_{j\\mid y=1}^{x_j}\\tag{SE.11} \\\\\\\\\n",
    "\\end{gather}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling an Email as a Quasi-Multinomial Random Variable\n",
    "\n",
    "Earlier we noted that the Naive Bayes Classifier (NBC) models an email, given its class, as a multinomial random variable with the multinomial coefficient omitted. Let's recall the definition of the multinomial distribution:\n",
    "\n",
    "A sequence of $t$ independent and identical experiments (aka trials) is performed. Suppose that each experiment can result in any one of $r$ possible outcomes, with respective probabilities $p_1, p_2, ..., p_r$ that sum to $1$: $\\sum_{j=1}^{r}p_j=1$. Let $T_j$ denote the number of $t$ trials that result in outcome number $j$. Then\n",
    "\n",
    "$$\n",
    "P(T_1=t_1, T_2=t_2, ..., T_r=t_r) = \\frac{t!}{t_1!t_2!\\dots t_r!}p_1^{t_1}p_2^{t_2}\\dots p_r^{t_r}\n",
    "$$\n",
    "\n",
    "Let's apply this to email generation. Each choice of a word is an IID trial and each trial can result in any word from our vocabulary. So $t$ is the number of words in the email and $r=\\lvert V\\rvert$ is the number of possible outcomes for each trial. The respective probabilities are $\\phi_{0\\mid y=c}, \\phi_{1\\mid y=c}, ..., \\phi_{13\\mid y=c}$ and $t_j$ is number of times that word $j$ is chosen from the vocabulary.\n",
    "\n",
    "Modeling the email \"discount for coffee for investors\" as a true multinomial random variable, we would compare\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee for investors})&\\propto P(\\text{discount for coffee for investors}\\mid\\text{Spam})P(\\text{Spam})\\\\\\\\\n",
    "    &\\approx \\frac{5!}{1!2!1!1!1!}\\phi_{2\\mid y=1}\\phi_{3\\mid y=1}^{2}\\phi_{1\\mid y=1}\\phi_{8\\mid y=1}\\frac{3}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee for investors})&\\propto P(\\text{discount for coffee for investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})\\\\\\\\\n",
    "    &\\approx \\frac{5!}{1!2!1!1!1!}\\phi_{2\\mid y=0}\\phi_{3\\mid y=0}^{2}\\phi_{1\\mid y=0}\\phi_{8\\mid y=0}\\frac{2}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "But the probabilities $P(\\text{discount for coffee for investors}\\mid\\text{Spam})$ and $P(\\text{discount for coffee for investors}\\mid\\text{Not-Spam})$ have the same multinomial coefficient: $\\frac{5!}{1!2!1!1!1!}$. So removing this coefficient has no effect on the result of the comparison. And we get the same result with this comparison:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee for investors})&\\propto P(\\text{discount for coffee for investors}\\mid\\text{Spam})P(\\text{Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=1}\\phi_{3\\mid y=1}^{2}\\phi_{1\\mid y=1}\\phi_{8\\mid y=1}\\frac{3}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee for investors})&\\propto P(\\text{discount for coffee for investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})\\\\\\\\\n",
    "    &\\approx \\phi_{2\\mid y=0}\\phi_{3\\mid y=0}^{2}\\phi_{1\\mid y=0}\\phi_{8\\mid y=0}\\frac{2}{5}\n",
    "\\end{align*}$$\n",
    "\n",
    "More generally, let $x^{(i)}$ denote the multinomial vector representation of some email. Then\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{P}(x^{(i)}\\mid y=c) = \\frac{n_i}{x_1^{(i)}!x_2^{(i)}!\\dots x_n^{(i)}!}\\phi_{1\\mid y=c}^{x_1^{(i)}}\\phi_{2\\mid y=c}^{x_2^{(i)}}\\dots\\phi_{n\\mid y=c}^{x_n^{(i)}} = \\frac{n_i}{x_1^{(i)}!x_2^{(i)}!\\dots x_n^{(i)}!}\\prod_{j=1}^{n}\\phi_{j\\mid y=c}^{x_j^{(i)}}\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{P}(y=1\\mid x^{(i)})\\propto\\hat{P}(x^{(i)}\\mid y=1)\\hat{P}(y=1)=\\frac{n_i}{x_1^{(i)}!x_2^{(i)}!\\dots x_n^{(i)}!}\\Big(\\prod_{j=1}^{n}\\phi_{j\\mid y=1}^{x_j^{(i)}}\\Big)\\hat{P}(y=1)\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{P}(y=0\\mid x^{(i)})\\propto\\hat{P}(x^{(i)}\\mid y=0)\\hat{P}(y=0)=\\frac{n_i}{x_1^{(i)}!x_2^{(i)}!\\dots x_n^{(i)}!}\\Big(\\prod_{j=1}^{n}\\phi_{j\\mid y=0}^{x_j^{(i)}}\\Big)\\hat{P}(y=0)\n",
    "\\end{align*}$$\n",
    "\n",
    "Dividing both estimates by the constant $\\frac{n_i}{x_1^{(i)}!x_2^{(i)}!\\dots x_n^{(i)}!}$ won't change the ratio between them. So we drop the unnecessary computation and compare\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{P}(y=1\\mid x^{(i)})\\propto\\Big(\\prod_{j=1}^{n}\\phi_{j\\mid y=1}^{x_j^{(i)}}\\Big)\\hat{P}(y=1)\n",
    "\\end{align*}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\hat{P}(y=0\\mid x^{(i)})\\propto\\Big(\\prod_{j=1}^{n}\\phi_{j\\mid y=0}^{x_j^{(i)}}\\Big)\\hat{P}(y=0)\n",
    "\\end{align*}$$\n",
    "\n",
    "We can demonstrate this programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=0  b=4  mnpw=0.00010000000000000002  mnpo=0.00010000000000000002\n",
      "a=1  b=3  mnpw=0.0036000000000000008  mnpo=0.0009000000000000002\n",
      "a=2  b=2  mnpw=0.048600000000000004  mnpo=0.008100000000000001\n",
      "a=3  b=1  mnpw=0.2916  mnpo=0.0729\n",
      "a=4  b=0  mnpw=0.6561  mnpo=0.6561\n",
      "spwc=1.0  spoc=0.7381\n",
      "\n",
      "WITH multinomial coefficients:\n",
      "s1=[ 0.00091618  0.00023694  0.00002036  0.00012216  0.00030539  0.00030539\n",
      "  0.0002036   0.00040719  0.00015778  0.00000036]\n",
      "s0=[ 0.00014746  0.00002949  0.00066355  0.00044237  0.00014746  0.00014746\n",
      "  0.00044237  0.00022118  0.00000033  0.00008493]\n",
      "rsw=[   6.21327485    8.03440714   32.59150848    3.62127872    2.07109162\n",
      "    2.07109162    2.17276723    1.84097033  477.66986535  233.89404699]\n",
      "\n",
      "withOUT multinomial coefficients:\n",
      "s1=[ 0.00003817  0.00000395  0.0000017   0.00000509  0.00001272  0.00001272\n",
      "  0.00000848  0.00001697  0.00000006  0.        ]\n",
      "s0=[ 0.00000614  0.00000049  0.0000553   0.00001843  0.00000614  0.00000614\n",
      "  0.00001843  0.00000922  0.          0.00000024]\n",
      "rso=[   6.21327485    8.03440714   32.59150848    3.62127872    2.07109162\n",
      "    2.07109162    2.17276723    1.84097033  477.66986535  233.89404699]\n",
      "\n",
      "We see that the ratios for each test email are identical: [ 0.  0.  0.  0. -0. -0.  0. -0.  0. -0.]\n"
     ]
    }
   ],
   "source": [
    "fct = lambda n: special.factorial(n, exact=True)\n",
    "\n",
    "def multinom_coeff(tjs=[]):\n",
    "    t = np.sum(tjs)\n",
    "    d = [fct(tj) for tj in tjs]\n",
    "    return fct(t) / np.prod(d)\n",
    "\n",
    "def multinom(tjs=[], pjs=[], with_coeff=True):\n",
    "    if len(tjs) != len(pjs):\n",
    "        raise ValueError(\"len-tjs={} not equal to len-pjs={}\".format(len(tjs), len(pjs)))\n",
    "    if np.sum(pjs) != 1.:\n",
    "        raise ValueError(\"sum-pjs={} not equal to 1.\".format(np.sum(pjs)))\n",
    "    return (multinom_coeff(tjs) if with_coeff else 1) * np.prod(np.power(pjs, tjs))\n",
    "\n",
    "# check our multinomial implementation\n",
    "spwc, spoc = 0., 0.\n",
    "for a in [0,1,2,3,4]:\n",
    "    b = 4 - a\n",
    "    mnpw, mnpo = multinom([a,b], [.9,.1]), multinom([a,b], [.9,.1], with_coeff=False)\n",
    "    print(\"a={}  b={}  mnpw={}  mnpo={}\".format(a, b, mnpw, mnpo))\n",
    "    spwc+=mnpw\n",
    "    spoc+=mnpo\n",
    "print(\"spwc={}  spoc={}\\n\".format(spwc, spoc))\n",
    "\n",
    "def multinom_coeff_mtrx(tjs):\n",
    "    t = np.sum(tjs, axis=1)\n",
    "    return fct(t) / np.prod(fct(tjs), axis=1)\n",
    "\n",
    "def predict_2(phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0, V, test_emails):\n",
    "    Xtest = parse_emails(test_emails, V)\n",
    "    pX1 = np.power(phi_js_yeq1, Xtest)\n",
    "    pX0 = np.power(phi_js_yeq0, Xtest)\n",
    "    mncs = multinom_coeff_mtrx(Xtest)\n",
    "    s1 = phi_yeq1 * mncs * np.prod(pX1, axis=1)\n",
    "    s0 = phi_yeq0 * mncs * np.prod(pX0, axis=1)\n",
    "    rs = s1/s0\n",
    "    rs[rs<1] = 1./rs[rs<1]\n",
    "    return s1, s0, rs\n",
    "\n",
    "def train_and_predict_2(X, y, V, test_emails):\n",
    "    phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0 = train_1(X, y, V)\n",
    "    return predict_2(phi_js_yeq1, phi_js_yeq0, phi_yeq1, phi_yeq0, V, test_emails)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "s1, s0, rsw = train_and_predict_2(Xtr_1, ytr_1, Voc1, test_emails)\n",
    "print(\"WITH multinomial coefficients:\")\n",
    "print(\"s1={}\\ns0={}\\nrsw={}\\n\".format(s1.round(8), s0.round(8), rsw))\n",
    "s1, s0, rso = train_and_predict_1(Xtr_1, ytr_1, Voc1, test_emails)\n",
    "print(\"withOUT multinomial coefficients:\")\n",
    "print(\"s1={}\\ns0={}\\nrso={}\".format(s1.round(8), s0.round(8), rso))\n",
    "\n",
    "print(\"\\nWe see that the ratios for each test email are identical: {}\".format(rsw-rso))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Word Frequency as a Binomial Random Variable\n",
    "\n",
    "Suppose we would like to know the probability that a given word appears exactly $3$ times in an arbitrary email that is classified as spam.\n",
    "\n",
    "Let's say the email consists of $7$ words and let's say the word is the $j^{th}$ word in our vocabulary. Then this is a binonial random variable with parameters $7$ and $\\phi_{j\\mid y=1}$:\n",
    "\n",
    "$$\n",
    "p(3\\mid y=1)=\\binom{7}{3}\\phi_{j\\mid y=1}^{3}(1-\\phi_{j\\mid y=1})^{7-3}\n",
    "$$\n",
    "\n",
    "The reason this is a binomial random variable is because the trials (word choice/generation) are IID. We define success as choosing word $w_j$ and define failure as not choosing word $w_j$. Then the probability of success is $\\phi_{j\\mid y=1}$ and the probability of failure is $1-\\phi_{j\\mid y=1}$.\n",
    "\n",
    "More generally, let $n_i$ denote the number of words in the $i^{th}$ observation. Then the probability that we get exactly $x_j^{(i)}$ occurrences of word $w_j$ in the $i^{th}$ observation is\n",
    "\n",
    "$$\n",
    "p(x_j^{(i)}\\mid y=1)=\\binom{n_{i}}{x_j^{(i)}}\\phi_{j\\mid y=1}^{x_j^{(i)}}(1-\\phi_{j\\mid y=1})^{n_{i}-x_j^{(i)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB Likelihood\n",
    "\n",
    "Let's derive the log-likelihood in the case of Bernoulli Naive Bayes.\n",
    "\n",
    "Let $y$ be a discrete random variable with values in $\\{0,1\\}$. Define $\\phi_y\\equiv \\hat{P}(y=1)=\\hat{p}_{y}(1)$.\n",
    "\n",
    "Let $x=[x_1,...,x_n]^{T}$ be a discrete random vector with values in $\\{0,1,2...\\}^{n}$, where $n$ is the number of features. That is, an observation from $x$ is a column vector in $\\mathbb{R}^n$ whose elements are nonnegative integers. \n",
    "\n",
    "Given a training set $\\{x^{(i)},y^{(i)}\\}_{i=1}^{m}$ where each $(x^{(i)},y^{(i)})$ is an independent observation from the joint random variable $(x,y)$, then define\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\phi_{j\\mid y=1}\\equiv\\hat{P}(x_j=x_j^{(i)}\\mid y=1)=\\hat{p}_{x_j\\mid y}(x_j^{(i)}\\mid 1)\\\\\\\\\n",
    "\\phi_{j\\mid y=0}\\equiv\\hat{P}(x_j=x_j^{(i)}\\mid y=0)=\\hat{p}_{x_j\\mid y}(x_j^{(i)}\\mid 0)\n",
    "\\end{gather}$$\n",
    "\n",
    "The joint likelihood of the training data is\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\mathscr{L}\\big(\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y;\\{x^{(i)},y^{(i)}\\}_{i=1}^{m}\\big) &= P\\big((x,y=x^{(1)},y^{(1)}),...,(x,y=x^{(m)},y^{(m)});\\phi_{j|y=1},\\phi_{j|y=0},\\phi_y\\big)\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}P\\big(x,y=x^{(i)},y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big)\\tag{MNBL.1}\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}p_{x,y}\\big(x^{(i)},y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big)\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}p_{x\\mid y}\\big(x^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0}\\big)p_{y}\\big(y^{(i)};\\phi_y\\big)\\tag{MNBL.2}\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "MNBL.1 holds from the assumption that observations are independent. MNBL.2 holds from the definition of conditional probability. And the log-likelihood is\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\ell\\big(\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big) &= \\sum_{i=1}^m\\mathrm{ln}\\big[p_{x\\mid y}(x^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big]\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\mathrm{ln}\\Big[\\prod_{j=1}^{n}p_{x_{j}\\mid y}(x_j^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\Big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big]\\tag{MNBL.3}\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\sum_{j=1}^{n}\\mathrm{ln}\\big[p_{x_{j}\\mid y}(x_j^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big]\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\sum_{j=1}^{n}\\Big\\{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}\\geq1\\}\\mathrm{ln}\\Big[\\binom{n_{i}}{x_j^{(i)}}\\phi_{j\\mid y=1}^{x_j^{(i)}}(1-\\phi_{j\\mid y=1})^{n_{i}-x_j^{(i)}}\\Big] \\\\\n",
    "     &+\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}\\geq1\\}\\mathrm{ln}\\Big[\\binom{n_{i}}{x_j^{(i)}}\\phi_{j\\mid y=0}^{x_j^{(i)}}(1-\\phi_{j\\mid y=0})^{n_{i}-x_j^{(i)}}\\Big] \\\\\n",
    "     &+ \\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}\\mathrm{ln}\\big[(1-\\phi_{j\\mid y=1})^{n_{i}}\\big]+\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=0\\}\\mathrm{ln}\\big[(1-\\phi_{j|y=0})^{n_{i}}\\big]\\Big\\} \\\\\n",
    "     &+ \\sum_{i=1}^m\\big[\\boldsymbol{1}\\{y^{(i)}=1\\}\\mathrm{ln}(\\phi_y)+\\boldsymbol{1}\\{y^{(i)}=0\\}\\mathrm{ln}(1-\\phi_y)\\big]\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\sum_{j=1}^{n}\\Big\\{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}\\geq1\\}\\Big[\\mathrm{ln}\\binom{n_{i}}{x_j^{(i)}} + x_j^{(i)}\\mathrm{ln}(\\phi_{j\\mid y=1})+(n_{i}-x_j^{(i)})\\mathrm{ln}(1-\\phi_{j\\mid y=1})\\Big]\\\\\n",
    "     &+\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}\\geq1\\}\\Big[\\mathrm{ln}\\binom{n_{i}}{x_j^{(i)}} + x_j^{(i)}\\mathrm{ln}(\\phi_{j\\mid y=0})+(n_{i}-x_j^{(i)})\\mathrm{ln}(1-\\phi_{j\\mid y=0})\\Big] \\\\\n",
    "     &+ \\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}n_{i}\\mathrm{ln}(1-\\phi_{j\\mid y=1})+\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=0\\}n_{i}\\mathrm{ln}(1-\\phi_{j|y=0})\\Big\\} \\tag{MNBL.4} \\\\\n",
    "     &+ \\sum_{i=1}^m\\big[\\boldsymbol{1}\\{y^{(i)}=1\\}\\mathrm{ln}(\\phi_y)+\\boldsymbol{1}\\{y^{(i)}=0\\}\\mathrm{ln}(1-\\phi_y)\\big]\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "MNBL.3 follows from the Naive Bayes Assumption: $x_1,...,x_n$ are conditionally independent given $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB MLE\n",
    "\n",
    "To maximize in $\\phi_{j|y=1}$, we compute the partial derivative of MNBL.4:\n",
    "\n",
    "$$\\begin{align*} \n",
    "0=\\frac{\\partial\\ell}{\\partial\\phi_{j\\mid y=1}} &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}\\geq1\\}}\\Big(\\frac{x_j^{(i)}}{\\phi_{j\\mid y=1}}-\\frac{n_{i}-x_j^{(i)}}{1-\\phi_{j\\mid y=1}}\\Big) - \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}}\\frac{n_{i}}{1-\\phi_{j\\mid y=1}}\\\\\n",
    "     &= \\sum_{i=1}^m{A_{i,j}}\\Big(\\frac{x_j^{(i)}}{\\phi_{j\\mid y=1}}-\\frac{n_{i}-x_j^{(i)}}{1-\\phi_{j\\mid y=1}}\\Big) - \\sum_{i=1}^m{B_{i,j}}\\frac{n_{i}}{1-\\phi_{j\\mid y=1}}\\\\\n",
    "     &= \\sum_{i=1}^m{A_{i,j}}\\frac{x_j^{(i)}}{\\phi_{j\\mid y=1}} - \\sum_{i=1}^m{A_{i,j}}\\frac{n_{i}-x_j^{(i)}}{1-\\phi_{j\\mid y=1}} - \\sum_{i=1}^m{B_{i,j}}\\frac{n_{i}}{1-\\phi_{j\\mid y=1}}\\\\\n",
    "     &= \\sum_{i=1}^m{A_{i,j}}\\frac{x_j^{(i)}}{\\phi_{j\\mid y=1}} - \\sum_{i=1}^m\\Big({A_{i,j}}\\frac{n_{i}-x_j^{(i)}}{1-\\phi_{j\\mid y=1}} + {B_{i,j}}\\frac{n_{i}}{1-\\phi_{j\\mid y=1}}\\Big)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where $A_{i,j}\\equiv\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}\\geq1\\}$ and $B_{i,j}\\equiv\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}$. Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m\\Big({A_{i,j}}\\frac{n_{i}-x_j^{(i)}}{1-\\phi_{j\\mid y=1}}+{B_{i,j}}\\frac{n_{i}}{1-\\phi_{j\\mid y=1}}\\Big)= \\sum_{i=1}^m{A_{i,j}}\\frac{x_j^{(i)}}{\\phi_{j\\mid y=1}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\frac{1}{1-\\phi_{j\\mid y=1}}\\sum_{i=1}^m\\big({A_{i,j}}(n_{i}-x_j^{(i)})+{B_{i,j}}n_{i}\\big)=\\frac{1}{\\phi_{j\\mid y=1}}\\sum_{i=1}^m{A_{i,j}}x_j^{(i)}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j\\mid y=1}\\sum_{i=1}^m\\big({A_{i,j}}(n_{i}-x_j^{(i)})+{B_{i,j}}n_{i}\\big)&=(1-\\phi_{j\\mid y=1})\\sum_{i=1}^m{A_{i,j}}x_j^{(i)}\\\\\n",
    "    &=\\sum_{i=1}^m{A_{i,j}}x_j^{(i)}-\\phi_{j\\mid y=1}\\sum_{i=1}^m{A_{i,j}}x_j^{(i)}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{A_{i,j}}x_j^{(i)}&=\\phi_{j\\mid y=1}\\sum_{i=1}^m{A_{i,j}}x_j^{(i)}+\\phi_{j\\mid y=1}\\sum_{i=1}^m\\big({A_{i,j}}(n_{i}-x_j^{(i)})+{B_{i,j}}n_{i}\\big)\\\\\n",
    "   &=\\phi_{j\\mid y=1}\\Big[\\sum_{i=1}^m{A_{i,j}}x_j^{(i)}+\\sum_{i=1}^m\\big({A_{i,j}}(n_{i}-x_j^{(i)}) + {B_{i,j}}n_{i}\\big)\\Big]\\\\\n",
    "   &=\\phi_{j\\mid y=1}\\sum_{i=1}^m\\Big[{A_{i,j}}x_j^{(i)}+{A_{i,j}}(n_{i}-x_j^{(i)})+{B_{i,j}}n_{i}\\Big]\\\\\n",
    "   &=\\phi_{j\\mid y=1}\\sum_{i=1}^m\\Big[{A_{i,j}}x_j^{(i)}+{A_{i,j}}n_{i}-A_{i,j}x_j^{(i)}+{B_{i,j}}n_{i}\\Big]\\\\\n",
    "   &=\\phi_{j\\mid y=1}\\sum_{i=1}^m\\big[{A_{i,j}}n_{i}+{B_{i,j}}n_{i}\\big]\\\\\n",
    "   &=\\phi_{j\\mid y=1}\\sum_{i=1}^m(A_{i,j}+B_{i,j})n_{i}\\\\\n",
    "   &=\\phi_{j\\mid y=1}\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}n_{i}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "since\n",
    "\n",
    "$$\\begin{align*} \n",
    "A_{i,j}+B_{i,j} &=\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}\\geq1\\}+\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}=\\boldsymbol{1}\\{y^{(i)}=1\\}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Note that\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}\\geq1\\}x_j^{(i)}=\\begin{cases}0=\\boldsymbol{1}\\{y^{(i)}=1\\}x_j^{(i)}&x_j^{(i)}=0\\\\\\boldsymbol{1}\\{y^{(i)}=1\\}x_j^{(i)}&x_j^{(i)}\\geq1\\end{cases}\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{j\\mid y=1}&=\\frac{\\sum_{i=1}^m{A_{i,j}}x_j^{(i)}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}n_{i}} =\\frac{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}\\geq1\\}x_j^{(i)}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}n_{i}} =\\frac{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}x_j^{(i)}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}n_{i}}\\tag{MNBL.5}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Similarly we can compute\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j|y=0} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=1\\}}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=0\\}} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}}x_j^{(i)}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=0\\}}\\tag{MNBL.6}\n",
    "\\end{align*}$$\n",
    "\n",
    "MNBL.6 and MNBL.7 are identical to the corresponding multinomial equations in SE.8 and SE.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Training\n",
    "\n",
    "We wish to produce three things from training:\n",
    "\n",
    "1. An array of the $\\{\\phi_{j\\mid y=1}\\}_{j=1}^{n}$\n",
    "1. An array of the $\\{\\phi_{j\\mid y=0}\\}_{j=1}^{n}$\n",
    "1. The number $\\phi_y$\n",
    "\n",
    "Let $X$ denote the training data matrix. Let $X_1$ denote the matrix of rows from $X$ such that the corresponding label is $y=1$, and similarly for $X_0$. From SE.1.8, we have\n",
    "\n",
    "$$\n",
    "\\phi_{j\\mid y=1}\\equiv\\frac{\\sum_{i=1}^{m}\\boldsymbol{1} \\{y^{(i)} =1 \\}x_j^{(i)} + 1}{\\sum_{i=1}^{m} \\boldsymbol{1} \\{ y^{(i)} = 1 \\} n_i + \\lvert V \\rvert}=\\frac{\\sum_{i=1}^{m_1}x_j^{(1,i)} + 1}{\\sum_{i=1}^{m_1} n_{1,i} + \\lvert V \\rvert}\n",
    "$$\n",
    "\n",
    "where $m_1$ is the number of rows in matrix $X_1$, $x^{(1,i)}$ is the $i^{th}$ row in matrix $X_1$, and $n_{1,i}$ is the total number of words in the $i^{th}$ row of matrix $X_1$. But then $\\sum_{i=1}^{m_1} n_{1,i}$ is total number of words in matrix $X_1$, denoted by $t_1$:\n",
    "\n",
    "$$\n",
    "\\phi_{j\\mid y=1}=\\frac{\\sum_{i=1}^{m_1}x_j^{(1,i)} + 1}{t_1 + \\lvert V \\rvert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb_train_ex(X, y):\n",
    "    X1, X0 = X[y==1], X[y==0]\n",
    "    vocab_size = X.shape[1]\n",
    "    phi_js_yeq1 = (np.sum(X1, axis=0) + 1) / (np.sum(X1) + vocab_size)\n",
    "    phi_js_yeq0 = (np.sum(X0, axis=0) + 1) / (np.sum(X0) + vocab_size)\n",
    "    phi_y = np.sum(y==1) / y.shape[0]\n",
    "    return phi_js_yeq1, phi_js_yeq0, phi_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction & Underflow\n",
    "\n",
    "Earlier we compared the following values and predicted spam or not:\n",
    "\n",
    "$$\\begin{align*}\n",
    "P(\\text{Spam}\\mid \\text{discount for coffee investors})&\\propto P(\\text{discount for coffee investors}\\mid\\text{Spam})P(\\text{Spam})\\\\\\\\\n",
    "P(\\text{Not-Spam}\\mid \\text{discount for coffee investors})&\\propto P(\\text{discount for coffee investors}\\mid\\text{Not-Spam})P(\\text{Not-Spam})\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "More succintly, we picked the larger of these two quantities\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y=1\\mid x)&\\propto p(x\\mid y=1)p(y=1)\\\\\\\\\n",
    "p(y=0\\mid x)&\\propto p(x\\mid y=0)p(y=0)\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "But when our vocabulary is sufficiently large, we will easily encounter an __underflow__ problem. Recall that we computed $p(x\\mid y=c)$ with the Naive Bayes assumption of conditional independence:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(x\\mid y=1)=\\prod_{j=1}^{n}p(x_j\\mid y=1)\\approx\\prod_{j=1}^{n}\\phi_{j\\mid y=1}^{x_j}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $n$ is the size of our vocabulary. But the product of a large number of values between $0$ and $1$ will get very close to $0$. And the standard computer representation of real numbers cannot handle numbers that are too small, and instead rounds them off to zero... underflow.\n",
    "\n",
    "Let's take a very conservative example and show that $p(x\\mid y=1)\\approx0$ can easily produce underflow. Suppose we have a $10{,}000$ words in our vocabulary. And suppose $20\\%$ of the words each have a probability near $0.99$ of appearing in a given email/document given that it is spam. Also suppose that the remaining $80\\%$ of the words each have a probability near $0.5$ of appearing in a given email/document given that it is spam.\n",
    "\n",
    "This is an unrealistic example but it's conservative in that we will show how easy it is to produce underflow. In reality, the proposed probabilities would be much lower and hence more likely to induce underflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeroprod=0.0  nzprod=5e-324\n",
      "zeroprod=0.0\n"
     ]
    }
   ],
   "source": [
    "uf_a = .000001 * np.random.randn(8000)+.5\n",
    "uf_b = .000001 * np.random.randn(2000)+.99\n",
    "zeroprod = np.prod(np.concatenate((uf_a, uf_b)))\n",
    "uf_a = .000001 * np.random.randn(8000)+.501\n",
    "nzprod = np.prod(np.concatenate((uf_a, uf_b)))\n",
    "print(\"zeroprod={}  nzprod={}\".format(zeroprod, nzprod))\n",
    "\n",
    "# a bit more realistic\n",
    "uf_a = .000001 * np.random.randn(8000)+.01\n",
    "uf_b = .000001 * np.random.randn(1000)+.5\n",
    "uf_c = .000001 * np.random.randn(500)+.80\n",
    "uf_d = .000001 * np.random.randn(500)+.99\n",
    "zeroprod = np.prod(np.concatenate((uf_a, uf_b, uf_c, uf_d)))\n",
    "print(\"zeroprod={}\".format(zeroprod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid underflow, we compute\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y=1\\mid x) &= \\frac{p(x\\mid y=1)p(y=1)}{p(x)} \\\\\n",
    "              &= \\frac{p(x\\mid y=1)p(y=1)}{p(x\\mid y=1)p(y=1) + p(x\\mid y=0) p(y=0)} \\\\\n",
    "              &= \\frac{1}{1 + \\frac{p(x\\mid y=0) p(y=0)}{p(x\\mid y=1) p(y=1)}} \\\\\n",
    "              &= \\frac{1}{1 + \\mathrm{exp}\\big(\\mathrm{log}\\big[\\frac{p(x\\mid y=0) p(y=0)}{p(x\\mid y=1) p(y=1)}\\big]\\big)} \\\\\n",
    "              &= \\frac{1}{1 + \\mathrm{exp}\\big(\\mathrm{log}[p(x\\mid y=0)] + \\mathrm{log}[p(y=0)] - \\mathrm{log}[p(x\\mid y=1)] - \\mathrm{log}[p(y=1)]\\big)} \\\\\n",
    "              &\\approx \\frac{1}{1 + \\mathrm{exp}\\big(\\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=0}]x_j + \\mathrm{log}[1 - \\phi_y] - \\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=1}]x_j - \\mathrm{log}[\\phi_y]\\big)}\\tag{PRUN.1}\n",
    "\\end{align*}$$\n",
    "\n",
    "since\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathrm{log}[p(x\\mid y=1)] &= \\mathrm{log}\\big[\\prod_{j=1}^{n}p(x_j\\mid y=1)\\big] \\\\\n",
    "              &= \\sum_{j=1}^{n}\\mathrm{log}[p(x_j\\mid y=1)] \\\\\n",
    "              &\\approx \\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=1}^{x_j}] \\\\\n",
    "              &= \\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=1}]x_j \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Let's check that this avoids underflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This produces the 'invalid value' warning due to underflow:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def generate_random_phi_js_yeq(std = .0001):\n",
    "    uf_a = np.maximum(1e-5, .00001*np.random.randn(8000)+.01) # words occurring rarely\n",
    "    uf_b = std * np.random.randn(500)+.2 # sometimes\n",
    "    uf_c = std * np.random.randn(500)+.5 # frequently\n",
    "    uf_d = std * np.random.randn(500)+.8 # very frequently\n",
    "    uf_e = np.minimum(100-(1e-7), .006*np.random.randn(500)+.99) # all the time\n",
    "    return np.concatenate((uf_a, uf_b, uf_c, uf_d, uf_e))\n",
    "\n",
    "phi_js_yeq1 = generate_random_phi_js_yeq()\n",
    "phi_js_yeq0 = generate_random_phi_js_yeq()\n",
    "\n",
    "py1 = 27/100\n",
    "py0 = 1 - py1\n",
    "phi_y = py1\n",
    "\n",
    "xa = np.random.choice(5, 8000, p=[.9, .06, .02, .015, .005])\n",
    "xb = np.random.choice(5, 500, p=[.7, .2, .05, .03, .02])\n",
    "xc = np.random.choice(5, 500, p=[.4, .3, .2, .07, .03])\n",
    "xd = np.random.choice(5, 500, p=[.1, .2, .3, .3, .1])\n",
    "xe = np.random.choice(5, 500, p=[.005, .185, .41, .25, .15])\n",
    "x = np.concatenate((xa, xb, xc, xd, xe))\n",
    "\n",
    "pxy1 = np.prod(np.power(phi_js_yeq1, x))\n",
    "pxy0 = np.prod(np.power(phi_js_yeq0, x))\n",
    "\n",
    "py1x_uf = lambda pxy0, py0, pxy1, py1: (pxy1 * py1) / (pxy1 * py1 + pxy0 * py0)\n",
    "\n",
    "py1x_uf(pxy0, py0, pxy1, py1)\n",
    "print(\"This produces the 'invalid value' warning due to underflow:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_clip_ufw = 0.2698\n",
      "log_probs=0.8698990719382897  ly1=-6782.928210888821  ly0=-6783.052934392027\n",
      "pred_avoid_uf = 0.2953\n"
     ]
    }
   ],
   "source": [
    "cl = lambda x, t=1e-320: np.clip(x, t, 1-t)\n",
    "py1x_cuf = lambda pxy0, py0, pxy1, py1: (cl(pxy1) * py1) / (cl(pxy1) * py1 + cl(pxy0) * py0)\n",
    "\n",
    "def py1x_auf(phi_js_yeq1, phi_js_yeq0, phi_y, x, prnt_log_probs=True):\n",
    "    log_phi_js_yeq1 = np.sum(np.log(phi_js_yeq1) * x)\n",
    "    log_phi_js_yeq0 = np.sum(np.log(phi_js_yeq0) * x)\n",
    "    log_probs = log_phi_js_yeq0 + np.log(1-phi_y) - log_phi_js_yeq1 - np.log(phi_y)\n",
    "    if prnt_log_probs:\n",
    "        print(\"log_probs={}  ly1={}  ly0={}\".format(log_probs, log_phi_js_yeq1, log_phi_js_yeq0))\n",
    "    return 1./(1 + np.exp(log_probs))\n",
    "\n",
    "pred_clip_ufw = py1x_cuf(pxy0, py0, pxy1, py1)\n",
    "print(\"pred_clip_ufw = %.4f\" % pred_clip_ufw)\n",
    "\n",
    "pred_avoid_uf = py1x_auf(phi_js_yeq1, phi_js_yeq0, phi_y, x)\n",
    "print(\"pred_avoid_uf = %.4f\" % pred_avoid_uf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Prediction\n",
    "\n",
    "Suppose our test matrix $X$ has $q$ observations. Then\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y\\mid X) &\\approx\\begin{bmatrix}\\frac{1}{1 + \\mathrm{exp}\\big(\\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=0}]x_j^{(1)} + \\mathrm{log}[1 - \\phi_y] - \\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=1}]x_j^{(1)} - \\mathrm{log}[\\phi_y]\\big)}\\\\\\vdots\\\\\\frac{1}{1 + \\mathrm{exp}\\big(\\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=0}]x_j^{(q)} + \\mathrm{log}[1 - \\phi_y] - \\sum_{j=1}^{n}\\mathrm{log}[\\phi_{j\\mid y=1}]x_j^{(q)} - \\mathrm{log}[\\phi_y]\\big)}\\end{bmatrix}\\\\\\\\\n",
    "    &= 1 /^B (1 +^B \\exp(r))\n",
    "\\end{align*}$$\n",
    "\n",
    "where $/^B$ denotes divide broadcast, $+^B$ denotes plus broadcast, and $r$ is the vector of log propability ratios:\n",
    "\n",
    "$$\\begin{align*}\n",
    "r &\\equiv \\begin{bmatrix}\\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=0})x_j^{(1)} + \\mathrm{log}(1 - \\phi_y) - \\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=1})x_j^{(1)} - \\mathrm{log}(\\phi_y)\\\\\\vdots\\\\\\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=0})x_j^{(q)} + \\mathrm{log}(1 - \\phi_y) - \\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=1})x_j^{(q)} - \\mathrm{log}(\\phi_y)\\end{bmatrix} \\\\\\\\\n",
    " &= \\begin{bmatrix}\\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=0})x_j^{(1)}\\\\\\vdots\\\\\\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=0})x_j^{(q)}\\end{bmatrix}\n",
    " + \\begin{bmatrix}\\mathrm{log}(1 - \\phi_y)\\\\\\vdots\\\\\\mathrm{log}(1 - \\phi_y)\\end{bmatrix}\n",
    " + \\begin{bmatrix}-\\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=1})x_j^{(1)}\\\\\\vdots\\\\- \\sum_{j=1}^{n}\\mathrm{log}(\\phi_{j\\mid y=1})x_j^{(q)}\\end{bmatrix}\n",
    " + \\begin{bmatrix}- \\mathrm{log}(\\phi_y)\\\\\\vdots\\\\- \\mathrm{log}(\\phi_y)\\end{bmatrix} \\\\\\\\\n",
    " &= \\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(1)}+\\dots+\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(1)}\\\\\\vdots\\\\\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(q)}+\\dots+\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(q)}\\end{bmatrix}\n",
    " +^B \\mathrm{log}(1 - \\phi_y) \n",
    " -\\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=1})x_1^{(1)}+\\dots+\\mathrm{log}(\\phi_{n\\mid y=1})x_n^{(1)}\\\\\\vdots\\\\\\mathrm{log}(\\phi_{1\\mid y=1})x_1^{(q)}+\\dots+\\mathrm{log}(\\phi_{n\\mid y=1})x_n^{(q)}\\end{bmatrix}\n",
    " -^B \\mathrm{log}(\\phi_y) \\\\\\\\\n",
    " &= {\\Large S}_{ax=1}\\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(1)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(1)}\\\\\\vdots&\\ddots&\\vdots\\\\\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(q)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(q)}\\end{bmatrix}\n",
    " +^B \\mathrm{log}(1 - \\phi_y) \n",
    " -{\\Large S}_{ax=1}\\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=1})x_1^{(1)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=1})x_n^{(1)}\\\\\\vdots&\\ddots&\\vdots\\\\\\mathrm{log}(\\phi_{1\\mid y=1})x_1^{(q)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=1})x_n^{(q)}\\end{bmatrix}\n",
    " -^B \\mathrm{log}(\\phi_y) \\\\\\\\\n",
    " &= {\\Large S}_{ax=1}L_0\n",
    " +^B \\mathrm{log}(1 - \\phi_y) \n",
    " -{\\Large S}_{ax=1}L_1\n",
    " -^B \\mathrm{log}(\\phi_y) \\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where ${\\large S}_{ax=1}L$ denotes the sum of the elements of the matrix $L$ across axis $1$ (i.e. sum the rows) and\n",
    "\n",
    "$$\\begin{align*}\n",
    "L_0 &\\equiv \\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(1)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(1)}\\\\\\vdots&\\ddots&\\vdots\\\\\\mathrm{log}(\\phi_{1\\mid y=0})x_1^{(q)}&\\dots&\\mathrm{log}(\\phi_{n\\mid y=0})x_n^{(q)}\\end{bmatrix} \\\\\\\\\n",
    "    &= \\begin{bmatrix}\\mathrm{log}(\\phi_{1\\mid y=0})&\\dots&\\mathrm{log}(\\phi_{n\\mid y=0})\\end{bmatrix}*_{R} \\begin{bmatrix}x_1^{(1)}&\\dots&x_n^{(1)}\\\\\\vdots&\\ddots&\\vdots\\\\x_1^{(q)}&\\dots&x_n^{(q)}\\end{bmatrix} \\\\\\\\\n",
    "    &= \\mathrm{log}\\big(\\begin{bmatrix}\\phi_{1\\mid y=0}&\\dots&\\phi_{n\\mid y=0}\\end{bmatrix}\\big)*_{R}X \\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where $v^T*_RX$ denotes multiplying elementwise the row vector $v^T$ by each row of matrix $X$. And similarly for $L_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email='discount for coffee investors' class=1 prob=0.8613667133673921\n",
      "email='discount for coffee for investors' class=1 prob=0.8893120505812462\n",
      "email='a coffee is coffee' class=0 prob=0.02976942820520807\n",
      "email='a coffee is free' class=0 prob=0.21639032410492626\n",
      "email='investors coffee is free' class=1 prob=0.6743828824748668\n",
      "email='investors coffee is discount' class=1 prob=0.6743828824748668\n",
      "email='investors coffee is game' class=0 prob=0.31518227681947986\n",
      "email='investors coffee get game' class=1 prob=0.648007587142401\n",
      "email='investors get prices free for qualified investors' class=1 prob=0.9979108774702627\n",
      "email='our coffee grab is lets coffee' class=0 prob=0.004257238584003545\n"
     ]
    }
   ],
   "source": [
    "def nb_test_ex(X, state):\n",
    "    output = np.zeros(X.shape[0], dtype=int)\n",
    "    phi_js_yeq1, phi_js_yeq0, phi_y = state['phi_js_yeq1'], state['phi_js_yeq0'], state['phi_y']\n",
    "\n",
    "    L1 = np.log(phi_js_yeq1) * X # (n,) * (X.shape[0], n) = (X.shape[0], n)\n",
    "    L0 = np.log(phi_js_yeq0) * X\n",
    "    log_phi_yeq1 = np.sum(L1, axis=1) # (X.shape[0],)\n",
    "    log_phi_yeq0 = np.sum(L0, axis=1)\n",
    "    r = log_phi_yeq0 + np.log(1-phi_y) - log_phi_yeq1 - np.log(phi_y)\n",
    "    probs = 1./(1 + np.exp(np.clip(r, -700, 700)))\n",
    "    \n",
    "    output[probs>.5] = 1\n",
    "    return output, probs\n",
    "\n",
    "phi_js_yeq1, phi_js_yeq0, phi_y = nb_train_ex(Xtr_1, ytr_1)\n",
    "state = {'phi_js_yeq1':phi_js_yeq1, 'phi_js_yeq0':phi_js_yeq0, 'phi_y':phi_y}\n",
    "preds, probs = nb_test_ex(parse_emails(test_emails, Voc1), state)\n",
    "for email, cls, prob in zip(test_emails, preds, probs):\n",
    "    print(\"email='{}' class={} prob={}\".format(email, cls, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the probability distribution of words is identical between classes, the NBC (Naive Bayes Classifier)    \n",
      "cannot classify any better than random:\n",
      "accuracy=0.4805\n",
      "When the distributions are even slightly different between classes, the NBC can classify much better than random:\n",
      "accuracy=0.7945\n",
      "When the distributions are even marginally different between classes, the NBC can classify perfectly:\n",
      "accuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "def generate_class_c_matrix(c=0, num_obs=5000):\n",
    "    Xc = []\n",
    "    for obs in range(num_obs):\n",
    "        xa = np.random.choice(5, 8000, p=[.9, .06, .02, .015, .005])\n",
    "        xb = np.random.choice(5, 500, p=[.7, .2, .05, .03, .02])\n",
    "        xc = np.random.choice(5, 500, p=[.4, .3, .2, .07, .03])\n",
    "        xd = np.random.choice(5, 500, p=[.005, .16, .39, .24, .205])\n",
    "        xe = np.random.choice(5, 500, p=[.005, .185, .41, .25, .15])\n",
    "        if c == 0:\n",
    "            x = np.concatenate((xa, xb, xc, xd, xe))\n",
    "        elif c == 1:\n",
    "            x = np.concatenate((xa, xb, xc, xe, xd))\n",
    "        elif c == 2:\n",
    "            x = np.concatenate((xa, xb, xe, xd, xc))\n",
    "        Xc.append(x)\n",
    "    return np.array(Xc, dtype=int)\n",
    "\n",
    "def gen_tr_te(oc=1):\n",
    "    X1 = generate_class_c_matrix(c=oc)\n",
    "    X0 = generate_class_c_matrix(c=0)\n",
    "    Xtr_2 = np.vstack((X1, X0))\n",
    "    ytr_2 = np.concatenate((np.full(5000, 1), np.full(5000, 0)))\n",
    "\n",
    "    X1 = generate_class_c_matrix(c=oc, num_obs=1000)\n",
    "    X0 = generate_class_c_matrix(c=0, num_obs=1000)\n",
    "    Xte_2 = np.vstack((X1, X0))\n",
    "    yte_2 = np.concatenate((np.full(1000, 1), np.full(1000, 0)))\n",
    "\n",
    "    phi_js_yeq1, phi_js_yeq0, phi_y = nb_train_ex(Xtr_2, ytr_2)\n",
    "    state = {'phi_js_yeq1':phi_js_yeq1, 'phi_js_yeq0':phi_js_yeq0, 'phi_y':phi_y}\n",
    "    preds, probs = nb_test_ex(Xte_2, state)\n",
    "    print(\"accuracy={}\".format(np.sum(preds==yte_2)/2000))\n",
    "\n",
    "print(\"When the probability distribution of words is identical between classes, the NBC (Naive Bayes Classifier)\\\n",
    "    \\ncannot classify any better than random:\")\n",
    "gen_tr_te(oc=0)\n",
    "print(\"When the distributions are even slightly different between classes, the NBC can classify much better than random:\")\n",
    "gen_tr_te(oc=1)\n",
    "print(\"When the distributions are even marginally different between classes, the NBC can classify perfectly:\")\n",
    "gen_tr_te(oc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See implementation in `nb.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types: Xtr=<class 'numpy.ndarray'>  ttr=<class 'list'>  ttre=<class 'str'>  ytr=<class 'numpy.ndarray'>\n",
      "shapes: Xtr=(2144, 1448)  tl=1448  ytr=(2144,)\n",
      "previews:\n",
      "tokens_tr=['abil', 'absolut', 'abus']\n",
      "ytr=[1 0 1]\n",
      "\n",
      "And the test data...\n",
      "\n",
      "types: Xte=<class 'numpy.ndarray'>  tte=<class 'list'>  ttee=<class 'str'>  yte=<class 'numpy.ndarray'>\n",
      "shapes: Xte=(800, 1448)  tte=1448  yte=(800,)\n",
      "previews:\n",
      "tokens_te=['abil', 'absolut', 'abus']\n",
      "yte=[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Xtr, tokens_tr, ytr = nb.readMatrix('unzipped_spam_data/MATRIX.TRAIN')\n",
    "print(\"types: Xtr={}  ttr={}  ttre={}  ytr={}\".format(type(Xtr), type(tokens_tr), type(tokens_tr[0]), type(ytr)))\n",
    "print(\"shapes: Xtr={}  tl={}  ytr={}\".format(Xtr.shape, len(tokens_tr), ytr.shape))\n",
    "print(\"previews:\\ntokens_tr={}\\nytr={}\".format(tokens_tr[:3], ytr[:3]))\n",
    "\n",
    "print(\"\\nAnd the test data...\\n\")\n",
    "\n",
    "Xte, tokens_te, yte = nb.readMatrix('./unzipped_spam_data/MATRIX.TEST')\n",
    "print(\"types: Xte={}  tte={}  ttee={}  yte={}\".format(type(Xte), type(tokens_te), type(tokens_te[0]), type(yte)))\n",
    "print(\"shapes: Xte={}  tte={}  yte={}\".format(Xte.shape, len(tokens_te), yte.shape))\n",
    "print(\"previews:\\ntokens_te={}\\nyte={}\".format(tokens_te[:3], yte[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.0163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.016250000000000001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = nb.nb_train(Xtr, ytr)\n",
    "output = nb.nb_test(Xte, state)\n",
    "nb.evaluate(output, yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation in the question is nonsensical. The probability $p(x_j=i\\mid y=1)$ has nothing to do with token $i$. Instead we will check\n",
    "\n",
    "$$\n",
    "\\text{estimated probability ratio for token }j=\\frac{\\phi_{j\\mid y=1}}{\\phi_{j\\mid y=0}}\\approx\\frac{p(w_j\\mid y=1)}{p(w_j\\mid y=0)}\n",
    "$$\n",
    "\n",
    "since the strictly increasing log is irrelevant here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['phi_js_yeq1', 'phi_js_yeq0', 'phi_y'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['httpaddr', 'spam', 'unsubscrib', 'ebai', 'valet', 'diploma', 'dvd',\n",
       "       'websit', 'click', 'lowest', 'numberpx', 'arial', 'helvetica',\n",
       "       'serif', 'nashua', 'amherst', 'mortgag', 'refin', 'charset',\n",
       "       'newslett', 'customerservic', 'numberpt', 'iso', 'web', 'lender',\n",
       "       'numberd', 'loan', 'dailybargainmail', 'coral', 'html', 'unsolicit',\n",
       "       'www', 'fl', 'holidai', 'equiti', 'tal', 'consolid', 'bachelor',\n",
       "       'sweepstak', 'subscript', 'mba', 'bonu', 'ae', 'refinanc', 'hover',\n",
       "       'mime', 'untitl', 'subscrib', 'recur', 'postal'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_ratios = state['phi_js_yeq1'] / state['phi_js_yeq0']\n",
    "ordered_descending = np.argsort(prob_ratios)[::-1] # or np.flip(np.argsort(prob_ratios), axis=0)\n",
    "tokens = np.array(tokens_tr)\n",
    "tokens[ordered_descending][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filenames = ['./unzipped_spam_data/MATRIX.TRAIN.400', './unzipped_spam_data/MATRIX.TRAIN.200', './unzipped_spam_data/MATRIX.TRAIN.800', './unzipped_spam_data/MATRIX.TRAIN.1400', './unzipped_spam_data/MATRIX.TRAIN.50', './unzipped_spam_data/MATRIX.TRAIN.100']\n",
      "\n",
      "Error: 0.0387\n",
      "Error: 0.0262\n",
      "Error: 0.0262\n",
      "Error: 0.0187\n",
      "Error: 0.0175\n",
      "Error: 0.0163\n",
      "\n",
      "nb_sizes=[50, 100, 200, 400, 800, 1400]\n",
      "nb_errs=[0.03875, 0.026249999999999999, 0.026249999999999999, 0.018749999999999999, 0.017500000000000002, 0.016250000000000001]\n"
     ]
    }
   ],
   "source": [
    "filenames = glob.glob('./unzipped_spam_data/MATRIX.TRAIN.[0-9]*')\n",
    "print(\"filenames = {}\\n\".format(filenames))\n",
    "\n",
    "files = sorted(filenames, key=lambda s: int(s.rsplit('.')[-1]))\n",
    "\n",
    "nb_sizes = []\n",
    "nb_errs = []\n",
    "\n",
    "for f in files:\n",
    "    Xf, tokf, yf = nb.readMatrix(f)\n",
    "    nb_sizes.append(Xf.shape[0])\n",
    "    state_f = nb.nb_train(Xf, yf)\n",
    "    output_f = nb.nb_test(Xte, state_f)\n",
    "    nb_errs.append(nb.evaluate(output_f, yte))\n",
    "\n",
    "print(\"\\nnb_sizes={}\\nnb_errs={}\".format(nb_sizes, nb_errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11426b908>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVpL/D5aGmlIIDNGFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1141f5470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nb_sizes, nb_errs)\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.0125\n",
      "Error: 0.0112\n",
      "Error: 0.0125\n",
      "Error: 0.0037\n",
      "Error: 0.0000\n",
      "Error: 0.0000\n"
     ]
    }
   ],
   "source": [
    "svm_sizes = []\n",
    "svm_errs = []\n",
    "svm_Xte, svm_tok, svm_yte = svm.readMatrix('./unzipped_spam_data/MATRIX.TEST')\n",
    "\n",
    "for f in files:\n",
    "    Xf, tokf, yf = svm.readMatrix(f)\n",
    "    svm_sizes.append(Xf.shape[0])\n",
    "    state_f = svm.svm_train(Xf, yf)\n",
    "    output_f = svm.svm_test(svm_Xte, state_f)\n",
    "    svm_errs.append(svm.evaluate(output_f, svm_yte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x114256f28>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFCQiIhKX/w83Kd4/t69bkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f1ba518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(svm_sizes, svm_errs)\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1147a09e8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnXmxWo7E1YIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114905630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nb_sizes, nb_errs, label='navie bayes')\n",
    "plt.plot(svm_sizes, svm_errs, label='svm')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM outperforms NB at each training set size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli NB Likelihood\n",
    "\n",
    "Let's derive the log-likelihood in the case of Bernoulli Naive Bayes.\n",
    "\n",
    "Let $y$ be a discrete random variable with values in $\\{0,1\\}$. Define $\\phi_y\\equiv \\hat{P}(y=1)=\\hat{p}_{y}(1)$.\n",
    "\n",
    "Let $x=[x_1,...,x_n]^{T}$ be a discrete random vector with values in $\\{0,1\\}^{n}$, where $n$ is the number of features. That is, an observation from $x$ is a column vector in $\\mathbb{R}^n$ whose elements are $0$ or $1$. Define\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\phi_{j\\mid y=1}\\equiv\\hat{P}(x_j=1\\mid y=1)=\\hat{p}_{x_j\\mid y}(1\\mid 1)\\\\\\\\\n",
    "\\phi_{j\\mid y=0}\\equiv\\hat{P}(x_j=1\\mid y=0)=\\hat{p}_{x_j\\mid y}(1\\mid 0)\n",
    "\\end{gather}$$\n",
    "\n",
    "Given a training set $\\{x^{(i)},y^{(i)}\\}_{i=1}^{m}$ where each $(x^{(i)},y^{(i)})$ is an independent observation from the joint random variable $(x,y)$, then the joint likelihood of the training data is\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\mathscr{L}\\big(\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y;\\{x^{(i)},y^{(i)}\\}_{i=1}^{m}\\big) &= P\\big((x,y=x^{(1)},y^{(1)}),...,(x,y=x^{(m)},y^{(m)});\\phi_{j|y=1},\\phi_{j|y=0},\\phi_y\\big)\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}P\\big(x,y=x^{(i)},y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big)\\tag{SCN.1}\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}p_{x,y}\\big(x^{(i)},y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big)\\\\\\\\\n",
    "    &= \\prod_{i=1}^{m}p_{x\\mid y}\\big(x^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0}\\big)p_{y}\\big(y^{(i)};\\phi_y\\big)\\tag{SCN.2}\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "SCN.1 holds from the assumption that observations are independent. SCN.2 holds from the definition of conditional probability. And the log-likelihood is\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\ell\\big(\\phi_{j\\mid y=1},\\phi_{j\\mid y=0},\\phi_y\\big) &= \\sum_{i=1}^m\\mathrm{ln}\\big[p_{x\\mid y}(x^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big]\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\mathrm{ln}\\Big[\\prod_{j=1}^{n}p_{x_{j}\\mid y}(x_j^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\Big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big]\\tag{SCN.3}\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\sum_{j=1}^{n}\\mathrm{ln}\\big[p_{x_{j}\\mid y}(x_j^{(i)}\\mid y^{(i)};\\phi_{j\\mid y=1},\\phi_{j\\mid y=0})\\big]+ \\sum_{i=1}^m\\mathrm{ln}\\big[p_{y}(y^{(i)};\\phi_y)\\big] \\tag{SCN.4}\\\\\\\\\n",
    "     &= \\sum_{i=1}^m\\sum_{j=1}^{n}\\big\\{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}\\mathrm{ln}(\\phi_{j\\mid y=1})+\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=1\\}\\mathrm{ln}(\\phi_{j\\mid y=0}) \\\\\n",
    "     &+ \\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}\\mathrm{ln}(1-\\phi_{j\\mid y=1})+\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=0\\}\\mathrm{ln}(1-\\phi_{j|y=0})\\big\\} \\tag{SCN.5} \\\\\n",
    "     &+ \\sum_{i=1}^m\\big\\{\\boldsymbol{1}\\{y^{(i)}=1\\}\\mathrm{ln}(\\phi_y)+\\boldsymbol{1}\\{y^{(i)}=0\\}\\mathrm{ln}(1-\\phi_y)\\big\\}\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "SCN.3 follows from the Naive Bayes Assumption: $x_1^{(i)},...,x_n^{(i)}$ are conditionally independent given $y^{(i)}$. Intuitively, if we know whether an email is spam or not, then the appearance of different words is assumed to be independent. For example, if we know that an email is spam, then the appearances of \"price\" and \"discount\" are assumed to be independent events. This example emphasizes the general inaccuracy of this assumption and explains \"Naive\" in the name. If we know the email is spam and we know that \"price\" appeared, then it seems more likely that \"discount\" also appeared. Nevertheless, NB can give good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli NB MLE\n",
    "\n",
    "To maximize the log-likelihood in $\\phi_y$, let's compute the partial derivative of SCN.5 with respect to $\\phi_y$ and set it equal to zero:\n",
    "\n",
    "$$\\begin{align*} \n",
    "0=\\frac{\\partial\\ell}{\\partial\\phi_y} &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\frac1{\\phi_y} - \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}}\\frac1{1-\\phi_y}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}}\\frac1{1-\\phi_y} = \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\frac1{\\phi_y}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\frac1{1-\\phi_y}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}} = \\frac1{\\phi_y}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Multiplying both sides by $\\phi_y(1-\\phi_y)$, we get\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}} &= (1-\\phi_y)\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\\\\n",
    "    &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}} - \\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}} &= \\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}} + \\phi_y\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\\\\\n",
    "    &= \\phi_y\\sum_{i=1}^m\\big[\\boldsymbol{1}\\{y^{(i)}=0\\} + \\boldsymbol{1}\\{y^{(i)}=1\\}\\big]\\\\\n",
    "    &= \\phi_y\\sum_{i=1}^m1\\\\\n",
    "    &= \\phi_y m\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_y = \\frac1m\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can maximize in $\\phi_{j|y=1}$:\n",
    "\n",
    "$$\\begin{align*} \n",
    "0=\\frac{\\partial\\ell}{\\partial\\phi_{j|y=1}} &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\frac1{\\phi_{j|y=1}} - \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}}\\frac1{1-\\phi_{j|y=1}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}}\\frac1{1-\\phi_{j|y=1}} = \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\frac1{\\phi_{j|y=1}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\frac1{1-\\phi_{j|y=1}}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}} = \\frac1{\\phi_{j|y=1}}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Multiplying both sides by $\\phi_{j|y=1}(1-\\phi_{j|y=1})$, we get\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}} &= (1-\\phi_{j|y=1})\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\\\\n",
    "    &= \\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}} - \\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}} &= \\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\}} + \\phi_{j|y=1}\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}\\\\\n",
    "    &= \\phi_{j|y=1}\\sum_{i=1}^m\\big[\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=0\\} + \\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}\\big]\\\\\n",
    "    &= \\phi_{j|y=1}\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j|y=1} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\wedge x_j^{(i)}=1\\}}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=1\\}}x_j^{(i)}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=1\\}}\\tag{SCN.6}\n",
    "\\end{align*}$$\n",
    "\n",
    "Similarly we can compute\n",
    "\n",
    "$$\\begin{align*} \n",
    "\\phi_{j|y=0} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\wedge x_j^{(i)}=1\\}}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=0\\}} = \\frac{\\sum_{i=1}^m{\\boldsymbol{1}\\{y^{(i)}=0\\}}x_j^{(i)}}{\\sum_{i=1}^m\\boldsymbol{1}\\{y^{(i)}=0\\}}\\tag{SCN.7}\n",
    "\\end{align*}$$\n",
    "\n",
    "SCN.6 and SCN.7 are very similar to the corresponding multinomial equations in SE.8 and SE.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
