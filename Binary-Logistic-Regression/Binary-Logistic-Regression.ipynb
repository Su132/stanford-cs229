{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm as normal\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "def graphs(functions, x_ranges, format_str='r-', lw=1., plts_per_row=4):\n",
    "    if len(functions) != len(x_ranges):\n",
    "        raise ValueError(\"The number of functions ({}) doesn't equal the number of x ranges ({}).\"\n",
    "                         .format(len(functions), len(x_ranges)))\n",
    "    numb_plots = len(functions)\n",
    "    nrows = numb_plots // plts_per_row + (0 if numb_plots % plts_per_row == 0 else 1)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=plts_per_row, sharex=False, sharey=True,\n",
    "                             figsize=(20 * (plts_per_row/4), (4/plts_per_row) * np.log(1 + np.exp(nrows+plts_per_row))))\n",
    "    if numb_plots > 1: axes = axes.ravel()\n",
    "\n",
    "    for k, fx in enumerate(zip(functions, x_ranges)):\n",
    "        f, x_range = fx[0], np.array(fx[1])\n",
    "        ax = axes[k] if numb_plots > 1 else axes\n",
    "        ax.plot(x_range, f(x_range), format_str, lw=lw)\n",
    "\n",
    "def graphs_1plt(functions, labels, x_range, lw=1.):\n",
    "    if len(functions) != len(labels):\n",
    "        raise ValueError(\"The number of functions ({}) doesn't equal the number of labels ({}).\"\n",
    "                         .format(len(functions), len(labels)))\n",
    "    for fl in zip(functions, labels):\n",
    "        f, label = fl[0], np.array(fl[1])\n",
    "        plt.plot(x_range, f(x_range), lw=lw, label=label)\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From class-notes-1, p.16-19:\n",
    "\n",
    "The __sigmoid__ or __logistic function__ is\n",
    "\n",
    "$$\n",
    "g(z)\\equiv\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "This function __squashes__ the real line to the open interval $(0,1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAGNCAYAAABUjEMEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzPwPkFVRtWf8Sf4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bebc048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    # we want to prevent overflow in np.exp(), so we first clip z\n",
    "    signal = np.clip(z, -700, 700)\n",
    "    return 1. / (1 + np.exp(-signal))\n",
    "\n",
    "zs = [np.linspace(-i,i) for i in [3,6,10,100,1000]]\n",
    "fs = len(zs)*[sigmoid]\n",
    "graphs(fs, zs, plts_per_row=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid has some important properties:\n",
    "\n",
    "$$\\begin{gather}\n",
    "0 < g(z) < 1\\quad\\text{for all }z\\in\\mathbb{R}\\tag{Sigmoid.1}\\\\\\\\\n",
    "\\lim_{z\\rightarrow-\\infty}g(z)=0\\tag{Sigmoid.2}\\\\\\\\\n",
    "\\lim_{z\\rightarrow\\infty}g(z)=1\\tag{Sigmoid.3}\\\\\\\\\n",
    "g(0)=\\frac{1}{2}\\tag{Sigmoid.4}\\\\\\\\\n",
    "g(-z)=1-g(z)\\tag{Sigmoid.5}\\\\\\\\\n",
    "\\frac{d}{dz}g(z)=g(z)(1-g(z))\\tag{Sigmoid.6}\\\\\\\\\n",
    "\\end{gather}$$\n",
    "\n",
    "The first three properties make the sigmoid a cumulative distribution function.\n",
    "\n",
    "Sigmoid.1 follows because: we see that $g>0$ since the denominator $1+e^{-z}$ is positive for all $z$. To show that $g(z)<1$ for all $z\\in\\mathbb{R}$, we want to show that $1+e^{-z}=\\frac{1}{g(z)}>1$ for all $z\\in\\mathbb{R}$. This is equivalent to showing that $e^{-z}>0$ for all $z\\in\\mathbb{R}$, which we know is true.\n",
    "\n",
    "Sigmoid.2 follows because:\n",
    "\n",
    "$$\n",
    "\\lim_{z\\rightarrow-\\infty}g(z)=\\frac{1}{1+\\lim_{z\\rightarrow-\\infty}e^{-z}}=\\frac{1}{1+e^{--\\infty}}=\\frac{1}{1+e^{\\infty}}=\\frac{1}{1+\\infty}=0\n",
    "$$\n",
    "\n",
    "Sigmoid.3 follows because: \n",
    "\n",
    "$$\n",
    "\\lim_{z\\rightarrow\\infty}g(z)=\\frac{1}{1+\\lim_{z\\rightarrow\\infty}e^{-z}}=\\frac{1}{1+e^{-\\infty}}=\\frac{1}{1+0}=1\n",
    "$$\n",
    "\n",
    "Sigmoid.4 follows because $g(0)=\\frac{1}{1+e^{-0}}=\\frac{1}{1+1}=\\frac{1}{2}$.\n",
    "\n",
    "Sigmoid.5 follows because: \n",
    "\n",
    "$$\n",
    "g(-z)=\\frac{1}{1+e^{--z}}=\\frac{1}{1+e^{z}}=\\frac{e^{-z}}{e^{-z}(1+e^{z})}=\\frac{1+e^{-z}-1}{1+e^{-z}}=1-\\frac{1}{1+e^{-z}}=1-g(z)\n",
    "$$\n",
    "\n",
    "Sigmoid.6 follows because:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dz}g(z) =\\frac{d\\big(\\frac{1}{1 + e^{-z}}\\big)}{dz}  \n",
    "                              =\\frac{0 - 1 \\cdot\\frac{d(1 + e^{-z})}{dz}}{(1 + e^{-z})^2} \n",
    "                              =\\frac{--e^{-z}}{(1 + e^{-z})^2}\n",
    "                              =\\frac{e^{-z}}{(1 + e^{-z})^2} \n",
    "                              =\\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}}\n",
    "                              =g(z) \\cdot \\frac{1+e^{-z}-1}{1 + e^{-z}}\n",
    "                              =g(z)(1 - g(z))\n",
    "$$\n",
    "\n",
    "Since the sigmoid is a CDF, its derivative is a probability density function. It's interesting to compare its graph to that of the normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114319128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid_grad(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "fs = [sigmoid_grad, normal(0,1.6).pdf]\n",
    "labels = ['sigmoid derivative', 'normal']\n",
    "\n",
    "graphs_1plt(fs, labels, np.linspace(-5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given training data $X$ and $y$:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}x_0^{(1)}&\\dots&x_n^{(1)}\\\\\\vdots&\\ddots&\\vdots\\\\x_0^{(m)}&\\dots&x_n^{(m)}\\end{bmatrix}\n",
    "=\\begin{bmatrix}(x^{(1)})^T\\\\\\vdots\\\\(x^{(m)})^T\\end{bmatrix}\n",
    "\\quad\\quad\n",
    "y=\\begin{bmatrix}y^{(1)}\\\\\\vdots\\\\y^{(m)}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $x_0^{(i)}=1$ for all $i$ and $y^{(i)}\\in\\{0,1\\}$ for all $i$. What is the probability distribution of the $y^{(i)}$'s given the $x^{(i)}$'s? The joint probability is given by $P(y|X;\\theta)$. This quantity is typically viewed a function of $y$ (and perhaps $X$),\n",
    "for a fixed value of $\\theta$. When we wish to explicitly view this as a function of $\\theta$, we will instead call it the likelihood function:\n",
    "\n",
    "$$\n",
    "L(\\theta)=L(\\theta;X,y) = P(y|X;\\theta)\n",
    "$$\n",
    "\n",
    "We assume that $y$ is conditionally independent given $X$:\n",
    "\n",
    "$$\n",
    "L(\\theta)=P(y|X;\\theta)=\\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\\theta)\\tag{BLR.1}\n",
    "$$\n",
    "\n",
    "The principal of maximum likelihood says that we should should choose $\\theta$ so as to make the data as high probability as possible. I.e., we should choose $\\theta$ to maximize $L(\\theta)$.\n",
    "\n",
    "We define the hypothesis (the function to be learned from the training) as\n",
    "\n",
    "$$\n",
    "h_\\theta(x)\\equiv g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "and we assume that\n",
    "\n",
    "$$\\begin{gather}\n",
    "P(y^{(i)}=1|x^{(i)};\\theta) = h_\\theta(x^{(i)})\\\\\\\\\n",
    "P(y^{(i)}=0|x^{(i)};\\theta) = 1-h_\\theta(x^{(i)})\\\\\\\\\n",
    "\\end{gather}$$\n",
    "\n",
    "This can be written more compactly:\n",
    "\n",
    "$$\n",
    "P(y^{(i)}|x^{(i)};\\theta) = \\big(h_\\theta(x^{(i)})\\big)^{y^{(i)}}\\big(1-h_\\theta(x^{(i)})\\big)^{1-y^{(i)}}\n",
    "$$\n",
    "\n",
    "Then BLR.1 becomes\n",
    "\n",
    "$$\n",
    "L(\\theta)=P(y|X;\\theta)=\\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^{m}\\big(h_\\theta(x^{(i)})\\big)^{y^{(i)}}\\big(1-h_\\theta(x^{(i)})\\big)^{1-y^{(i)}}\\tag{BLR.2}\n",
    "$$\n",
    "\n",
    "As usual, it is easier to maximize the log-likelihood:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\ell(\\theta)=\\log[L(\\theta)]&=\\sum_{i=1}^{m}\\log\\big[\\big(h_\\theta(x^{(i)})\\big)^{y^{(i)}}\\big(1-h_\\theta(x^{(i)})\\big)^{1-y^{(i)}}\\big]\\\\\n",
    "    &=\\sum_{i=1}^{m}\\log\\big[\\big(h_\\theta(x^{(i)})\\big)^{y^{(i)}}\\big]+\\log\\big[\\big(1-h_\\theta(x^{(i)})\\big)^{1-y^{(i)}}\\big]\\\\\n",
    "    &=\\sum_{i=1}^{m}y^{(i)}\\log\\big(h_\\theta(x^{(i)})\\big)+(1-y^{(i)})\\log\\big(1-h_\\theta(x^{(i)})\\big)\\tag{BLR.3}\n",
    "\\end{align*}$$\n",
    "\n",
    "Set $z^{(i)}\\equiv\\theta^Tx^{(i)}$. Then maximizing the log-likelihood is the same as minimizing this objective function:\n",
    "\n",
    "$$\\begin{align*}\n",
    "J(\\theta)\\equiv-\\frac{1}{m}\\ell(\\theta)&=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log\\big(h_\\theta(x^{(i)})\\big)+(1-y^{(i)})\\log\\big(1-h_\\theta(x^{(i)})\\big)\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log g(z^{(i)})+(1-y^{(i)})\\log\\big(1-g(z^{(i)})\\big)\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log g(z^{(i)})+(1-y^{(i)})\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log g(z^{(i)})+\\log g(-z^{(i)})-y^{(i)}\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\big[\\log g(z^{(i)})-\\log\\big(g(-z^{(i)})\\big)\\big]+\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log\\frac{g(z^{(i)})}{g(-z^{(i)})}+\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log\\Big(g(z^{(i)})\\frac{1}{g(-z^{(i)})}\\Big)+\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log\\Big(\\frac{1}{1+e^{-z^{(i)}}}\\frac{1}{\\frac{1}{1+e^{z^{(i)}}}}\\Big)+\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log\\Big(\\frac{1}{1+e^{-z^{(i)}}}\\frac{1+e^{z^{(i)}}}{1}\\Big)+\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log\\Big(\\frac{1+e^{z^{(i)}}}{1+e^{-z^{(i)}}}\\Big)+\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log\\Big(\\frac{e^{z^{(i)}}(e^{-z^{(i)}}+1)}{1+e^{-z^{(i)}}}\\Big)+\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log\\big(e^{z^{(i)}}\\big)+\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}z^{(i)}+\\log g(-z^{(i)})\\\\\n",
    "    &=-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\theta^Tx^{(i)}+\\log \\frac{1}{1+e^{z^{(i)}}}\\\\\n",
    "    &=\\frac{1}{m}\\sum_{i=1}^{m}-y^{(i)}\\theta^Tx^{(i)}-\\log \\frac{1}{1+e^{z^{(i)}}}\\\\\n",
    "    &=\\frac{1}{m}\\sum_{i=1}^{m}-y^{(i)}\\theta^Tx^{(i)}+\\log\\Big(\\Big[\\frac{1}{1+e^{z^{(i)}}}\\Big]^{-1}\\Big)\\\\\n",
    "    &=\\frac{1}{m}\\sum_{i=1}^{m}-y^{(i)}\\theta^Tx^{(i)}+\\log (1+e^{z^{(i)}})\\\\\n",
    "    &=\\frac{1}{m}\\sum_{i=1}^{m}\\log(1+e^{\\theta^Tx^{(i)}})-y^{(i)}\\theta^Tx^{(i)}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Succintly, we have\n",
    "\n",
    "$$\n",
    "J(\\theta)\\equiv-\\frac{1}{m}\\ell(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\log(1+e^{\\theta^Tx^{(i)}})-y^{(i)}\\theta^Tx^{(i)}\\tag{BLR.4}\\\\\n",
    "$$\n",
    "\n",
    "NOTE: $J(\\theta)\\equiv-\\frac{1}{m}\\ell(\\theta)$ is called the negative-log-likelihood objective function. The scaling  term $\\frac{1}{m}$ is optional.\n",
    "\n",
    "Let's take a partial derivative of a single observation:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_j}\\big(\\log(1+e^{\\theta^Tx^{(i)}})-y^{(i)}\\theta^Tx^{(i)}\\big) &= \\frac{1}{1+e^{\\theta^Tx^{(i)}}}e^{\\theta^Tx^{(i)}}x_j^{(i)}-y^{(i)}x_j^{(i)}\\\\\\\\\n",
    "     &= \\frac{e^{\\theta^Tx^{(i)}}}{1+e^{\\theta^Tx^{(i)}}}x_j^{(i)}-y^{(i)}x_j^{(i)}\\\\\\\\\n",
    "     &= \\frac{e^{-\\theta^Tx^{(i)}}e^{\\theta^Tx^{(i)}}}{e^{-\\theta^Tx^{(i)}}(1+e^{\\theta^Tx^{(i)}})}x_j^{(i)}-y^{(i)}x_j^{(i)}\\\\\\\\\n",
    "     &= \\frac{1}{1+e^{-\\theta^Tx^{(i)}}}x_j^{(i)}-y^{(i)}x_j^{(i)}\\\\\\\\\n",
    "     &= g(\\theta^Tx^{(i)})x_j^{(i)}-y^{(i)}x_j^{(i)}\\\\\\\\\n",
    "     &= \\big(g(\\theta^Tx^{(i)})-y^{(i)}\\big)x_j^{(i)}\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "So we have\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_j}J(\\theta) &= \\frac{1}{m}\\sum_{i=1}^{m}\\big(g(\\theta^Tx^{(i)})-y^{(i)}\\big)x_j^{(i)}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Note that for $a^{(i)}\\in\\mathbb{R}$ and $b^{(i)}\\in\\mathbb{R}^2$, we have\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\begin{bmatrix}\\sum_{i=1}^{2}a^{(i)}b_1^{(i)}\\\\\\sum_{i=1}^{2}a^{(i)}b_2^{(i)}\\end{bmatrix} &= \\begin{bmatrix}a^{(1)}b_1^{(1)}+a^{(2)}b_1^{(2)}\\\\a^{(1)}b_2^{(1)}+a^{(2)}b_2^{(2)}\\end{bmatrix}\\\\\\\\\n",
    "    &= \\begin{bmatrix}a^{(1)}b_1^{(1)}\\\\a^{(1)}b_2^{(1)}\\end{bmatrix}+\\begin{bmatrix}a^{(2)}b_1^{(2)}\\\\a^{(2)}b_2^{(2)}\\end{bmatrix}\\\\\\\\\n",
    "    &= a^{(1)}\\begin{bmatrix}b_1^{(1)}\\\\b_2^{(1)}\\end{bmatrix}+a^{(2)}\\begin{bmatrix}b_1^{(2)}\\\\b_2^{(2)}\\end{bmatrix}\\\\\\\\\n",
    "    &=a^{(1)}b^{(1)}+a^{(2)}b^{(2)}\\\\\\\\\n",
    "    &=\\sum_{i=1}^{2}a^{(i)}b^{(i)}\\tag{BLR.5}\n",
    "\\end{align*}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla J(\\theta) &= \\begin{bmatrix}\\frac{1}{m}\\sum_{i=1}^{m}\\big(g(\\theta^Tx^{(i)})-y^{(i)}\\big)x_0^{(i)}\\\\\\vdots\\\\\\frac{1}{m}\\sum_{i=1}^{m}\\big(g(\\theta^Tx^{(i)})-y^{(i)}\\big)x_n^{(i)}\\end{bmatrix}\\\\\\\\\n",
    "     &= \\frac{1}{m}\\sum_{i=1}^{m}\\big(g(\\theta^Tx^{(i)})-y^{(i)}\\big)x^{(i)}\\tag{by BLR.5}\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Alternatively, we can vectorize the gradient:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla J(\\theta) &= \\begin{bmatrix}\\frac{1}{m}\\sum_{i=1}^{m}\\big(g(\\theta^Tx^{(i)})-y^{(i)}\\big)x_0^{(i)}\\\\\\dots\\\\\\frac{1}{m}\\sum_{i=1}^{m}\\big(g(\\theta^Tx^{(i)})-y^{(i)}\\big)x_n^{(i)}\\end{bmatrix}\\\\\\\\\n",
    "     &= \\frac{1}{m}\\begin{bmatrix}x_0^{(1)}&\\dots&x_0^{(m)}\\\\\\vdots&\\ddots&\\vdots\\\\x_n^{(1)}&\\dots&x_n^{(m)}\\end{bmatrix}\\begin{bmatrix}g(\\theta^Tx^{(1)})-y^{(1)}\\\\\\vdots\\\\g(\\theta^Tx^{(m)})-y^{(m)}\\end{bmatrix}\\\\\\\\\n",
    "     &= \\frac{1}{m}X^T(h-y)\\\\\\\\\n",
    "     &= \\frac{1}{m}([X^T(h-y)]^T)^T\\\\\\\\\n",
    "     &= \\frac{1}{m}((h-y)^TX)^T\\\\\\\\\n",
    "     &= \\frac{1}{m}[(g(X\\theta)-y)^TX]^T\n",
    "\\end{align*}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{align*}\n",
    "h\\equiv\\begin{bmatrix}g(\\theta^Tx^{(1)})\\\\\\vdots\\\\g(\\theta^Tx^{(m)})\\end{bmatrix}=g\\Bigg(\\begin{bmatrix}\\theta^Tx^{(1)}\\\\\\vdots\\\\\\theta^Tx^{(m)}\\end{bmatrix}\\Bigg)=g\\Bigg(\\begin{bmatrix}(x^{(1)})^T\\theta\\\\\\vdots\\\\(x^{(m)})^T\\theta\\end{bmatrix}\\Bigg)=g\\Bigg(\\begin{bmatrix}(x^{(1)})^T\\\\\\vdots\\\\(x^{(m)})^T\\end{bmatrix}\\theta\\Bigg)=g(X\\theta)\n",
    "\\end{align*}$$\n",
    "\n",
    "and where $g(v)$ for a vector $v\\in\\mathbb{R}^n$ means elementwise sigmoid.\n",
    "\n",
    "We can also vectorize the objective function from BLR.4:\n",
    "\n",
    "$$\\begin{align*}\n",
    "J(\\theta)\\equiv-\\frac{1}{m}\\ell(\\theta) &= \\frac{1}{m}\\sum_{i=1}^{m}\\log(1+e^{\\theta^Tx^{(i)}})-y^{(i)}\\theta^Tx^{(i)}\\\\\\\\\n",
    "     &= \\frac{1}{m}\\sum_{i=1}^{m}\\log(1+e^{(x^{(i)})^T\\theta})-\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\theta^Tx^{(i)}\\\\\\\\\n",
    "     &= \\frac{1}{m}{\\Large S}\\begin{bmatrix}\\log(1+e^{(x^{(1)})^T\\theta})\\\\\\vdots\\\\\\log(1+e^{(x^{(m)})^T\\theta})\\end{bmatrix}-\\frac{1}{m}\\theta^T\\sum_{i=1}^{m}y^{(i)}x^{(i)}\\\\\\\\\n",
    "     &= \\frac{1}{m}{\\Large S}\\begin{bmatrix}\\log(1+e^{\\langle X_{1:},\\theta\\rangle})\\\\\\vdots\\\\\\log(1+e^{\\langle X_{m:},\\theta\\rangle})\\end{bmatrix}-\\frac{1}{m}\\theta^T\\begin{bmatrix}\\sum_{i=1}^{m}y^{(i)}x_0^{(i)}\\\\\\vdots\\\\\\sum_{i=1}^{m}y^{(i)}x_n^{(i)}\\end{bmatrix}\\tag{by BLR.5}\\\\\\\\\n",
    "     &= \\frac{1}{m}{\\Large S}\\log\\Bigg(1+^B\\exp\\begin{bmatrix}\\langle X_{1:},\\theta\\rangle\\\\\\vdots\\\\\\langle X_{m:},\\theta\\rangle\\end{bmatrix}\\Bigg)-\\frac{1}{m}\\Bigg(\\theta^T\\begin{bmatrix}\\sum_{i=1}^{m}y^{(i)}x_0^{(i)}\\\\\\vdots\\\\\\sum_{i=1}^{m}y^{(i)}x_n^{(i)}\\end{bmatrix}\\Bigg)^T\\\\\\\\\n",
    "     &= \\frac{1}{m}{\\Large S}\\log\\big[1+^B\\exp(X\\theta)\\big]-\\frac{1}{m}\\begin{bmatrix}\\sum_{i=1}^{m}y^{(i)}x_0^{(i)}&\\dots&\\sum_{i=1}^{m}y^{(i)}x_n^{(i)}\\end{bmatrix}\\theta\\\\\\\\\n",
    "     &= \\frac{1}{m}{\\Large S}\\log\\big[1+^B\\exp(X\\theta)\\big]-\\frac{1}{m}\\begin{bmatrix}y^{(1)}&\\dots&y^{(m)}\\end{bmatrix}\\begin{bmatrix}x_0^{(1)}&\\dots&x_n^{(1)}\\\\\\vdots&\\ddots&\\vdots\\\\x_0^{(m)}&\\dots&x_n^{(m)}\\end{bmatrix}\\theta\\\\\\\\\n",
    "     &= \\frac{1}{m}\\Big({\\Large S}\\log\\big[1+^B\\exp(X\\theta)\\big]-y^TX\\theta\\Big)\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where ${\\large S}$ denotes the sum of the elements of the vector and $1+^Bv$ denotes adding $1$ to each of the elements of the vector $v$ (broadcasting). And for any function $f$ (such as $\\log$ or $\\exp$) and vector $v$, $f(v)$ denotes elementwise application of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood_cost_slow(X, theta, y):  \n",
    "    xt = X.dot(theta)\n",
    "    h = np.clip(sigmoid(xt), 1e-16, 1-(1e-16))\n",
    "    return np.mean(-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "\n",
    "# this can also be implemented without the scaling term 1/xt.shape[0]\n",
    "def llc(xt, y, regularlization=0, theta=0):\n",
    "    return 1/xt.shape[0] * (np.sum(np.log(1 + np.exp(np.clip(xt, -700, 700)))) - np.dot(y,xt)) \\\n",
    "        + (regularlization * (np.linalg.norm(theta)**2))\n",
    "\n",
    "# this can also be implemented without the scaling term 1/xt.shape[0]\n",
    "def llg(X, xt, y, regularlization=0, theta=0):\n",
    "    return 1/xt.shape[0] * (sigmoid(xt) - y).dot(X) + (regularlization * 2 * theta)\n",
    "\n",
    "def log_likelihood_cost(X, theta, y, cost_or_grad='both', regularlization=0):\n",
    "    xt = X.dot(theta)\n",
    "    th = theta if regularlization else 0\n",
    "    if cost_or_grad == 'both':\n",
    "        return (llc(xt, y, regularlization=regularlization, theta=th), llg(X, xt, y, regularlization=regularlization, theta=th))\n",
    "    if cost_or_grad == 'cost':\n",
    "        return llc(xt, y, regularlization=regularlization, theta=th)\n",
    "    if cost_or_grad == 'grad':\n",
    "        return llg(X, xt, y, regularlization=regularlization, theta=th)\n",
    "\n",
    "def logistic_regression(X, y, learning_rate, scale_lr_geom=False, scale_lr_prop=0, regularlization=0, prntiters=1000, tol=1e-7):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    thetas, thetas_sample, errors, losses = [], [], [], []\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        prev_theta = theta\n",
    "        cost, grad = log_likelihood_cost(X, theta, y, regularlization=regularlization, cost_or_grad='both')\n",
    "        theta = theta - learning_rate * grad\n",
    "        thetas.append(theta)\n",
    "        errors.append(np.linalg.norm(prev_theta - theta))\n",
    "        losses.append(cost)\n",
    "        if scale_lr_geom: learning_rate /= (i**2)\n",
    "        if scale_lr_prop: learning_rate *= scale_lr_prop\n",
    "        if i % prntiters == 0:\n",
    "            print('iters={} theta={} error={} loss={}'.format(i, np.mean(theta), errors[-1], losses[-1]))\n",
    "            thetas_sample.append(theta)\n",
    "        if errors[-1] < tol:\n",
    "            print('Converged in %d iterations' % i)\n",
    "            break\n",
    "        if i == 200 * 1000:\n",
    "            break\n",
    "    return np.array(thetas), thetas_sample, errors, losses\n",
    "\n",
    "def time_tester(X, y, learning_rate, max_iters, and_cost=False, prntiters=1000, opts=False):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    thetas = []\n",
    "    for i in range(max_iters):\n",
    "        if and_cost:\n",
    "            cost, grad = log_likelihood_cost(X, theta, y, cost_or_grad='both')\n",
    "        else:\n",
    "            xt = X.dot(theta)\n",
    "            grad = llg(X, xt, y)\n",
    "        theta = theta - learning_rate * grad\n",
    "        if opts:\n",
    "            error = np.linalg.norm(theta[-1] - theta)\n",
    "            thetas.append(theta)\n",
    "            loss = llc(xt, y)\n",
    "            if i % prntiters == 0:\n",
    "                print(\"iter={} theta={} loss={} error={}\".format(i, np.mean(theta), loss, error))\n",
    "    return theta, np.array(thetas)\n",
    "\n",
    "def compare_cost_fns(X, y, iterations=10*1000):\n",
    "    theta = np.random.randn(X.shape[1])\n",
    "    start = time.time()\n",
    "    for i in range(iterations):\n",
    "        log_likelihood_cost(X, theta, y, cost_or_grad='cost')\n",
    "    end = time.time()\n",
    "    print(\"training time for fast cost: {} seconds   iters={}\".format(end - start, iterations))\n",
    "    start = time.time()\n",
    "    for i in range(iterations):\n",
    "        log_likelihood_cost_slow(X, theta, y)\n",
    "    end = time.time()\n",
    "    print(\"training time for slow cost: {} seconds   iters={}\".format(end - start, iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial Xitn=(10000, 784)  yitn=(10000,)\n",
      "ind=(10000,)  sumind=2115  Xind=(2115, 784)  yind=(2115,)\n",
      "X_train=(1692, 784)  X_test=(422, 784)  y_train=(1692,)  y_test=(422,)\n",
      "Xy_train=(1692, 785)  Xy_test=(422, 785)\n"
     ]
    }
   ],
   "source": [
    "# let's collect and organize the data\n",
    "\n",
    "Xitn = np.ndfromtxt('images.csv', delimiter=',')\n",
    "yitn = np.ndfromtxt(\"labels.csv\", delimiter=',', dtype=np.int8)\n",
    "print(\"initial Xitn={}  yitn={}\".format(Xitn.shape, yitn.shape))\n",
    "\n",
    "ind = np.logical_or(yitn == 1, yitn == 0)\n",
    "Xind = Xitn[ind, :]\n",
    "yind = yitn[ind]\n",
    "print(\"ind={}  sumind={}  Xind={}  yind={}\".format(ind.shape, np.sum(ind), Xind.shape, yind.shape))\n",
    "\n",
    "num_train = int(len(yind) * 0.8)\n",
    "X_train = Xind[0:num_train, :]\n",
    "X_test = Xind[num_train:-1,:]\n",
    "y_train = yind[0:num_train]\n",
    "y_test = yind[num_train:-1]\n",
    "print(\"X_train={}  X_test={}  y_train={}  y_test={}\".format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))\n",
    "\n",
    "Xy_train = np.hstack((X_train, y_train.reshape(-1,1)))\n",
    "Xy_test = np.hstack((X_test, y_test.reshape(-1,1)))\n",
    "print(\"Xy_train={}  Xy_test={}\".format(Xy_train.shape, Xy_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRONG: slow cost=5.573007834682271  fast cost=5.573007834681763  diff=5.080380560684716e-13\n",
      "WRONG: slow cost=5.896191305178432  fast cost=5.8961913051791255  diff=-6.936673457857978e-13\n",
      "WRONG: slow cost=5.501053995663017  fast cost=5.5010539956625  diff=5.169198402654729e-13\n",
      "WRONG: slow cost=5.91613088315236  fast cost=5.9161308831531265  diff=-7.664979762012081e-13\n",
      "WRONG: slow cost=5.534945654772681  fast cost=5.534945654773196  diff=-5.151434834260726e-13\n",
      "WRONG: slow cost=5.479477598436633  fast cost=5.479477598436003  diff=6.297184995673888e-13\n",
      "WRONG: slow cost=5.855639574742285  fast cost=5.855639574743685  diff=-1.3997691894473974e-12\n",
      "WRONG: slow cost=5.769005834530796  fast cost=5.769005834531689  diff=-8.926193117986259e-13\n",
      "WRONG: slow cost=5.469440024404497  fast cost=5.469440024405206  diff=-7.096545573404001e-13\n",
      "WRONG: slow cost=5.764425494888452  fast cost=5.764425494887692  diff=7.602807272633072e-13\n",
      "WRONG: slow cost=6.345137061248626  fast cost=6.345137061247452  diff=1.1741718708435656e-12\n",
      "WRONG: slow cost=5.777230916644063  fast cost=5.777230916644594  diff=-5.311306949806749e-13\n",
      "WRONG: slow cost=5.77700738232853  fast cost=5.7770073823280095  diff=5.204725539442734e-13\n",
      "wrong=13  iters=5000  perc=0.0026\n"
     ]
    }
   ],
   "source": [
    "# verify cost function\n",
    "\n",
    "obs, attrs = 1695, 37\n",
    "iterations = 5 * 1000\n",
    "wrong = 0\n",
    "\n",
    "for i in range(iterations):\n",
    "    Xct = np.random.rand(obs, attrs)\n",
    "    theta = np.random.rand(attrs)\n",
    "    yct = np.random.rand(obs)\n",
    "    cost_s = log_likelihood_cost_slow(Xct, theta, yct)\n",
    "    cost_f = log_likelihood_cost(Xct, theta, yct, cost_or_grad='cost')\n",
    "    if np.abs(cost_s - cost_f) > 5e-13:\n",
    "        print(\"WRONG: slow cost={}  fast cost={}  diff={}\".format(cost_s, cost_f, cost_s - cost_f))\n",
    "        wrong+=1\n",
    "\n",
    "print(\"wrong={}  iters={}  perc={}\".format(wrong, iterations, wrong/iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 5.473459959030151 seconds   percent=1.0\n",
      "time elapsed: 5.445704936981201 seconds   percent=1.0\n"
     ]
    }
   ],
   "source": [
    "# gradient speed test 1\n",
    "# This gradient implementation is from the cs229 Vectorization discussion: logistic_regression.py.\n",
    "# The two implementations take about the same amount of time to run, on this speed test and the\n",
    "# next one. But mine is consistently, slightly faster.\n",
    "\n",
    "def h_vec(theta, X):\n",
    "    signal = np.clip( np.matmul(X, theta), -500, 500 )\n",
    "    return 1 / (1 + np.exp(-signal))\n",
    "\n",
    "def GD (theta, X_train, y_train, alpha):\n",
    "    diff = h_vec(theta, X_train) - y_train\n",
    "    diff = np.reshape(diff, [1, -1])\n",
    "    dx = np.matmul(diff, X_train)\n",
    "    theta -= alpha * np.squeeze(dx)\n",
    "    return theta\n",
    "    \n",
    "def train_vec(X_train, y_train, max_iter, alpha):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    for i in range(max_iter):\n",
    "        theta = GD(theta, X_train, y_train, alpha)       \n",
    "    return theta\n",
    "\n",
    "speed_test_iterations = 10 * 1000\n",
    "alpha = 0.01\n",
    "\n",
    "start = time.time()\n",
    "theta = train_vec(X_train, y_train, speed_test_iterations, alpha)\n",
    "end = time.time()\n",
    "pred = (np.sign(h_vec(theta, X_test) - 0.5) + 1) / 2\n",
    "print(\"time elapsed: {} seconds   percent={}\".format(end - start, np.sum(pred == y_test) / len(y_test)))\n",
    "\n",
    "start = time.time()\n",
    "theta, thetas = time_tester(X_train, y_train, learning_rate=alpha, max_iters=speed_test_iterations, and_cost=False, opts=False)\n",
    "end = time.time()\n",
    "h = sigmoid(X_test.dot(theta))\n",
    "pred = (np.sign(h - 0.5) + 1) / 2\n",
    "print(\"time elapsed: {} seconds   percent={}\".format(end - start, np.sum(pred == y_test) / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 88.40023493766785 seconds   iters=10000\n",
      "training time: 88.92623400688171 seconds   iters=10000\n"
     ]
    }
   ],
   "source": [
    "# gradient speed test 2\n",
    "\n",
    "import random\n",
    "rs=2\n",
    "np.random.seed(rs)\n",
    "random.seed(rs)\n",
    "\n",
    "speed_test_iterations = 10 * 1000\n",
    "alpha = .01\n",
    "numb_obs = 23 * 1000\n",
    "Xst = np.random.rand(numb_obs,784)\n",
    "yst = np.array([random.getrandbits(1) for i in range(numb_obs)])\n",
    "\n",
    "start = time.time()\n",
    "theta, thetas = time_tester(Xst, yst, learning_rate=alpha, max_iters=speed_test_iterations, and_cost=False, opts=False)\n",
    "end = time.time()\n",
    "print(\"training time: {} seconds   iters={}\".format(end - start, speed_test_iterations))\n",
    "\n",
    "start = time.time()\n",
    "theta = train_vec(Xst, yst, speed_test_iterations, alpha)\n",
    "end = time.time()\n",
    "print(\"training time: {} seconds   iters={}\".format(end - start, speed_test_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time for fast cost: 28.637451887130737 seconds   iters=100000\n",
      "training time for slow cost: 31.423377752304077 seconds   iters=100000\n"
     ]
    }
   ],
   "source": [
    "# cost speed test\n",
    "\n",
    "compare_cost_fns(X_train, y_train, iterations=100*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is logistic regression with the logisitic cost, from PS-02, Q1:\n",
    "\n",
    "def add_intercept(X_):\n",
    "    m, n = X_.shape\n",
    "    X = np.zeros((m, n + 1))\n",
    "    X[:, 0] = 1\n",
    "    X[:, 1:] = X_\n",
    "    return X\n",
    "\n",
    "def load_data(filename):\n",
    "    D = np.loadtxt(filename)\n",
    "    Y = D[:, 0]\n",
    "    X = D[:, 1:]\n",
    "    return add_intercept(X), Y\n",
    "\n",
    "def logistic_cost(probs, regularlization=0, theta=0):\n",
    "    return -np.mean(np.log(probs)) + (regularlization * (np.linalg.norm(theta)**2))\n",
    "\n",
    "def logistic_grad(X, Y, probs, regularlization=0, theta=0):\n",
    "    return -(1./X.shape[0]) * (X.T.dot(probs * Y)) + (regularlization * 2 * theta)\n",
    "\n",
    "def logistic_cost_and_grad(X, Y, theta, regularlization=0, cog='both'):\n",
    "    margins = Y * X.dot(theta)\n",
    "    th = theta if regularlization else 0\n",
    "    if cog == 'grad':\n",
    "        probs = sigmoid(-margins)\n",
    "        return logistic_grad(X, Y, probs, regularlization=regularlization, theta=th)\n",
    "    if cog == 'cost':\n",
    "        probs = sigmoid(margins)\n",
    "        return logistic_cost(probs, regularlization=regularlization, theta=th)\n",
    "    if cog == 'both':\n",
    "        pc = sigmoid(margins)\n",
    "        pg = 1 - pc\n",
    "        cost = logistic_cost(pc, regularlization=regularlization, theta=th)\n",
    "        grad = logistic_grad(X, Y, pg, regularlization=regularlization, theta=th)\n",
    "        return cost, grad\n",
    "\n",
    "def logistic_regression_2(X, Y, scale_lr_geom=False, scale_lr_prop=0, regularlization=0, tol=1e-15):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    learning_rate = 10\n",
    "    thetas, thetas_sample, errors, losses = [], [], [], []\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        prev_theta = theta\n",
    "        cost, grad = logistic_cost_and_grad(X, Y, theta, regularlization=regularlization, cog='both')\n",
    "        theta = theta - learning_rate * grad\n",
    "        thetas.append(theta)\n",
    "        errors.append(np.linalg.norm(prev_theta - theta))\n",
    "        losses.append(cost)\n",
    "        if scale_lr_geom: learning_rate /= (i**2)\n",
    "        if scale_lr_prop: learning_rate *= scale_lr_prop\n",
    "        if i % 10000 == 0:\n",
    "            print('iters={} theta={} error={} loss={}'.format(i, theta, errors[-1], losses[-1]))\n",
    "            thetas_sample.append(theta)\n",
    "        if errors[-1] < tol:\n",
    "            print('Converged in %d iterations' % i)\n",
    "            break\n",
    "        if i == 200 * 1000:\n",
    "            break\n",
    "    return np.array(thetas), thetas_sample, errors, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781\n",
      "781\n",
      "199\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "y_train_2 = y_train.copy()\n",
    "print(sum(y_train_2==0))\n",
    "y_train_2[y_train_2==0]=-1\n",
    "print(sum(y_train_2==-1))\n",
    "\n",
    "y_test_2 = y_test.copy()\n",
    "print(sum(y_test_2==0))\n",
    "y_test_2[y_test_2==0]=-1\n",
    "print(sum(y_test_2==-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 13 iterations\n",
      "number_correct=422  number_tested=422  percentage correct=1.0\n"
     ]
    }
   ],
   "source": [
    "# Notice that the gradient of the logistic cost gives much faster convergence and the same accuracy\n",
    "\n",
    "thetas_2, thetas_sample_2, errors_2, losses_2 = logistic_regression_2(X_train, y_train_2, .01, tol=1e-15)\n",
    "pred = np.sign(sigmoid(X_test.dot(thetas_2[-1])) - 0.5)\n",
    "number_correct = np.sum(pred == y_test_2)\n",
    "number_tested = len(y_test_2)\n",
    "print(\"number_correct={}  number_tested={}  percentage correct={}\".format(number_correct, number_tested, number_correct / number_tested))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters=1000 theta=-0.03385757677723093 error=5.423972289571108e-07 loss=-1263.8944895305808\n",
      "iters=2000 theta=-0.03386206920521454 error=2.6862897258321127e-07 loss=-1263.7935615838076\n",
      "iters=3000 theta=-0.03386468216836832 error=1.7853010353704674e-07 loss=-1263.7348767853491\n",
      "iters=4000 theta=-0.033866532036649334 error=1.3369190404779807e-07 loss=-1263.693335521272\n",
      "iters=5000 theta=-0.03386796522412732 error=1.0685567410204589e-07 loss=-1263.6611535129487\n",
      "iters=6000 theta=-0.03386913536648768 error=8.899239383794842e-08 loss=-1263.6348792347956\n",
      "Converged in 6673 iterations\n"
     ]
    }
   ],
   "source": [
    "thetas, thetas_sample, errors, losses = logistic_regression(X_train, y_train, learning_rate=.01, regularlization=0, prntiters=1000, tol=8e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = thetas[-1]\n",
    "theta.shape\n",
    "h = sigmoid(X_test.dot(theta))\n",
    "np.sum([(h>1e-230) & (h<1-1e-230)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_correct=422  number_tested=422  percentage correct=1.0\n"
     ]
    }
   ],
   "source": [
    "pred = (np.sign(h - 0.5) + 1) / 2\n",
    "number_correct = np.sum(pred == y_test)\n",
    "number_tested = len(y_test)\n",
    "print(\"number_correct={}  number_tested={}  percentage correct={}\".format(number_correct, number_tested, number_correct / number_tested))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
